{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import scipy.optimize\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c_list = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0] #temperature data in celsius\n",
    "t_u_list = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] #temperature data in unknown units\n",
    "t_c = torch.tensor(t_c_list)\n",
    "t_u = torch.tensor(t_u_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our linear model function\n",
    "def model(x:torch.Tensor, w:torch.Tensor, b:torch.Tensor)->torch.Tensor:\n",
    "    #Weight and bias\n",
    "    return w*x + b\n",
    "\n",
    "popt, pcov = scipy.optimize.curve_fit(model, t_u_list, t_c_list, p0=(1,1))\n",
    "\n",
    "pearson_r, p = scipy.stats.pearsonr(t_u_list, t_c_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(t_u, t_c, label = 'data', color='r')\n",
    "plt.plot(t_u, model(t_u, popt[0], popt[1]), label = f'Linear fit, w = {popt[0]:.2f}, b = {popt[1]:.2f}')\n",
    "plt.ylabel('Temperature / Celsius')\n",
    "plt.xlabel('Temperature / Unknown Units')\n",
    "plt.title(f'Linear Fit Between Celsius and Unknown Unit Temp Data \\n r = {pearson_r:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the loss function\n",
    "def loss_fn(t_p:torch.Tensor, t_c:torch.Tensor)->torch.Tensor:\n",
    "    sqr_diff = (t_p - t_c)**2\n",
    "    return sqr_diff.mean()\n",
    "\n",
    "#Define the derivative of the loss function\n",
    "def dloss_fn(t_p:torch.Tensor, t_c:torch.Tensor)->torch.Tensor:\n",
    "    dsqr_diff = 2 * (t_p - t_c) / t_c.shape[0]\n",
    "    return dsqr_diff\n",
    "\n",
    "#Derivatives of model wrt parameters\n",
    "def dmodel_dw(t_u, w, b)->torch.Tensor:\n",
    "    \"Derivative of t_p wrt to w\"\n",
    "    return t_u\n",
    "\n",
    "def dmodel_db(t_u, w, b)->torch.Tensor:\n",
    "    \"Derivative of t_p wrt b\"\n",
    "    return 1.0\n",
    "\n",
    "#Definition of gradient, using the \n",
    "def grad_function(t_u, t_c, t_p, w, b):\n",
    "    \"Calculates the gradients of the loss wrt w and b and returns the loss sums as stacks\"\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b) #Using chain rule\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])\n",
    "\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    \"\"\"Performs n_epochs iterations of gradient descent and returns the estimated parameters\"\"\"\n",
    "    for _ in range (n_epochs):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_function(t_u, t_c, t_p, w, b)\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        print(f'Epoch: {_}, Loss: {loss} \\n\\\n",
    "              Params: {params}\\n\\\n",
    "              Grad: {grad}')\n",
    "    \n",
    "    return params, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 80.36434173583984 \n",
      "              Params: tensor([1.7761, 0.1064])\n",
      "              Grad: tensor([-77.6140, -10.6400])\n",
      "Epoch: 1, Loss: 37.574913024902344 \n",
      "              Params: tensor([2.0848, 0.1303])\n",
      "              Grad: tensor([-30.8623,  -2.3864])\n",
      "Epoch: 2, Loss: 30.871076583862305 \n",
      "              Params: tensor([2.2094, 0.1217])\n",
      "              Grad: tensor([-12.4631,   0.8587])\n",
      "Epoch: 3, Loss: 29.756193161010742 \n",
      "              Params: tensor([2.2616, 0.1004])\n",
      "              Grad: tensor([-5.2218,  2.1327])\n",
      "Epoch: 4, Loss: 29.507152557373047 \n",
      "              Params: tensor([2.2853, 0.0740])\n",
      "              Grad: tensor([-2.3715,  2.6310])\n",
      "Epoch: 5, Loss: 29.3924560546875 \n",
      "              Params: tensor([2.2978, 0.0458])\n",
      "              Grad: tensor([-1.2492,  2.8241])\n",
      "Epoch: 6, Loss: 29.298828125 \n",
      "              Params: tensor([2.3059, 0.0168])\n",
      "              Grad: tensor([-0.8071,  2.8970])\n",
      "Epoch: 7, Loss: 29.208717346191406 \n",
      "              Params: tensor([ 2.3122, -0.0124])\n",
      "              Grad: tensor([-0.6325,  2.9227])\n",
      "Epoch: 8, Loss: 29.119415283203125 \n",
      "              Params: tensor([ 2.3178, -0.0417])\n",
      "              Grad: tensor([-0.5633,  2.9298])\n",
      "Epoch: 9, Loss: 29.030488967895508 \n",
      "              Params: tensor([ 2.3232, -0.0710])\n",
      "              Grad: tensor([-0.5355,  2.9295])\n",
      "Epoch: 10, Loss: 28.941877365112305 \n",
      "              Params: tensor([ 2.3284, -0.1003])\n",
      "              Grad: tensor([-0.5240,  2.9264])\n",
      "Epoch: 11, Loss: 28.853565216064453 \n",
      "              Params: tensor([ 2.3336, -0.1295])\n",
      "              Grad: tensor([-0.5190,  2.9222])\n",
      "Epoch: 12, Loss: 28.765552520751953 \n",
      "              Params: tensor([ 2.3388, -0.1587])\n",
      "              Grad: tensor([-0.5165,  2.9175])\n",
      "Epoch: 13, Loss: 28.6778507232666 \n",
      "              Params: tensor([ 2.3439, -0.1878])\n",
      "              Grad: tensor([-0.5150,  2.9126])\n",
      "Epoch: 14, Loss: 28.590431213378906 \n",
      "              Params: tensor([ 2.3491, -0.2169])\n",
      "              Grad: tensor([-0.5138,  2.9077])\n",
      "Epoch: 15, Loss: 28.503318786621094 \n",
      "              Params: tensor([ 2.3542, -0.2459])\n",
      "              Grad: tensor([-0.5129,  2.9028])\n",
      "Epoch: 16, Loss: 28.4164981842041 \n",
      "              Params: tensor([ 2.3593, -0.2749])\n",
      "              Grad: tensor([-0.5120,  2.8979])\n",
      "Epoch: 17, Loss: 28.329973220825195 \n",
      "              Params: tensor([ 2.3644, -0.3038])\n",
      "              Grad: tensor([-0.5111,  2.8930])\n",
      "Epoch: 18, Loss: 28.243741989135742 \n",
      "              Params: tensor([ 2.3695, -0.3327])\n",
      "              Grad: tensor([-0.5102,  2.8881])\n",
      "Epoch: 19, Loss: 28.157804489135742 \n",
      "              Params: tensor([ 2.3746, -0.3615])\n",
      "              Grad: tensor([-0.5093,  2.8832])\n",
      "Epoch: 20, Loss: 28.07215118408203 \n",
      "              Params: tensor([ 2.3797, -0.3903])\n",
      "              Grad: tensor([-0.5084,  2.8783])\n",
      "Epoch: 21, Loss: 27.986797332763672 \n",
      "              Params: tensor([ 2.3848, -0.4190])\n",
      "              Grad: tensor([-0.5076,  2.8734])\n",
      "Epoch: 22, Loss: 27.9017276763916 \n",
      "              Params: tensor([ 2.3899, -0.4477])\n",
      "              Grad: tensor([-0.5067,  2.8685])\n",
      "Epoch: 23, Loss: 27.81694984436035 \n",
      "              Params: tensor([ 2.3949, -0.4763])\n",
      "              Grad: tensor([-0.5059,  2.8636])\n",
      "Epoch: 24, Loss: 27.732463836669922 \n",
      "              Params: tensor([ 2.4000, -0.5049])\n",
      "              Grad: tensor([-0.5050,  2.8588])\n",
      "Epoch: 25, Loss: 27.648256301879883 \n",
      "              Params: tensor([ 2.4050, -0.5335])\n",
      "              Grad: tensor([-0.5042,  2.8539])\n",
      "Epoch: 26, Loss: 27.56434440612793 \n",
      "              Params: tensor([ 2.4101, -0.5620])\n",
      "              Grad: tensor([-0.5033,  2.8490])\n",
      "Epoch: 27, Loss: 27.4807071685791 \n",
      "              Params: tensor([ 2.4151, -0.5904])\n",
      "              Grad: tensor([-0.5024,  2.8442])\n",
      "Epoch: 28, Loss: 27.397361755371094 \n",
      "              Params: tensor([ 2.4201, -0.6188])\n",
      "              Grad: tensor([-0.5016,  2.8394])\n",
      "Epoch: 29, Loss: 27.314294815063477 \n",
      "              Params: tensor([ 2.4251, -0.6471])\n",
      "              Grad: tensor([-0.5007,  2.8346])\n",
      "Epoch: 30, Loss: 27.23151206970215 \n",
      "              Params: tensor([ 2.4301, -0.6754])\n",
      "              Grad: tensor([-0.4999,  2.8297])\n",
      "Epoch: 31, Loss: 27.149009704589844 \n",
      "              Params: tensor([ 2.4351, -0.7037])\n",
      "              Grad: tensor([-0.4990,  2.8249])\n",
      "Epoch: 32, Loss: 27.066789627075195 \n",
      "              Params: tensor([ 2.4401, -0.7319])\n",
      "              Grad: tensor([-0.4982,  2.8201])\n",
      "Epoch: 33, Loss: 26.984844207763672 \n",
      "              Params: tensor([ 2.4450, -0.7600])\n",
      "              Grad: tensor([-0.4973,  2.8153])\n",
      "Epoch: 34, Loss: 26.903175354003906 \n",
      "              Params: tensor([ 2.4500, -0.7881])\n",
      "              Grad: tensor([-0.4965,  2.8106])\n",
      "Epoch: 35, Loss: 26.82179069519043 \n",
      "              Params: tensor([ 2.4550, -0.8162])\n",
      "              Grad: tensor([-0.4957,  2.8058])\n",
      "Epoch: 36, Loss: 26.740678787231445 \n",
      "              Params: tensor([ 2.4599, -0.8442])\n",
      "              Grad: tensor([-0.4948,  2.8010])\n",
      "Epoch: 37, Loss: 26.65983772277832 \n",
      "              Params: tensor([ 2.4649, -0.8722])\n",
      "              Grad: tensor([-0.4940,  2.7963])\n",
      "Epoch: 38, Loss: 26.57927894592285 \n",
      "              Params: tensor([ 2.4698, -0.9001])\n",
      "              Grad: tensor([-0.4931,  2.7915])\n",
      "Epoch: 39, Loss: 26.498987197875977 \n",
      "              Params: tensor([ 2.4747, -0.9280])\n",
      "              Grad: tensor([-0.4923,  2.7868])\n",
      "Epoch: 40, Loss: 26.418973922729492 \n",
      "              Params: tensor([ 2.4796, -0.9558])\n",
      "              Grad: tensor([-0.4915,  2.7820])\n",
      "Epoch: 41, Loss: 26.3392276763916 \n",
      "              Params: tensor([ 2.4845, -0.9836])\n",
      "              Grad: tensor([-0.4906,  2.7773])\n",
      "Epoch: 42, Loss: 26.259754180908203 \n",
      "              Params: tensor([ 2.4894, -1.0113])\n",
      "              Grad: tensor([-0.4898,  2.7726])\n",
      "Epoch: 43, Loss: 26.1805477142334 \n",
      "              Params: tensor([ 2.4943, -1.0390])\n",
      "              Grad: tensor([-0.4890,  2.7679])\n",
      "Epoch: 44, Loss: 26.10161590576172 \n",
      "              Params: tensor([ 2.4992, -1.0666])\n",
      "              Grad: tensor([-0.4881,  2.7632])\n",
      "Epoch: 45, Loss: 26.022947311401367 \n",
      "              Params: tensor([ 2.5041, -1.0942])\n",
      "              Grad: tensor([-0.4873,  2.7585])\n",
      "Epoch: 46, Loss: 25.944543838500977 \n",
      "              Params: tensor([ 2.5089, -1.1217])\n",
      "              Grad: tensor([-0.4865,  2.7538])\n",
      "Epoch: 47, Loss: 25.866416931152344 \n",
      "              Params: tensor([ 2.5138, -1.1492])\n",
      "              Grad: tensor([-0.4856,  2.7491])\n",
      "Epoch: 48, Loss: 25.788549423217773 \n",
      "              Params: tensor([ 2.5186, -1.1766])\n",
      "              Grad: tensor([-0.4848,  2.7444])\n",
      "Epoch: 49, Loss: 25.7109375 \n",
      "              Params: tensor([ 2.5235, -1.2040])\n",
      "              Grad: tensor([-0.4840,  2.7398])\n",
      "Epoch: 50, Loss: 25.63360023498535 \n",
      "              Params: tensor([ 2.5283, -1.2314])\n",
      "              Grad: tensor([-0.4832,  2.7351])\n",
      "Epoch: 51, Loss: 25.5565242767334 \n",
      "              Params: tensor([ 2.5331, -1.2587])\n",
      "              Grad: tensor([-0.4823,  2.7305])\n",
      "Epoch: 52, Loss: 25.479700088500977 \n",
      "              Params: tensor([ 2.5379, -1.2860])\n",
      "              Grad: tensor([-0.4815,  2.7258])\n",
      "Epoch: 53, Loss: 25.403148651123047 \n",
      "              Params: tensor([ 2.5428, -1.3132])\n",
      "              Grad: tensor([-0.4807,  2.7212])\n",
      "Epoch: 54, Loss: 25.32685089111328 \n",
      "              Params: tensor([ 2.5476, -1.3403])\n",
      "              Grad: tensor([-0.4799,  2.7166])\n",
      "Epoch: 55, Loss: 25.250810623168945 \n",
      "              Params: tensor([ 2.5523, -1.3675])\n",
      "              Grad: tensor([-0.4791,  2.7120])\n",
      "Epoch: 56, Loss: 25.17503547668457 \n",
      "              Params: tensor([ 2.5571, -1.3945])\n",
      "              Grad: tensor([-0.4783,  2.7074])\n",
      "Epoch: 57, Loss: 25.099512100219727 \n",
      "              Params: tensor([ 2.5619, -1.4216])\n",
      "              Grad: tensor([-0.4775,  2.7028])\n",
      "Epoch: 58, Loss: 25.024248123168945 \n",
      "              Params: tensor([ 2.5667, -1.4485])\n",
      "              Grad: tensor([-0.4766,  2.6982])\n",
      "Epoch: 59, Loss: 24.949235916137695 \n",
      "              Params: tensor([ 2.5714, -1.4755])\n",
      "              Grad: tensor([-0.4758,  2.6936])\n",
      "Epoch: 60, Loss: 24.874483108520508 \n",
      "              Params: tensor([ 2.5762, -1.5024])\n",
      "              Grad: tensor([-0.4750,  2.6890])\n",
      "Epoch: 61, Loss: 24.799976348876953 \n",
      "              Params: tensor([ 2.5809, -1.5292])\n",
      "              Grad: tensor([-0.4742,  2.6845])\n",
      "Epoch: 62, Loss: 24.725736618041992 \n",
      "              Params: tensor([ 2.5857, -1.5560])\n",
      "              Grad: tensor([-0.4734,  2.6799])\n",
      "Epoch: 63, Loss: 24.6517391204834 \n",
      "              Params: tensor([ 2.5904, -1.5828])\n",
      "              Grad: tensor([-0.4726,  2.6753])\n",
      "Epoch: 64, Loss: 24.577985763549805 \n",
      "              Params: tensor([ 2.5951, -1.6095])\n",
      "              Grad: tensor([-0.4718,  2.6708])\n",
      "Epoch: 65, Loss: 24.504493713378906 \n",
      "              Params: tensor([ 2.5998, -1.6361])\n",
      "              Grad: tensor([-0.4710,  2.6663])\n",
      "Epoch: 66, Loss: 24.431251525878906 \n",
      "              Params: tensor([ 2.6045, -1.6628])\n",
      "              Grad: tensor([-0.4702,  2.6617])\n",
      "Epoch: 67, Loss: 24.358257293701172 \n",
      "              Params: tensor([ 2.6092, -1.6893])\n",
      "              Grad: tensor([-0.4694,  2.6572])\n",
      "Epoch: 68, Loss: 24.285505294799805 \n",
      "              Params: tensor([ 2.6139, -1.7159])\n",
      "              Grad: tensor([-0.4686,  2.6527])\n",
      "Epoch: 69, Loss: 24.21299934387207 \n",
      "              Params: tensor([ 2.6186, -1.7423])\n",
      "              Grad: tensor([-0.4678,  2.6482])\n",
      "Epoch: 70, Loss: 24.1407413482666 \n",
      "              Params: tensor([ 2.6232, -1.7688])\n",
      "              Grad: tensor([-0.4670,  2.6437])\n",
      "Epoch: 71, Loss: 24.06873321533203 \n",
      "              Params: tensor([ 2.6279, -1.7952])\n",
      "              Grad: tensor([-0.4662,  2.6392])\n",
      "Epoch: 72, Loss: 23.996971130371094 \n",
      "              Params: tensor([ 2.6326, -1.8215])\n",
      "              Grad: tensor([-0.4654,  2.6347])\n",
      "Epoch: 73, Loss: 23.925445556640625 \n",
      "              Params: tensor([ 2.6372, -1.8478])\n",
      "              Grad: tensor([-0.4646,  2.6302])\n",
      "Epoch: 74, Loss: 23.854167938232422 \n",
      "              Params: tensor([ 2.6418, -1.8741])\n",
      "              Grad: tensor([-0.4638,  2.6258])\n",
      "Epoch: 75, Loss: 23.783124923706055 \n",
      "              Params: tensor([ 2.6465, -1.9003])\n",
      "              Grad: tensor([-0.4631,  2.6213])\n",
      "Epoch: 76, Loss: 23.71232795715332 \n",
      "              Params: tensor([ 2.6511, -1.9265])\n",
      "              Grad: tensor([-0.4623,  2.6169])\n",
      "Epoch: 77, Loss: 23.641773223876953 \n",
      "              Params: tensor([ 2.6557, -1.9526])\n",
      "              Grad: tensor([-0.4615,  2.6124])\n",
      "Epoch: 78, Loss: 23.571455001831055 \n",
      "              Params: tensor([ 2.6603, -1.9787])\n",
      "              Grad: tensor([-0.4607,  2.6080])\n",
      "Epoch: 79, Loss: 23.501379013061523 \n",
      "              Params: tensor([ 2.6649, -2.0047])\n",
      "              Grad: tensor([-0.4599,  2.6035])\n",
      "Epoch: 80, Loss: 23.431537628173828 \n",
      "              Params: tensor([ 2.6695, -2.0307])\n",
      "              Grad: tensor([-0.4591,  2.5991])\n",
      "Epoch: 81, Loss: 23.361936569213867 \n",
      "              Params: tensor([ 2.6741, -2.0566])\n",
      "              Grad: tensor([-0.4584,  2.5947])\n",
      "Epoch: 82, Loss: 23.292570114135742 \n",
      "              Params: tensor([ 2.6787, -2.0825])\n",
      "              Grad: tensor([-0.4576,  2.5903])\n",
      "Epoch: 83, Loss: 23.22343635559082 \n",
      "              Params: tensor([ 2.6832, -2.1084])\n",
      "              Grad: tensor([-0.4568,  2.5859])\n",
      "Epoch: 84, Loss: 23.154541015625 \n",
      "              Params: tensor([ 2.6878, -2.1342])\n",
      "              Grad: tensor([-0.4560,  2.5815])\n",
      "Epoch: 85, Loss: 23.08588218688965 \n",
      "              Params: tensor([ 2.6923, -2.1600])\n",
      "              Grad: tensor([-0.4553,  2.5771])\n",
      "Epoch: 86, Loss: 23.017446517944336 \n",
      "              Params: tensor([ 2.6969, -2.1857])\n",
      "              Grad: tensor([-0.4545,  2.5727])\n",
      "Epoch: 87, Loss: 22.949251174926758 \n",
      "              Params: tensor([ 2.7014, -2.2114])\n",
      "              Grad: tensor([-0.4537,  2.5684])\n",
      "Epoch: 88, Loss: 22.881282806396484 \n",
      "              Params: tensor([ 2.7060, -2.2370])\n",
      "              Grad: tensor([-0.4529,  2.5640])\n",
      "Epoch: 89, Loss: 22.813549041748047 \n",
      "              Params: tensor([ 2.7105, -2.2626])\n",
      "              Grad: tensor([-0.4522,  2.5597])\n",
      "Epoch: 90, Loss: 22.746044158935547 \n",
      "              Params: tensor([ 2.7150, -2.2882])\n",
      "              Grad: tensor([-0.4514,  2.5553])\n",
      "Epoch: 91, Loss: 22.67876625061035 \n",
      "              Params: tensor([ 2.7195, -2.3137])\n",
      "              Grad: tensor([-0.4506,  2.5510])\n",
      "Epoch: 92, Loss: 22.611717224121094 \n",
      "              Params: tensor([ 2.7240, -2.3392])\n",
      "              Grad: tensor([-0.4499,  2.5466])\n",
      "Epoch: 93, Loss: 22.544898986816406 \n",
      "              Params: tensor([ 2.7285, -2.3646])\n",
      "              Grad: tensor([-0.4491,  2.5423])\n",
      "Epoch: 94, Loss: 22.47830581665039 \n",
      "              Params: tensor([ 2.7330, -2.3900])\n",
      "              Grad: tensor([-0.4483,  2.5380])\n",
      "Epoch: 95, Loss: 22.41193389892578 \n",
      "              Params: tensor([ 2.7374, -2.4153])\n",
      "              Grad: tensor([-0.4476,  2.5337])\n",
      "Epoch: 96, Loss: 22.345792770385742 \n",
      "              Params: tensor([ 2.7419, -2.4406])\n",
      "              Grad: tensor([-0.4468,  2.5294])\n",
      "Epoch: 97, Loss: 22.279874801635742 \n",
      "              Params: tensor([ 2.7464, -2.4658])\n",
      "              Grad: tensor([-0.4461,  2.5251])\n",
      "Epoch: 98, Loss: 22.21418571472168 \n",
      "              Params: tensor([ 2.7508, -2.4910])\n",
      "              Grad: tensor([-0.4453,  2.5208])\n",
      "Epoch: 99, Loss: 22.148710250854492 \n",
      "              Params: tensor([ 2.7553, -2.5162])\n",
      "              Grad: tensor([-0.4446,  2.5165])\n",
      "Epoch: 100, Loss: 22.083463668823242 \n",
      "              Params: tensor([ 2.7597, -2.5413])\n",
      "              Grad: tensor([-0.4438,  2.5122])\n",
      "Epoch: 101, Loss: 22.018436431884766 \n",
      "              Params: tensor([ 2.7641, -2.5664])\n",
      "              Grad: tensor([-0.4430,  2.5080])\n",
      "Epoch: 102, Loss: 21.953632354736328 \n",
      "              Params: tensor([ 2.7686, -2.5914])\n",
      "              Grad: tensor([-0.4423,  2.5037])\n",
      "Epoch: 103, Loss: 21.88904571533203 \n",
      "              Params: tensor([ 2.7730, -2.6164])\n",
      "              Grad: tensor([-0.4415,  2.4994])\n",
      "Epoch: 104, Loss: 21.824676513671875 \n",
      "              Params: tensor([ 2.7774, -2.6414])\n",
      "              Grad: tensor([-0.4408,  2.4952])\n",
      "Epoch: 105, Loss: 21.760528564453125 \n",
      "              Params: tensor([ 2.7818, -2.6663])\n",
      "              Grad: tensor([-0.4400,  2.4910])\n",
      "Epoch: 106, Loss: 21.69659996032715 \n",
      "              Params: tensor([ 2.7862, -2.6912])\n",
      "              Grad: tensor([-0.4393,  2.4867])\n",
      "Epoch: 107, Loss: 21.632883071899414 \n",
      "              Params: tensor([ 2.7906, -2.7160])\n",
      "              Grad: tensor([-0.4385,  2.4825])\n",
      "Epoch: 108, Loss: 21.56938934326172 \n",
      "              Params: tensor([ 2.7949, -2.7408])\n",
      "              Grad: tensor([-0.4378,  2.4783])\n",
      "Epoch: 109, Loss: 21.506101608276367 \n",
      "              Params: tensor([ 2.7993, -2.7655])\n",
      "              Grad: tensor([-0.4370,  2.4741])\n",
      "Epoch: 110, Loss: 21.443037033081055 \n",
      "              Params: tensor([ 2.8037, -2.7902])\n",
      "              Grad: tensor([-0.4363,  2.4699])\n",
      "Epoch: 111, Loss: 21.380186080932617 \n",
      "              Params: tensor([ 2.8080, -2.8149])\n",
      "              Grad: tensor([-0.4356,  2.4657])\n",
      "Epoch: 112, Loss: 21.317548751831055 \n",
      "              Params: tensor([ 2.8124, -2.8395])\n",
      "              Grad: tensor([-0.4348,  2.4615])\n",
      "Epoch: 113, Loss: 21.255117416381836 \n",
      "              Params: tensor([ 2.8167, -2.8641])\n",
      "              Grad: tensor([-0.4341,  2.4573])\n",
      "Epoch: 114, Loss: 21.192907333374023 \n",
      "              Params: tensor([ 2.8211, -2.8886])\n",
      "              Grad: tensor([-0.4334,  2.4531])\n",
      "Epoch: 115, Loss: 21.130897521972656 \n",
      "              Params: tensor([ 2.8254, -2.9131])\n",
      "              Grad: tensor([-0.4326,  2.4490])\n",
      "Epoch: 116, Loss: 21.06910514831543 \n",
      "              Params: tensor([ 2.8297, -2.9375])\n",
      "              Grad: tensor([-0.4319,  2.4448])\n",
      "Epoch: 117, Loss: 21.007526397705078 \n",
      "              Params: tensor([ 2.8340, -2.9619])\n",
      "              Grad: tensor([-0.4311,  2.4407])\n",
      "Epoch: 118, Loss: 20.946149826049805 \n",
      "              Params: tensor([ 2.8383, -2.9863])\n",
      "              Grad: tensor([-0.4304,  2.4365])\n",
      "Epoch: 119, Loss: 20.884981155395508 \n",
      "              Params: tensor([ 2.8426, -3.0106])\n",
      "              Grad: tensor([-0.4297,  2.4324])\n",
      "Epoch: 120, Loss: 20.824024200439453 \n",
      "              Params: tensor([ 2.8469, -3.0349])\n",
      "              Grad: tensor([-0.4290,  2.4282])\n",
      "Epoch: 121, Loss: 20.763273239135742 \n",
      "              Params: tensor([ 2.8512, -3.0592])\n",
      "              Grad: tensor([-0.4282,  2.4241])\n",
      "Epoch: 122, Loss: 20.702728271484375 \n",
      "              Params: tensor([ 2.8555, -3.0834])\n",
      "              Grad: tensor([-0.4275,  2.4200])\n",
      "Epoch: 123, Loss: 20.642383575439453 \n",
      "              Params: tensor([ 2.8597, -3.1075])\n",
      "              Grad: tensor([-0.4268,  2.4159])\n",
      "Epoch: 124, Loss: 20.58224868774414 \n",
      "              Params: tensor([ 2.8640, -3.1316])\n",
      "              Grad: tensor([-0.4260,  2.4118])\n",
      "Epoch: 125, Loss: 20.522321701049805 \n",
      "              Params: tensor([ 2.8682, -3.1557])\n",
      "              Grad: tensor([-0.4253,  2.4077])\n",
      "Epoch: 126, Loss: 20.46259307861328 \n",
      "              Params: tensor([ 2.8725, -3.1797])\n",
      "              Grad: tensor([-0.4246,  2.4036])\n",
      "Epoch: 127, Loss: 20.40306854248047 \n",
      "              Params: tensor([ 2.8767, -3.2037])\n",
      "              Grad: tensor([-0.4239,  2.3995])\n",
      "Epoch: 128, Loss: 20.34374237060547 \n",
      "              Params: tensor([ 2.8810, -3.2277])\n",
      "              Grad: tensor([-0.4232,  2.3954])\n",
      "Epoch: 129, Loss: 20.284624099731445 \n",
      "              Params: tensor([ 2.8852, -3.2516])\n",
      "              Grad: tensor([-0.4224,  2.3914])\n",
      "Epoch: 130, Loss: 20.2257022857666 \n",
      "              Params: tensor([ 2.8894, -3.2755])\n",
      "              Grad: tensor([-0.4217,  2.3873])\n",
      "Epoch: 131, Loss: 20.166980743408203 \n",
      "              Params: tensor([ 2.8936, -3.2993])\n",
      "              Grad: tensor([-0.4210,  2.3832])\n",
      "Epoch: 132, Loss: 20.108461380004883 \n",
      "              Params: tensor([ 2.8978, -3.3231])\n",
      "              Grad: tensor([-0.4203,  2.3792])\n",
      "Epoch: 133, Loss: 20.05013656616211 \n",
      "              Params: tensor([ 2.9020, -3.3469])\n",
      "              Grad: tensor([-0.4196,  2.3752])\n",
      "Epoch: 134, Loss: 19.992015838623047 \n",
      "              Params: tensor([ 2.9062, -3.3706])\n",
      "              Grad: tensor([-0.4189,  2.3711])\n",
      "Epoch: 135, Loss: 19.934085845947266 \n",
      "              Params: tensor([ 2.9104, -3.3942])\n",
      "              Grad: tensor([-0.4182,  2.3671])\n",
      "Epoch: 136, Loss: 19.876352310180664 \n",
      "              Params: tensor([ 2.9146, -3.4179])\n",
      "              Grad: tensor([-0.4174,  2.3631])\n",
      "Epoch: 137, Loss: 19.818822860717773 \n",
      "              Params: tensor([ 2.9187, -3.4415])\n",
      "              Grad: tensor([-0.4167,  2.3591])\n",
      "Epoch: 138, Loss: 19.7614803314209 \n",
      "              Params: tensor([ 2.9229, -3.4650])\n",
      "              Grad: tensor([-0.4160,  2.3550])\n",
      "Epoch: 139, Loss: 19.704336166381836 \n",
      "              Params: tensor([ 2.9270, -3.4885])\n",
      "              Grad: tensor([-0.4153,  2.3510])\n",
      "Epoch: 140, Loss: 19.647384643554688 \n",
      "              Params: tensor([ 2.9312, -3.5120])\n",
      "              Grad: tensor([-0.4146,  2.3471])\n",
      "Epoch: 141, Loss: 19.590625762939453 \n",
      "              Params: tensor([ 2.9353, -3.5354])\n",
      "              Grad: tensor([-0.4139,  2.3431])\n",
      "Epoch: 142, Loss: 19.534061431884766 \n",
      "              Params: tensor([ 2.9395, -3.5588])\n",
      "              Grad: tensor([-0.4132,  2.3391])\n",
      "Epoch: 143, Loss: 19.477689743041992 \n",
      "              Params: tensor([ 2.9436, -3.5822])\n",
      "              Grad: tensor([-0.4125,  2.3351])\n",
      "Epoch: 144, Loss: 19.421506881713867 \n",
      "              Params: tensor([ 2.9477, -3.6055])\n",
      "              Grad: tensor([-0.4118,  2.3311])\n",
      "Epoch: 145, Loss: 19.365514755249023 \n",
      "              Params: tensor([ 2.9518, -3.6287])\n",
      "              Grad: tensor([-0.4111,  2.3272])\n",
      "Epoch: 146, Loss: 19.309715270996094 \n",
      "              Params: tensor([ 2.9559, -3.6520])\n",
      "              Grad: tensor([-0.4104,  2.3232])\n",
      "Epoch: 147, Loss: 19.254106521606445 \n",
      "              Params: tensor([ 2.9600, -3.6752])\n",
      "              Grad: tensor([-0.4097,  2.3193])\n",
      "Epoch: 148, Loss: 19.198684692382812 \n",
      "              Params: tensor([ 2.9641, -3.6983])\n",
      "              Grad: tensor([-0.4090,  2.3153])\n",
      "Epoch: 149, Loss: 19.14344596862793 \n",
      "              Params: tensor([ 2.9682, -3.7214])\n",
      "              Grad: tensor([-0.4083,  2.3114])\n",
      "Epoch: 150, Loss: 19.088401794433594 \n",
      "              Params: tensor([ 2.9723, -3.7445])\n",
      "              Grad: tensor([-0.4076,  2.3075])\n",
      "Epoch: 151, Loss: 19.03354263305664 \n",
      "              Params: tensor([ 2.9763, -3.7675])\n",
      "              Grad: tensor([-0.4069,  2.3036])\n",
      "Epoch: 152, Loss: 18.97886848449707 \n",
      "              Params: tensor([ 2.9804, -3.7905])\n",
      "              Grad: tensor([-0.4062,  2.2997])\n",
      "Epoch: 153, Loss: 18.92437744140625 \n",
      "              Params: tensor([ 2.9844, -3.8135])\n",
      "              Grad: tensor([-0.4056,  2.2957])\n",
      "Epoch: 154, Loss: 18.870080947875977 \n",
      "              Params: tensor([ 2.9885, -3.8364])\n",
      "              Grad: tensor([-0.4049,  2.2918])\n",
      "Epoch: 155, Loss: 18.815959930419922 \n",
      "              Params: tensor([ 2.9925, -3.8593])\n",
      "              Grad: tensor([-0.4042,  2.2880])\n",
      "Epoch: 156, Loss: 18.762022018432617 \n",
      "              Params: tensor([ 2.9966, -3.8821])\n",
      "              Grad: tensor([-0.4035,  2.2841])\n",
      "Epoch: 157, Loss: 18.708271026611328 \n",
      "              Params: tensor([ 3.0006, -3.9049])\n",
      "              Grad: tensor([-0.4028,  2.2802])\n",
      "Epoch: 158, Loss: 18.654699325561523 \n",
      "              Params: tensor([ 3.0046, -3.9277])\n",
      "              Grad: tensor([-0.4021,  2.2763])\n",
      "Epoch: 159, Loss: 18.6013126373291 \n",
      "              Params: tensor([ 3.0086, -3.9504])\n",
      "              Grad: tensor([-0.4014,  2.2724])\n",
      "Epoch: 160, Loss: 18.54810905456543 \n",
      "              Params: tensor([ 3.0126, -3.9731])\n",
      "              Grad: tensor([-0.4007,  2.2686])\n",
      "Epoch: 161, Loss: 18.495084762573242 \n",
      "              Params: tensor([ 3.0166, -3.9958])\n",
      "              Grad: tensor([-0.4001,  2.2647])\n",
      "Epoch: 162, Loss: 18.442235946655273 \n",
      "              Params: tensor([ 3.0206, -4.0184])\n",
      "              Grad: tensor([-0.3994,  2.2609])\n",
      "Epoch: 163, Loss: 18.389570236206055 \n",
      "              Params: tensor([ 3.0246, -4.0409])\n",
      "              Grad: tensor([-0.3987,  2.2570])\n",
      "Epoch: 164, Loss: 18.337080001831055 \n",
      "              Params: tensor([ 3.0286, -4.0635])\n",
      "              Grad: tensor([-0.3980,  2.2532])\n",
      "Epoch: 165, Loss: 18.28477668762207 \n",
      "              Params: tensor([ 3.0326, -4.0860])\n",
      "              Grad: tensor([-0.3974,  2.2494])\n",
      "Epoch: 166, Loss: 18.232641220092773 \n",
      "              Params: tensor([ 3.0365, -4.1084])\n",
      "              Grad: tensor([-0.3967,  2.2456])\n",
      "Epoch: 167, Loss: 18.18068504333496 \n",
      "              Params: tensor([ 3.0405, -4.1308])\n",
      "              Grad: tensor([-0.3960,  2.2417])\n",
      "Epoch: 168, Loss: 18.12890625 \n",
      "              Params: tensor([ 3.0445, -4.1532])\n",
      "              Grad: tensor([-0.3953,  2.2379])\n",
      "Epoch: 169, Loss: 18.077301025390625 \n",
      "              Params: tensor([ 3.0484, -4.1756])\n",
      "              Grad: tensor([-0.3947,  2.2341])\n",
      "Epoch: 170, Loss: 18.025876998901367 \n",
      "              Params: tensor([ 3.0523, -4.1979])\n",
      "              Grad: tensor([-0.3940,  2.2303])\n",
      "Epoch: 171, Loss: 17.97462272644043 \n",
      "              Params: tensor([ 3.0563, -4.2201])\n",
      "              Grad: tensor([-0.3933,  2.2266])\n",
      "Epoch: 172, Loss: 17.923545837402344 \n",
      "              Params: tensor([ 3.0602, -4.2424])\n",
      "              Grad: tensor([-0.3927,  2.2228])\n",
      "Epoch: 173, Loss: 17.872642517089844 \n",
      "              Params: tensor([ 3.0641, -4.2646])\n",
      "              Grad: tensor([-0.3920,  2.2190])\n",
      "Epoch: 174, Loss: 17.821908950805664 \n",
      "              Params: tensor([ 3.0680, -4.2867])\n",
      "              Grad: tensor([-0.3913,  2.2152])\n",
      "Epoch: 175, Loss: 17.771345138549805 \n",
      "              Params: tensor([ 3.0719, -4.3088])\n",
      "              Grad: tensor([-0.3907,  2.2115])\n",
      "Epoch: 176, Loss: 17.72095489501953 \n",
      "              Params: tensor([ 3.0758, -4.3309])\n",
      "              Grad: tensor([-0.3900,  2.2077])\n",
      "Epoch: 177, Loss: 17.670738220214844 \n",
      "              Params: tensor([ 3.0797, -4.3529])\n",
      "              Grad: tensor([-0.3893,  2.2040])\n",
      "Epoch: 178, Loss: 17.620689392089844 \n",
      "              Params: tensor([ 3.0836, -4.3749])\n",
      "              Grad: tensor([-0.3887,  2.2002])\n",
      "Epoch: 179, Loss: 17.57081413269043 \n",
      "              Params: tensor([ 3.0875, -4.3969])\n",
      "              Grad: tensor([-0.3880,  2.1965])\n",
      "Epoch: 180, Loss: 17.521102905273438 \n",
      "              Params: tensor([ 3.0914, -4.4188])\n",
      "              Grad: tensor([-0.3873,  2.1927])\n",
      "Epoch: 181, Loss: 17.47156524658203 \n",
      "              Params: tensor([ 3.0952, -4.4407])\n",
      "              Grad: tensor([-0.3867,  2.1890])\n",
      "Epoch: 182, Loss: 17.422191619873047 \n",
      "              Params: tensor([ 3.0991, -4.4626])\n",
      "              Grad: tensor([-0.3860,  2.1853])\n",
      "Epoch: 183, Loss: 17.37299346923828 \n",
      "              Params: tensor([ 3.1030, -4.4844])\n",
      "              Grad: tensor([-0.3854,  2.1816])\n",
      "Epoch: 184, Loss: 17.32395362854004 \n",
      "              Params: tensor([ 3.1068, -4.5062])\n",
      "              Grad: tensor([-0.3847,  2.1779])\n",
      "Epoch: 185, Loss: 17.275083541870117 \n",
      "              Params: tensor([ 3.1106, -4.5279])\n",
      "              Grad: tensor([-0.3841,  2.1742])\n",
      "Epoch: 186, Loss: 17.22637939453125 \n",
      "              Params: tensor([ 3.1145, -4.5496])\n",
      "              Grad: tensor([-0.3834,  2.1705])\n",
      "Epoch: 187, Loss: 17.177839279174805 \n",
      "              Params: tensor([ 3.1183, -4.5713])\n",
      "              Grad: tensor([-0.3828,  2.1668])\n",
      "Epoch: 188, Loss: 17.12946319580078 \n",
      "              Params: tensor([ 3.1221, -4.5929])\n",
      "              Grad: tensor([-0.3821,  2.1631])\n",
      "Epoch: 189, Loss: 17.081254959106445 \n",
      "              Params: tensor([ 3.1259, -4.6145])\n",
      "              Grad: tensor([-0.3815,  2.1594])\n",
      "Epoch: 190, Loss: 17.0332088470459 \n",
      "              Params: tensor([ 3.1298, -4.6361])\n",
      "              Grad: tensor([-0.3808,  2.1558])\n",
      "Epoch: 191, Loss: 16.985326766967773 \n",
      "              Params: tensor([ 3.1336, -4.6576])\n",
      "              Grad: tensor([-0.3802,  2.1521])\n",
      "Epoch: 192, Loss: 16.937604904174805 \n",
      "              Params: tensor([ 3.1374, -4.6791])\n",
      "              Grad: tensor([-0.3795,  2.1485])\n",
      "Epoch: 193, Loss: 16.890047073364258 \n",
      "              Params: tensor([ 3.1411, -4.7005])\n",
      "              Grad: tensor([-0.3789,  2.1448])\n",
      "Epoch: 194, Loss: 16.842649459838867 \n",
      "              Params: tensor([ 3.1449, -4.7219])\n",
      "              Grad: tensor([-0.3782,  2.1412])\n",
      "Epoch: 195, Loss: 16.795412063598633 \n",
      "              Params: tensor([ 3.1487, -4.7433])\n",
      "              Grad: tensor([-0.3776,  2.1375])\n",
      "Epoch: 196, Loss: 16.74833869934082 \n",
      "              Params: tensor([ 3.1525, -4.7646])\n",
      "              Grad: tensor([-0.3770,  2.1339])\n",
      "Epoch: 197, Loss: 16.7014217376709 \n",
      "              Params: tensor([ 3.1562, -4.7859])\n",
      "              Grad: tensor([-0.3763,  2.1303])\n",
      "Epoch: 198, Loss: 16.654661178588867 \n",
      "              Params: tensor([ 3.1600, -4.8072])\n",
      "              Grad: tensor([-0.3757,  2.1267])\n",
      "Epoch: 199, Loss: 16.60806655883789 \n",
      "              Params: tensor([ 3.1637, -4.8284])\n",
      "              Grad: tensor([-0.3750,  2.1230])\n",
      "Epoch: 200, Loss: 16.561622619628906 \n",
      "              Params: tensor([ 3.1675, -4.8496])\n",
      "              Grad: tensor([-0.3744,  2.1194])\n",
      "Epoch: 201, Loss: 16.515342712402344 \n",
      "              Params: tensor([ 3.1712, -4.8708])\n",
      "              Grad: tensor([-0.3738,  2.1158])\n",
      "Epoch: 202, Loss: 16.469219207763672 \n",
      "              Params: tensor([ 3.1750, -4.8919])\n",
      "              Grad: tensor([-0.3731,  2.1122])\n",
      "Epoch: 203, Loss: 16.423248291015625 \n",
      "              Params: tensor([ 3.1787, -4.9130])\n",
      "              Grad: tensor([-0.3725,  2.1087])\n",
      "Epoch: 204, Loss: 16.37743377685547 \n",
      "              Params: tensor([ 3.1824, -4.9341])\n",
      "              Grad: tensor([-0.3719,  2.1051])\n",
      "Epoch: 205, Loss: 16.331775665283203 \n",
      "              Params: tensor([ 3.1861, -4.9551])\n",
      "              Grad: tensor([-0.3712,  2.1015])\n",
      "Epoch: 206, Loss: 16.28627586364746 \n",
      "              Params: tensor([ 3.1898, -4.9760])\n",
      "              Grad: tensor([-0.3706,  2.0979])\n",
      "Epoch: 207, Loss: 16.240928649902344 \n",
      "              Params: tensor([ 3.1935, -4.9970])\n",
      "              Grad: tensor([-0.3700,  2.0944])\n",
      "Epoch: 208, Loss: 16.19573211669922 \n",
      "              Params: tensor([ 3.1972, -5.0179])\n",
      "              Grad: tensor([-0.3694,  2.0908])\n",
      "Epoch: 209, Loss: 16.150693893432617 \n",
      "              Params: tensor([ 3.2009, -5.0388])\n",
      "              Grad: tensor([-0.3687,  2.0873])\n",
      "Epoch: 210, Loss: 16.105806350708008 \n",
      "              Params: tensor([ 3.2046, -5.0596])\n",
      "              Grad: tensor([-0.3681,  2.0837])\n",
      "Epoch: 211, Loss: 16.061073303222656 \n",
      "              Params: tensor([ 3.2082, -5.0804])\n",
      "              Grad: tensor([-0.3675,  2.0802])\n",
      "Epoch: 212, Loss: 16.01648712158203 \n",
      "              Params: tensor([ 3.2119, -5.1012])\n",
      "              Grad: tensor([-0.3668,  2.0766])\n",
      "Epoch: 213, Loss: 15.972058296203613 \n",
      "              Params: tensor([ 3.2156, -5.1219])\n",
      "              Grad: tensor([-0.3662,  2.0731])\n",
      "Epoch: 214, Loss: 15.927776336669922 \n",
      "              Params: tensor([ 3.2192, -5.1426])\n",
      "              Grad: tensor([-0.3656,  2.0696])\n",
      "Epoch: 215, Loss: 15.883645057678223 \n",
      "              Params: tensor([ 3.2229, -5.1633])\n",
      "              Grad: tensor([-0.3650,  2.0661])\n",
      "Epoch: 216, Loss: 15.8396635055542 \n",
      "              Params: tensor([ 3.2265, -5.1839])\n",
      "              Grad: tensor([-0.3644,  2.0626])\n",
      "Epoch: 217, Loss: 15.795831680297852 \n",
      "              Params: tensor([ 3.2302, -5.2045])\n",
      "              Grad: tensor([-0.3637,  2.0591])\n",
      "Epoch: 218, Loss: 15.752152442932129 \n",
      "              Params: tensor([ 3.2338, -5.2250])\n",
      "              Grad: tensor([-0.3631,  2.0556])\n",
      "Epoch: 219, Loss: 15.708612442016602 \n",
      "              Params: tensor([ 3.2374, -5.2456])\n",
      "              Grad: tensor([-0.3625,  2.0521])\n",
      "Epoch: 220, Loss: 15.665225982666016 \n",
      "              Params: tensor([ 3.2410, -5.2660])\n",
      "              Grad: tensor([-0.3619,  2.0486])\n",
      "Epoch: 221, Loss: 15.621990203857422 \n",
      "              Params: tensor([ 3.2447, -5.2865])\n",
      "              Grad: tensor([-0.3613,  2.0451])\n",
      "Epoch: 222, Loss: 15.578896522521973 \n",
      "              Params: tensor([ 3.2483, -5.3069])\n",
      "              Grad: tensor([-0.3607,  2.0416])\n",
      "Epoch: 223, Loss: 15.53594970703125 \n",
      "              Params: tensor([ 3.2519, -5.3273])\n",
      "              Grad: tensor([-0.3601,  2.0382])\n",
      "Epoch: 224, Loss: 15.493149757385254 \n",
      "              Params: tensor([ 3.2555, -5.3476])\n",
      "              Grad: tensor([-0.3594,  2.0347])\n",
      "Epoch: 225, Loss: 15.450494766235352 \n",
      "              Params: tensor([ 3.2590, -5.3680])\n",
      "              Grad: tensor([-0.3588,  2.0312])\n",
      "Epoch: 226, Loss: 15.407980918884277 \n",
      "              Params: tensor([ 3.2626, -5.3882])\n",
      "              Grad: tensor([-0.3582,  2.0278])\n",
      "Epoch: 227, Loss: 15.365615844726562 \n",
      "              Params: tensor([ 3.2662, -5.4085])\n",
      "              Grad: tensor([-0.3576,  2.0243])\n",
      "Epoch: 228, Loss: 15.323395729064941 \n",
      "              Params: tensor([ 3.2698, -5.4287])\n",
      "              Grad: tensor([-0.3570,  2.0209])\n",
      "Epoch: 229, Loss: 15.281316757202148 \n",
      "              Params: tensor([ 3.2733, -5.4489])\n",
      "              Grad: tensor([-0.3564,  2.0175])\n",
      "Epoch: 230, Loss: 15.2393798828125 \n",
      "              Params: tensor([ 3.2769, -5.4690])\n",
      "              Grad: tensor([-0.3558,  2.0140])\n",
      "Epoch: 231, Loss: 15.197585105895996 \n",
      "              Params: tensor([ 3.2804, -5.4891])\n",
      "              Grad: tensor([-0.3552,  2.0106])\n",
      "Epoch: 232, Loss: 15.155932426452637 \n",
      "              Params: tensor([ 3.2840, -5.5092])\n",
      "              Grad: tensor([-0.3546,  2.0072])\n",
      "Epoch: 233, Loss: 15.114424705505371 \n",
      "              Params: tensor([ 3.2875, -5.5292])\n",
      "              Grad: tensor([-0.3540,  2.0038])\n",
      "Epoch: 234, Loss: 15.073055267333984 \n",
      "              Params: tensor([ 3.2911, -5.5492])\n",
      "              Grad: tensor([-0.3534,  2.0004])\n",
      "Epoch: 235, Loss: 15.03182315826416 \n",
      "              Params: tensor([ 3.2946, -5.5692])\n",
      "              Grad: tensor([-0.3528,  1.9970])\n",
      "Epoch: 236, Loss: 14.990734100341797 \n",
      "              Params: tensor([ 3.2981, -5.5891])\n",
      "              Grad: tensor([-0.3522,  1.9936])\n",
      "Epoch: 237, Loss: 14.949784278869629 \n",
      "              Params: tensor([ 3.3016, -5.6090])\n",
      "              Grad: tensor([-0.3516,  1.9902])\n",
      "Epoch: 238, Loss: 14.90897274017334 \n",
      "              Params: tensor([ 3.3051, -5.6289])\n",
      "              Grad: tensor([-0.3510,  1.9868])\n",
      "Epoch: 239, Loss: 14.868304252624512 \n",
      "              Params: tensor([ 3.3086, -5.6487])\n",
      "              Grad: tensor([-0.3504,  1.9835])\n",
      "Epoch: 240, Loss: 14.827767372131348 \n",
      "              Params: tensor([ 3.3121, -5.6685])\n",
      "              Grad: tensor([-0.3498,  1.9801])\n",
      "Epoch: 241, Loss: 14.787369728088379 \n",
      "              Params: tensor([ 3.3156, -5.6883])\n",
      "              Grad: tensor([-0.3492,  1.9767])\n",
      "Epoch: 242, Loss: 14.747109413146973 \n",
      "              Params: tensor([ 3.3191, -5.7080])\n",
      "              Grad: tensor([-0.3486,  1.9734])\n",
      "Epoch: 243, Loss: 14.706989288330078 \n",
      "              Params: tensor([ 3.3226, -5.7277])\n",
      "              Grad: tensor([-0.3480,  1.9700])\n",
      "Epoch: 244, Loss: 14.667001724243164 \n",
      "              Params: tensor([ 3.3261, -5.7474])\n",
      "              Grad: tensor([-0.3474,  1.9667])\n",
      "Epoch: 245, Loss: 14.627150535583496 \n",
      "              Params: tensor([ 3.3295, -5.7670])\n",
      "              Grad: tensor([-0.3468,  1.9633])\n",
      "Epoch: 246, Loss: 14.587435722351074 \n",
      "              Params: tensor([ 3.3330, -5.7866])\n",
      "              Grad: tensor([-0.3462,  1.9600])\n",
      "Epoch: 247, Loss: 14.547855377197266 \n",
      "              Params: tensor([ 3.3365, -5.8062])\n",
      "              Grad: tensor([-0.3456,  1.9567])\n",
      "Epoch: 248, Loss: 14.508408546447754 \n",
      "              Params: tensor([ 3.3399, -5.8257])\n",
      "              Grad: tensor([-0.3451,  1.9533])\n",
      "Epoch: 249, Loss: 14.469097137451172 \n",
      "              Params: tensor([ 3.3434, -5.8452])\n",
      "              Grad: tensor([-0.3445,  1.9500])\n",
      "Epoch: 250, Loss: 14.429920196533203 \n",
      "              Params: tensor([ 3.3468, -5.8647])\n",
      "              Grad: tensor([-0.3439,  1.9467])\n",
      "Epoch: 251, Loss: 14.390870094299316 \n",
      "              Params: tensor([ 3.3502, -5.8841])\n",
      "              Grad: tensor([-0.3433,  1.9434])\n",
      "Epoch: 252, Loss: 14.351956367492676 \n",
      "              Params: tensor([ 3.3537, -5.9035])\n",
      "              Grad: tensor([-0.3427,  1.9401])\n",
      "Epoch: 253, Loss: 14.313177108764648 \n",
      "              Params: tensor([ 3.3571, -5.9229])\n",
      "              Grad: tensor([-0.3421,  1.9368])\n",
      "Epoch: 254, Loss: 14.274529457092285 \n",
      "              Params: tensor([ 3.3605, -5.9422])\n",
      "              Grad: tensor([-0.3416,  1.9335])\n",
      "Epoch: 255, Loss: 14.236008644104004 \n",
      "              Params: tensor([ 3.3639, -5.9615])\n",
      "              Grad: tensor([-0.3410,  1.9302])\n",
      "Epoch: 256, Loss: 14.197620391845703 \n",
      "              Params: tensor([ 3.3673, -5.9808])\n",
      "              Grad: tensor([-0.3404,  1.9269])\n",
      "Epoch: 257, Loss: 14.15936279296875 \n",
      "              Params: tensor([ 3.3707, -6.0000])\n",
      "              Grad: tensor([-0.3398,  1.9237])\n",
      "Epoch: 258, Loss: 14.121233940124512 \n",
      "              Params: tensor([ 3.3741, -6.0192])\n",
      "              Grad: tensor([-0.3392,  1.9204])\n",
      "Epoch: 259, Loss: 14.083235740661621 \n",
      "              Params: tensor([ 3.3775, -6.0384])\n",
      "              Grad: tensor([-0.3387,  1.9171])\n",
      "Epoch: 260, Loss: 14.045367240905762 \n",
      "              Params: tensor([ 3.3809, -6.0576])\n",
      "              Grad: tensor([-0.3381,  1.9139])\n",
      "Epoch: 261, Loss: 14.0076265335083 \n",
      "              Params: tensor([ 3.3842, -6.0767])\n",
      "              Grad: tensor([-0.3375,  1.9106])\n",
      "Epoch: 262, Loss: 13.970015525817871 \n",
      "              Params: tensor([ 3.3876, -6.0957])\n",
      "              Grad: tensor([-0.3369,  1.9074])\n",
      "Epoch: 263, Loss: 13.932531356811523 \n",
      "              Params: tensor([ 3.3910, -6.1148])\n",
      "              Grad: tensor([-0.3364,  1.9041])\n",
      "Epoch: 264, Loss: 13.895172119140625 \n",
      "              Params: tensor([ 3.3943, -6.1338])\n",
      "              Grad: tensor([-0.3358,  1.9009])\n",
      "Epoch: 265, Loss: 13.857943534851074 \n",
      "              Params: tensor([ 3.3977, -6.1528])\n",
      "              Grad: tensor([-0.3352,  1.8977])\n",
      "Epoch: 266, Loss: 13.820837020874023 \n",
      "              Params: tensor([ 3.4010, -6.1717])\n",
      "              Grad: tensor([-0.3347,  1.8945])\n",
      "Epoch: 267, Loss: 13.783858299255371 \n",
      "              Params: tensor([ 3.4044, -6.1906])\n",
      "              Grad: tensor([-0.3341,  1.8912])\n",
      "Epoch: 268, Loss: 13.7470064163208 \n",
      "              Params: tensor([ 3.4077, -6.2095])\n",
      "              Grad: tensor([-0.3335,  1.8880])\n",
      "Epoch: 269, Loss: 13.710277557373047 \n",
      "              Params: tensor([ 3.4110, -6.2284])\n",
      "              Grad: tensor([-0.3330,  1.8848])\n",
      "Epoch: 270, Loss: 13.673675537109375 \n",
      "              Params: tensor([ 3.4144, -6.2472])\n",
      "              Grad: tensor([-0.3324,  1.8816])\n",
      "Epoch: 271, Loss: 13.637195587158203 \n",
      "              Params: tensor([ 3.4177, -6.2660])\n",
      "              Grad: tensor([-0.3318,  1.8784])\n",
      "Epoch: 272, Loss: 13.600841522216797 \n",
      "              Params: tensor([ 3.4210, -6.2847])\n",
      "              Grad: tensor([-0.3313,  1.8752])\n",
      "Epoch: 273, Loss: 13.564608573913574 \n",
      "              Params: tensor([ 3.4243, -6.3034])\n",
      "              Grad: tensor([-0.3307,  1.8720])\n",
      "Epoch: 274, Loss: 13.5285005569458 \n",
      "              Params: tensor([ 3.4276, -6.3221])\n",
      "              Grad: tensor([-0.3301,  1.8689])\n",
      "Epoch: 275, Loss: 13.492513656616211 \n",
      "              Params: tensor([ 3.4309, -6.3408])\n",
      "              Grad: tensor([-0.3296,  1.8657])\n",
      "Epoch: 276, Loss: 13.456650733947754 \n",
      "              Params: tensor([ 3.4342, -6.3594])\n",
      "              Grad: tensor([-0.3290,  1.8625])\n",
      "Epoch: 277, Loss: 13.420909881591797 \n",
      "              Params: tensor([ 3.4375, -6.3780])\n",
      "              Grad: tensor([-0.3285,  1.8594])\n",
      "Epoch: 278, Loss: 13.385287284851074 \n",
      "              Params: tensor([ 3.4407, -6.3966])\n",
      "              Grad: tensor([-0.3279,  1.8562])\n",
      "Epoch: 279, Loss: 13.349788665771484 \n",
      "              Params: tensor([ 3.4440, -6.4151])\n",
      "              Grad: tensor([-0.3274,  1.8530])\n",
      "Epoch: 280, Loss: 13.314407348632812 \n",
      "              Params: tensor([ 3.4473, -6.4336])\n",
      "              Grad: tensor([-0.3268,  1.8499])\n",
      "Epoch: 281, Loss: 13.279150009155273 \n",
      "              Params: tensor([ 3.4506, -6.4520])\n",
      "              Grad: tensor([-0.3262,  1.8468])\n",
      "Epoch: 282, Loss: 13.244009017944336 \n",
      "              Params: tensor([ 3.4538, -6.4705])\n",
      "              Grad: tensor([-0.3257,  1.8436])\n",
      "Epoch: 283, Loss: 13.208991050720215 \n",
      "              Params: tensor([ 3.4571, -6.4889])\n",
      "              Grad: tensor([-0.3251,  1.8405])\n",
      "Epoch: 284, Loss: 13.174088478088379 \n",
      "              Params: tensor([ 3.4603, -6.5073])\n",
      "              Grad: tensor([-0.3246,  1.8374])\n",
      "Epoch: 285, Loss: 13.139307022094727 \n",
      "              Params: tensor([ 3.4635, -6.5256])\n",
      "              Grad: tensor([-0.3240,  1.8342])\n",
      "Epoch: 286, Loss: 13.104639053344727 \n",
      "              Params: tensor([ 3.4668, -6.5439])\n",
      "              Grad: tensor([-0.3235,  1.8311])\n",
      "Epoch: 287, Loss: 13.07009220123291 \n",
      "              Params: tensor([ 3.4700, -6.5622])\n",
      "              Grad: tensor([-0.3229,  1.8280])\n",
      "Epoch: 288, Loss: 13.035663604736328 \n",
      "              Params: tensor([ 3.4732, -6.5804])\n",
      "              Grad: tensor([-0.3224,  1.8249])\n",
      "Epoch: 289, Loss: 13.001349449157715 \n",
      "              Params: tensor([ 3.4765, -6.5987])\n",
      "              Grad: tensor([-0.3218,  1.8218])\n",
      "Epoch: 290, Loss: 12.967151641845703 \n",
      "              Params: tensor([ 3.4797, -6.6169])\n",
      "              Grad: tensor([-0.3213,  1.8187])\n",
      "Epoch: 291, Loss: 12.933074951171875 \n",
      "              Params: tensor([ 3.4829, -6.6350])\n",
      "              Grad: tensor([-0.3207,  1.8156])\n",
      "Epoch: 292, Loss: 12.89910888671875 \n",
      "              Params: tensor([ 3.4861, -6.6531])\n",
      "              Grad: tensor([-0.3202,  1.8125])\n",
      "Epoch: 293, Loss: 12.865259170532227 \n",
      "              Params: tensor([ 3.4893, -6.6712])\n",
      "              Grad: tensor([-0.3196,  1.8095])\n",
      "Epoch: 294, Loss: 12.831524848937988 \n",
      "              Params: tensor([ 3.4925, -6.6893])\n",
      "              Grad: tensor([-0.3191,  1.8064])\n",
      "Epoch: 295, Loss: 12.797904014587402 \n",
      "              Params: tensor([ 3.4956, -6.7073])\n",
      "              Grad: tensor([-0.3186,  1.8033])\n",
      "Epoch: 296, Loss: 12.764398574829102 \n",
      "              Params: tensor([ 3.4988, -6.7253])\n",
      "              Grad: tensor([-0.3180,  1.8003])\n",
      "Epoch: 297, Loss: 12.731006622314453 \n",
      "              Params: tensor([ 3.5020, -6.7433])\n",
      "              Grad: tensor([-0.3175,  1.7972])\n",
      "Epoch: 298, Loss: 12.69772720336914 \n",
      "              Params: tensor([ 3.5052, -6.7612])\n",
      "              Grad: tensor([-0.3169,  1.7941])\n",
      "Epoch: 299, Loss: 12.664559364318848 \n",
      "              Params: tensor([ 3.5083, -6.7792])\n",
      "              Grad: tensor([-0.3164,  1.7911])\n",
      "Epoch: 300, Loss: 12.63150691986084 \n",
      "              Params: tensor([ 3.5115, -6.7970])\n",
      "              Grad: tensor([-0.3159,  1.7881])\n",
      "Epoch: 301, Loss: 12.598567962646484 \n",
      "              Params: tensor([ 3.5146, -6.8149])\n",
      "              Grad: tensor([-0.3153,  1.7850])\n",
      "Epoch: 302, Loss: 12.5657377243042 \n",
      "              Params: tensor([ 3.5178, -6.8327])\n",
      "              Grad: tensor([-0.3148,  1.7820])\n",
      "Epoch: 303, Loss: 12.533020973205566 \n",
      "              Params: tensor([ 3.5209, -6.8505])\n",
      "              Grad: tensor([-0.3143,  1.7790])\n",
      "Epoch: 304, Loss: 12.500412940979004 \n",
      "              Params: tensor([ 3.5241, -6.8683])\n",
      "              Grad: tensor([-0.3137,  1.7759])\n",
      "Epoch: 305, Loss: 12.46791934967041 \n",
      "              Params: tensor([ 3.5272, -6.8860])\n",
      "              Grad: tensor([-0.3132,  1.7729])\n",
      "Epoch: 306, Loss: 12.435531616210938 \n",
      "              Params: tensor([ 3.5303, -6.9037])\n",
      "              Grad: tensor([-0.3127,  1.7699])\n",
      "Epoch: 307, Loss: 12.4032564163208 \n",
      "              Params: tensor([ 3.5335, -6.9213])\n",
      "              Grad: tensor([-0.3121,  1.7669])\n",
      "Epoch: 308, Loss: 12.371089935302734 \n",
      "              Params: tensor([ 3.5366, -6.9390])\n",
      "              Grad: tensor([-0.3116,  1.7639])\n",
      "Epoch: 309, Loss: 12.339031219482422 \n",
      "              Params: tensor([ 3.5397, -6.9566])\n",
      "              Grad: tensor([-0.3111,  1.7609])\n",
      "Epoch: 310, Loss: 12.307082176208496 \n",
      "              Params: tensor([ 3.5428, -6.9742])\n",
      "              Grad: tensor([-0.3105,  1.7579])\n",
      "Epoch: 311, Loss: 12.275246620178223 \n",
      "              Params: tensor([ 3.5459, -6.9917])\n",
      "              Grad: tensor([-0.3100,  1.7549])\n",
      "Epoch: 312, Loss: 12.243509292602539 \n",
      "              Params: tensor([ 3.5490, -7.0092])\n",
      "              Grad: tensor([-0.3095,  1.7519])\n",
      "Epoch: 313, Loss: 12.21188735961914 \n",
      "              Params: tensor([ 3.5521, -7.0267])\n",
      "              Grad: tensor([-0.3090,  1.7490])\n",
      "Epoch: 314, Loss: 12.180370330810547 \n",
      "              Params: tensor([ 3.5552, -7.0442])\n",
      "              Grad: tensor([-0.3084,  1.7460])\n",
      "Epoch: 315, Loss: 12.148962020874023 \n",
      "              Params: tensor([ 3.5582, -7.0616])\n",
      "              Grad: tensor([-0.3079,  1.7430])\n",
      "Epoch: 316, Loss: 12.117656707763672 \n",
      "              Params: tensor([ 3.5613, -7.0790])\n",
      "              Grad: tensor([-0.3074,  1.7401])\n",
      "Epoch: 317, Loss: 12.086462020874023 \n",
      "              Params: tensor([ 3.5644, -7.0964])\n",
      "              Grad: tensor([-0.3069,  1.7371])\n",
      "Epoch: 318, Loss: 12.055373191833496 \n",
      "              Params: tensor([ 3.5674, -7.1137])\n",
      "              Grad: tensor([-0.3063,  1.7342])\n",
      "Epoch: 319, Loss: 12.024384498596191 \n",
      "              Params: tensor([ 3.5705, -7.1310])\n",
      "              Grad: tensor([-0.3058,  1.7312])\n",
      "Epoch: 320, Loss: 11.993508338928223 \n",
      "              Params: tensor([ 3.5736, -7.1483])\n",
      "              Grad: tensor([-0.3053,  1.7283])\n",
      "Epoch: 321, Loss: 11.96273136138916 \n",
      "              Params: tensor([ 3.5766, -7.1656])\n",
      "              Grad: tensor([-0.3048,  1.7253])\n",
      "Epoch: 322, Loss: 11.932056427001953 \n",
      "              Params: tensor([ 3.5796, -7.1828])\n",
      "              Grad: tensor([-0.3043,  1.7224])\n",
      "Epoch: 323, Loss: 11.90149211883545 \n",
      "              Params: tensor([ 3.5827, -7.2000])\n",
      "              Grad: tensor([-0.3037,  1.7195])\n",
      "Epoch: 324, Loss: 11.871028900146484 \n",
      "              Params: tensor([ 3.5857, -7.2172])\n",
      "              Grad: tensor([-0.3032,  1.7166])\n",
      "Epoch: 325, Loss: 11.840670585632324 \n",
      "              Params: tensor([ 3.5887, -7.2343])\n",
      "              Grad: tensor([-0.3027,  1.7136])\n",
      "Epoch: 326, Loss: 11.810413360595703 \n",
      "              Params: tensor([ 3.5918, -7.2514])\n",
      "              Grad: tensor([-0.3022,  1.7107])\n",
      "Epoch: 327, Loss: 11.780257225036621 \n",
      "              Params: tensor([ 3.5948, -7.2685])\n",
      "              Grad: tensor([-0.3017,  1.7078])\n",
      "Epoch: 328, Loss: 11.750207901000977 \n",
      "              Params: tensor([ 3.5978, -7.2855])\n",
      "              Grad: tensor([-0.3012,  1.7049])\n",
      "Epoch: 329, Loss: 11.720257759094238 \n",
      "              Params: tensor([ 3.6008, -7.3026])\n",
      "              Grad: tensor([-0.3007,  1.7020])\n",
      "Epoch: 330, Loss: 11.690411567687988 \n",
      "              Params: tensor([ 3.6038, -7.3196])\n",
      "              Grad: tensor([-0.3002,  1.6991])\n",
      "Epoch: 331, Loss: 11.660663604736328 \n",
      "              Params: tensor([ 3.6068, -7.3365])\n",
      "              Grad: tensor([-0.2996,  1.6963])\n",
      "Epoch: 332, Loss: 11.631014823913574 \n",
      "              Params: tensor([ 3.6098, -7.3535])\n",
      "              Grad: tensor([-0.2991,  1.6934])\n",
      "Epoch: 333, Loss: 11.601472854614258 \n",
      "              Params: tensor([ 3.6128, -7.3704])\n",
      "              Grad: tensor([-0.2986,  1.6905])\n",
      "Epoch: 334, Loss: 11.572030067443848 \n",
      "              Params: tensor([ 3.6158, -7.3872])\n",
      "              Grad: tensor([-0.2981,  1.6876])\n",
      "Epoch: 335, Loss: 11.542685508728027 \n",
      "              Params: tensor([ 3.6187, -7.4041])\n",
      "              Grad: tensor([-0.2976,  1.6848])\n",
      "Epoch: 336, Loss: 11.513440132141113 \n",
      "              Params: tensor([ 3.6217, -7.4209])\n",
      "              Grad: tensor([-0.2971,  1.6819])\n",
      "Epoch: 337, Loss: 11.484292984008789 \n",
      "              Params: tensor([ 3.6247, -7.4377])\n",
      "              Grad: tensor([-0.2966,  1.6790])\n",
      "Epoch: 338, Loss: 11.455245971679688 \n",
      "              Params: tensor([ 3.6276, -7.4545])\n",
      "              Grad: tensor([-0.2961,  1.6762])\n",
      "Epoch: 339, Loss: 11.426300048828125 \n",
      "              Params: tensor([ 3.6306, -7.4712])\n",
      "              Grad: tensor([-0.2956,  1.6733])\n",
      "Epoch: 340, Loss: 11.39744758605957 \n",
      "              Params: tensor([ 3.6335, -7.4879])\n",
      "              Grad: tensor([-0.2951,  1.6705])\n",
      "Epoch: 341, Loss: 11.368696212768555 \n",
      "              Params: tensor([ 3.6365, -7.5046])\n",
      "              Grad: tensor([-0.2946,  1.6677])\n",
      "Epoch: 342, Loss: 11.340043067932129 \n",
      "              Params: tensor([ 3.6394, -7.5212])\n",
      "              Grad: tensor([-0.2941,  1.6648])\n",
      "Epoch: 343, Loss: 11.311487197875977 \n",
      "              Params: tensor([ 3.6424, -7.5378])\n",
      "              Grad: tensor([-0.2936,  1.6620])\n",
      "Epoch: 344, Loss: 11.283027648925781 \n",
      "              Params: tensor([ 3.6453, -7.5544])\n",
      "              Grad: tensor([-0.2931,  1.6592])\n",
      "Epoch: 345, Loss: 11.254661560058594 \n",
      "              Params: tensor([ 3.6482, -7.5710])\n",
      "              Grad: tensor([-0.2926,  1.6564])\n",
      "Epoch: 346, Loss: 11.226395606994629 \n",
      "              Params: tensor([ 3.6511, -7.5875])\n",
      "              Grad: tensor([-0.2921,  1.6535])\n",
      "Epoch: 347, Loss: 11.198220252990723 \n",
      "              Params: tensor([ 3.6541, -7.6040])\n",
      "              Grad: tensor([-0.2916,  1.6507])\n",
      "Epoch: 348, Loss: 11.170149803161621 \n",
      "              Params: tensor([ 3.6570, -7.6205])\n",
      "              Grad: tensor([-0.2911,  1.6479])\n",
      "Epoch: 349, Loss: 11.142169952392578 \n",
      "              Params: tensor([ 3.6599, -7.6370])\n",
      "              Grad: tensor([-0.2906,  1.6451])\n",
      "Epoch: 350, Loss: 11.11428165435791 \n",
      "              Params: tensor([ 3.6628, -7.6534])\n",
      "              Grad: tensor([-0.2901,  1.6423])\n",
      "Epoch: 351, Loss: 11.086490631103516 \n",
      "              Params: tensor([ 3.6657, -7.6698])\n",
      "              Grad: tensor([-0.2896,  1.6395])\n",
      "Epoch: 352, Loss: 11.058796882629395 \n",
      "              Params: tensor([ 3.6686, -7.6861])\n",
      "              Grad: tensor([-0.2892,  1.6368])\n",
      "Epoch: 353, Loss: 11.031192779541016 \n",
      "              Params: tensor([ 3.6714, -7.7025])\n",
      "              Grad: tensor([-0.2886,  1.6340])\n",
      "Epoch: 354, Loss: 11.00368595123291 \n",
      "              Params: tensor([ 3.6743, -7.7188])\n",
      "              Grad: tensor([-0.2882,  1.6312])\n",
      "Epoch: 355, Loss: 10.976269721984863 \n",
      "              Params: tensor([ 3.6772, -7.7351])\n",
      "              Grad: tensor([-0.2877,  1.6284])\n",
      "Epoch: 356, Loss: 10.94894790649414 \n",
      "              Params: tensor([ 3.6801, -7.7513])\n",
      "              Grad: tensor([-0.2872,  1.6257])\n",
      "Epoch: 357, Loss: 10.92171859741211 \n",
      "              Params: tensor([ 3.6829, -7.7676])\n",
      "              Grad: tensor([-0.2867,  1.6229])\n",
      "Epoch: 358, Loss: 10.894580841064453 \n",
      "              Params: tensor([ 3.6858, -7.7838])\n",
      "              Grad: tensor([-0.2862,  1.6201])\n",
      "Epoch: 359, Loss: 10.867536544799805 \n",
      "              Params: tensor([ 3.6887, -7.7999])\n",
      "              Grad: tensor([-0.2857,  1.6174])\n",
      "Epoch: 360, Loss: 10.840582847595215 \n",
      "              Params: tensor([ 3.6915, -7.8161])\n",
      "              Grad: tensor([-0.2852,  1.6146])\n",
      "Epoch: 361, Loss: 10.813720703125 \n",
      "              Params: tensor([ 3.6944, -7.8322])\n",
      "              Grad: tensor([-0.2847,  1.6119])\n",
      "Epoch: 362, Loss: 10.78695011138916 \n",
      "              Params: tensor([ 3.6972, -7.8483])\n",
      "              Grad: tensor([-0.2843,  1.6092])\n",
      "Epoch: 363, Loss: 10.760270118713379 \n",
      "              Params: tensor([ 3.7000, -7.8644])\n",
      "              Grad: tensor([-0.2838,  1.6064])\n",
      "Epoch: 364, Loss: 10.733680725097656 \n",
      "              Params: tensor([ 3.7029, -7.8804])\n",
      "              Grad: tensor([-0.2833,  1.6037])\n",
      "Epoch: 365, Loss: 10.707183837890625 \n",
      "              Params: tensor([ 3.7057, -7.8964])\n",
      "              Grad: tensor([-0.2828,  1.6010])\n",
      "Epoch: 366, Loss: 10.680774688720703 \n",
      "              Params: tensor([ 3.7085, -7.9124])\n",
      "              Grad: tensor([-0.2823,  1.5983])\n",
      "Epoch: 367, Loss: 10.654454231262207 \n",
      "              Params: tensor([ 3.7113, -7.9284])\n",
      "              Grad: tensor([-0.2819,  1.5955])\n",
      "Epoch: 368, Loss: 10.628225326538086 \n",
      "              Params: tensor([ 3.7142, -7.9443])\n",
      "              Grad: tensor([-0.2814,  1.5928])\n",
      "Epoch: 369, Loss: 10.602086067199707 \n",
      "              Params: tensor([ 3.7170, -7.9602])\n",
      "              Grad: tensor([-0.2809,  1.5901])\n",
      "Epoch: 370, Loss: 10.576033592224121 \n",
      "              Params: tensor([ 3.7198, -7.9761])\n",
      "              Grad: tensor([-0.2804,  1.5874])\n",
      "Epoch: 371, Loss: 10.550070762634277 \n",
      "              Params: tensor([ 3.7226, -7.9919])\n",
      "              Grad: tensor([-0.2799,  1.5847])\n",
      "Epoch: 372, Loss: 10.52419376373291 \n",
      "              Params: tensor([ 3.7254, -8.0077])\n",
      "              Grad: tensor([-0.2795,  1.5820])\n",
      "Epoch: 373, Loss: 10.498409271240234 \n",
      "              Params: tensor([ 3.7282, -8.0235])\n",
      "              Grad: tensor([-0.2790,  1.5794])\n",
      "Epoch: 374, Loss: 10.47270679473877 \n",
      "              Params: tensor([ 3.7309, -8.0393])\n",
      "              Grad: tensor([-0.2785,  1.5767])\n",
      "Epoch: 375, Loss: 10.44709300994873 \n",
      "              Params: tensor([ 3.7337, -8.0550])\n",
      "              Grad: tensor([-0.2780,  1.5740])\n",
      "Epoch: 376, Loss: 10.421568870544434 \n",
      "              Params: tensor([ 3.7365, -8.0707])\n",
      "              Grad: tensor([-0.2776,  1.5713])\n",
      "Epoch: 377, Loss: 10.396132469177246 \n",
      "              Params: tensor([ 3.7393, -8.0864])\n",
      "              Grad: tensor([-0.2771,  1.5686])\n",
      "Epoch: 378, Loss: 10.370779037475586 \n",
      "              Params: tensor([ 3.7420, -8.1021])\n",
      "              Grad: tensor([-0.2766,  1.5660])\n",
      "Epoch: 379, Loss: 10.345510482788086 \n",
      "              Params: tensor([ 3.7448, -8.1177])\n",
      "              Grad: tensor([-0.2762,  1.5633])\n",
      "Epoch: 380, Loss: 10.320327758789062 \n",
      "              Params: tensor([ 3.7476, -8.1333])\n",
      "              Grad: tensor([-0.2757,  1.5607])\n",
      "Epoch: 381, Loss: 10.295233726501465 \n",
      "              Params: tensor([ 3.7503, -8.1489])\n",
      "              Grad: tensor([-0.2752,  1.5580])\n",
      "Epoch: 382, Loss: 10.270223617553711 \n",
      "              Params: tensor([ 3.7531, -8.1645])\n",
      "              Grad: tensor([-0.2748,  1.5554])\n",
      "Epoch: 383, Loss: 10.245296478271484 \n",
      "              Params: tensor([ 3.7558, -8.1800])\n",
      "              Grad: tensor([-0.2743,  1.5527])\n",
      "Epoch: 384, Loss: 10.220457077026367 \n",
      "              Params: tensor([ 3.7585, -8.1955])\n",
      "              Grad: tensor([-0.2738,  1.5501])\n",
      "Epoch: 385, Loss: 10.195700645446777 \n",
      "              Params: tensor([ 3.7613, -8.2110])\n",
      "              Grad: tensor([-0.2734,  1.5475])\n",
      "Epoch: 386, Loss: 10.171029090881348 \n",
      "              Params: tensor([ 3.7640, -8.2264])\n",
      "              Grad: tensor([-0.2729,  1.5448])\n",
      "Epoch: 387, Loss: 10.146437644958496 \n",
      "              Params: tensor([ 3.7667, -8.2418])\n",
      "              Grad: tensor([-0.2724,  1.5422])\n",
      "Epoch: 388, Loss: 10.12193489074707 \n",
      "              Params: tensor([ 3.7694, -8.2572])\n",
      "              Grad: tensor([-0.2720,  1.5396])\n",
      "Epoch: 389, Loss: 10.097512245178223 \n",
      "              Params: tensor([ 3.7722, -8.2726])\n",
      "              Grad: tensor([-0.2715,  1.5370])\n",
      "Epoch: 390, Loss: 10.073172569274902 \n",
      "              Params: tensor([ 3.7749, -8.2879])\n",
      "              Grad: tensor([-0.2711,  1.5344])\n",
      "Epoch: 391, Loss: 10.048918724060059 \n",
      "              Params: tensor([ 3.7776, -8.3033])\n",
      "              Grad: tensor([-0.2706,  1.5317])\n",
      "Epoch: 392, Loss: 10.02474308013916 \n",
      "              Params: tensor([ 3.7803, -8.3185])\n",
      "              Grad: tensor([-0.2701,  1.5291])\n",
      "Epoch: 393, Loss: 10.000652313232422 \n",
      "              Params: tensor([ 3.7830, -8.3338])\n",
      "              Grad: tensor([-0.2697,  1.5265])\n",
      "Epoch: 394, Loss: 9.976639747619629 \n",
      "              Params: tensor([ 3.7857, -8.3491])\n",
      "              Grad: tensor([-0.2692,  1.5240])\n",
      "Epoch: 395, Loss: 9.952712059020996 \n",
      "              Params: tensor([ 3.7884, -8.3643])\n",
      "              Grad: tensor([-0.2688,  1.5214])\n",
      "Epoch: 396, Loss: 9.928861618041992 \n",
      "              Params: tensor([ 3.7910, -8.3795])\n",
      "              Grad: tensor([-0.2683,  1.5188])\n",
      "Epoch: 397, Loss: 9.9050931930542 \n",
      "              Params: tensor([ 3.7937, -8.3946])\n",
      "              Grad: tensor([-0.2678,  1.5162])\n",
      "Epoch: 398, Loss: 9.88140869140625 \n",
      "              Params: tensor([ 3.7964, -8.4098])\n",
      "              Grad: tensor([-0.2674,  1.5136])\n",
      "Epoch: 399, Loss: 9.857804298400879 \n",
      "              Params: tensor([ 3.7991, -8.4249])\n",
      "              Grad: tensor([-0.2669,  1.5111])\n",
      "Epoch: 400, Loss: 9.834277153015137 \n",
      "              Params: tensor([ 3.8017, -8.4399])\n",
      "              Grad: tensor([-0.2665,  1.5085])\n",
      "Epoch: 401, Loss: 9.810831069946289 \n",
      "              Params: tensor([ 3.8044, -8.4550])\n",
      "              Grad: tensor([-0.2660,  1.5059])\n",
      "Epoch: 402, Loss: 9.787466049194336 \n",
      "              Params: tensor([ 3.8070, -8.4700])\n",
      "              Grad: tensor([-0.2656,  1.5034])\n",
      "Epoch: 403, Loss: 9.764176368713379 \n",
      "              Params: tensor([ 3.8097, -8.4851])\n",
      "              Grad: tensor([-0.2651,  1.5008])\n",
      "Epoch: 404, Loss: 9.740972518920898 \n",
      "              Params: tensor([ 3.8123, -8.5000])\n",
      "              Grad: tensor([-0.2647,  1.4983])\n",
      "Epoch: 405, Loss: 9.717843055725098 \n",
      "              Params: tensor([ 3.8150, -8.5150])\n",
      "              Grad: tensor([-0.2642,  1.4957])\n",
      "Epoch: 406, Loss: 9.694792747497559 \n",
      "              Params: tensor([ 3.8176, -8.5299])\n",
      "              Grad: tensor([-0.2638,  1.4932])\n",
      "Epoch: 407, Loss: 9.67182445526123 \n",
      "              Params: tensor([ 3.8202, -8.5448])\n",
      "              Grad: tensor([-0.2633,  1.4906])\n",
      "Epoch: 408, Loss: 9.64892578125 \n",
      "              Params: tensor([ 3.8229, -8.5597])\n",
      "              Grad: tensor([-0.2629,  1.4881])\n",
      "Epoch: 409, Loss: 9.626110076904297 \n",
      "              Params: tensor([ 3.8255, -8.5746])\n",
      "              Grad: tensor([-0.2624,  1.4856])\n",
      "Epoch: 410, Loss: 9.603372573852539 \n",
      "              Params: tensor([ 3.8281, -8.5894])\n",
      "              Grad: tensor([-0.2620,  1.4831])\n",
      "Epoch: 411, Loss: 9.580709457397461 \n",
      "              Params: tensor([ 3.8307, -8.6042])\n",
      "              Grad: tensor([-0.2615,  1.4805])\n",
      "Epoch: 412, Loss: 9.558124542236328 \n",
      "              Params: tensor([ 3.8333, -8.6190])\n",
      "              Grad: tensor([-0.2611,  1.4780])\n",
      "Epoch: 413, Loss: 9.535616874694824 \n",
      "              Params: tensor([ 3.8360, -8.6337])\n",
      "              Grad: tensor([-0.2606,  1.4755])\n",
      "Epoch: 414, Loss: 9.51318359375 \n",
      "              Params: tensor([ 3.8386, -8.6485])\n",
      "              Grad: tensor([-0.2602,  1.4730])\n",
      "Epoch: 415, Loss: 9.490828514099121 \n",
      "              Params: tensor([ 3.8412, -8.6632])\n",
      "              Grad: tensor([-0.2598,  1.4705])\n",
      "Epoch: 416, Loss: 9.468550682067871 \n",
      "              Params: tensor([ 3.8437, -8.6779])\n",
      "              Grad: tensor([-0.2593,  1.4680])\n",
      "Epoch: 417, Loss: 9.4463472366333 \n",
      "              Params: tensor([ 3.8463, -8.6925])\n",
      "              Grad: tensor([-0.2589,  1.4655])\n",
      "Epoch: 418, Loss: 9.424216270446777 \n",
      "              Params: tensor([ 3.8489, -8.7071])\n",
      "              Grad: tensor([-0.2584,  1.4630])\n",
      "Epoch: 419, Loss: 9.4021635055542 \n",
      "              Params: tensor([ 3.8515, -8.7217])\n",
      "              Grad: tensor([-0.2580,  1.4605])\n",
      "Epoch: 420, Loss: 9.380184173583984 \n",
      "              Params: tensor([ 3.8541, -8.7363])\n",
      "              Grad: tensor([-0.2576,  1.4581])\n",
      "Epoch: 421, Loss: 9.358282089233398 \n",
      "              Params: tensor([ 3.8566, -8.7509])\n",
      "              Grad: tensor([-0.2571,  1.4556])\n",
      "Epoch: 422, Loss: 9.336447715759277 \n",
      "              Params: tensor([ 3.8592, -8.7654])\n",
      "              Grad: tensor([-0.2567,  1.4531])\n",
      "Epoch: 423, Loss: 9.314695358276367 \n",
      "              Params: tensor([ 3.8618, -8.7799])\n",
      "              Grad: tensor([-0.2563,  1.4506])\n",
      "Epoch: 424, Loss: 9.293011665344238 \n",
      "              Params: tensor([ 3.8643, -8.7944])\n",
      "              Grad: tensor([-0.2558,  1.4482])\n",
      "Epoch: 425, Loss: 9.271403312683105 \n",
      "              Params: tensor([ 3.8669, -8.8089])\n",
      "              Grad: tensor([-0.2554,  1.4457])\n",
      "Epoch: 426, Loss: 9.249871253967285 \n",
      "              Params: tensor([ 3.8694, -8.8233])\n",
      "              Grad: tensor([-0.2550,  1.4433])\n",
      "Epoch: 427, Loss: 9.228409767150879 \n",
      "              Params: tensor([ 3.8720, -8.8377])\n",
      "              Grad: tensor([-0.2545,  1.4408])\n",
      "Epoch: 428, Loss: 9.207021713256836 \n",
      "              Params: tensor([ 3.8745, -8.8521])\n",
      "              Grad: tensor([-0.2541,  1.4384])\n",
      "Epoch: 429, Loss: 9.185704231262207 \n",
      "              Params: tensor([ 3.8771, -8.8664])\n",
      "              Grad: tensor([-0.2537,  1.4359])\n",
      "Epoch: 430, Loss: 9.164462089538574 \n",
      "              Params: tensor([ 3.8796, -8.8808])\n",
      "              Grad: tensor([-0.2532,  1.4335])\n",
      "Epoch: 431, Loss: 9.143288612365723 \n",
      "              Params: tensor([ 3.8821, -8.8951])\n",
      "              Grad: tensor([-0.2528,  1.4310])\n",
      "Epoch: 432, Loss: 9.122188568115234 \n",
      "              Params: tensor([ 3.8846, -8.9094])\n",
      "              Grad: tensor([-0.2524,  1.4286])\n",
      "Epoch: 433, Loss: 9.101160049438477 \n",
      "              Params: tensor([ 3.8872, -8.9236])\n",
      "              Grad: tensor([-0.2519,  1.4262])\n",
      "Epoch: 434, Loss: 9.080204010009766 \n",
      "              Params: tensor([ 3.8897, -8.9379])\n",
      "              Grad: tensor([-0.2515,  1.4238])\n",
      "Epoch: 435, Loss: 9.059317588806152 \n",
      "              Params: tensor([ 3.8922, -8.9521])\n",
      "              Grad: tensor([-0.2511,  1.4213])\n",
      "Epoch: 436, Loss: 9.038501739501953 \n",
      "              Params: tensor([ 3.8947, -8.9663])\n",
      "              Grad: tensor([-0.2507,  1.4189])\n",
      "Epoch: 437, Loss: 9.017757415771484 \n",
      "              Params: tensor([ 3.8972, -8.9804])\n",
      "              Grad: tensor([-0.2502,  1.4165])\n",
      "Epoch: 438, Loss: 8.99708366394043 \n",
      "              Params: tensor([ 3.8997, -8.9946])\n",
      "              Grad: tensor([-0.2498,  1.4141])\n",
      "Epoch: 439, Loss: 8.976478576660156 \n",
      "              Params: tensor([ 3.9022, -9.0087])\n",
      "              Grad: tensor([-0.2494,  1.4117])\n",
      "Epoch: 440, Loss: 8.955944061279297 \n",
      "              Params: tensor([ 3.9047, -9.0228])\n",
      "              Grad: tensor([-0.2489,  1.4093])\n",
      "Epoch: 441, Loss: 8.935480117797852 \n",
      "              Params: tensor([ 3.9072, -9.0369])\n",
      "              Grad: tensor([-0.2485,  1.4069])\n",
      "Epoch: 442, Loss: 8.915088653564453 \n",
      "              Params: tensor([ 3.9096, -9.0509])\n",
      "              Grad: tensor([-0.2481,  1.4045])\n",
      "Epoch: 443, Loss: 8.89476203918457 \n",
      "              Params: tensor([ 3.9121, -9.0649])\n",
      "              Grad: tensor([-0.2477,  1.4021])\n",
      "Epoch: 444, Loss: 8.874507904052734 \n",
      "              Params: tensor([ 3.9146, -9.0789])\n",
      "              Grad: tensor([-0.2473,  1.3998])\n",
      "Epoch: 445, Loss: 8.854317665100098 \n",
      "              Params: tensor([ 3.9171, -9.0929])\n",
      "              Grad: tensor([-0.2468,  1.3974])\n",
      "Epoch: 446, Loss: 8.834197044372559 \n",
      "              Params: tensor([ 3.9195, -9.1068])\n",
      "              Grad: tensor([-0.2464,  1.3950])\n",
      "Epoch: 447, Loss: 8.814148902893066 \n",
      "              Params: tensor([ 3.9220, -9.1208])\n",
      "              Grad: tensor([-0.2460,  1.3926])\n",
      "Epoch: 448, Loss: 8.794161796569824 \n",
      "              Params: tensor([ 3.9244, -9.1347])\n",
      "              Grad: tensor([-0.2456,  1.3903])\n",
      "Epoch: 449, Loss: 8.774252891540527 \n",
      "              Params: tensor([ 3.9269, -9.1486])\n",
      "              Grad: tensor([-0.2452,  1.3879])\n",
      "Epoch: 450, Loss: 8.75440502166748 \n",
      "              Params: tensor([ 3.9293, -9.1624])\n",
      "              Grad: tensor([-0.2448,  1.3856])\n",
      "Epoch: 451, Loss: 8.734622955322266 \n",
      "              Params: tensor([ 3.9318, -9.1762])\n",
      "              Grad: tensor([-0.2443,  1.3832])\n",
      "Epoch: 452, Loss: 8.714910507202148 \n",
      "              Params: tensor([ 3.9342, -9.1901])\n",
      "              Grad: tensor([-0.2439,  1.3808])\n",
      "Epoch: 453, Loss: 8.695265769958496 \n",
      "              Params: tensor([ 3.9367, -9.2038])\n",
      "              Grad: tensor([-0.2435,  1.3785])\n",
      "Epoch: 454, Loss: 8.675687789916992 \n",
      "              Params: tensor([ 3.9391, -9.2176])\n",
      "              Grad: tensor([-0.2431,  1.3762])\n",
      "Epoch: 455, Loss: 8.656172752380371 \n",
      "              Params: tensor([ 3.9415, -9.2313])\n",
      "              Grad: tensor([-0.2427,  1.3738])\n",
      "Epoch: 456, Loss: 8.63672924041748 \n",
      "              Params: tensor([ 3.9439, -9.2451])\n",
      "              Grad: tensor([-0.2423,  1.3715])\n",
      "Epoch: 457, Loss: 8.61734676361084 \n",
      "              Params: tensor([ 3.9464, -9.2587])\n",
      "              Grad: tensor([-0.2419,  1.3692])\n",
      "Epoch: 458, Loss: 8.598029136657715 \n",
      "              Params: tensor([ 3.9488, -9.2724])\n",
      "              Grad: tensor([-0.2414,  1.3668])\n",
      "Epoch: 459, Loss: 8.578781127929688 \n",
      "              Params: tensor([ 3.9512, -9.2861])\n",
      "              Grad: tensor([-0.2410,  1.3645])\n",
      "Epoch: 460, Loss: 8.55959701538086 \n",
      "              Params: tensor([ 3.9536, -9.2997])\n",
      "              Grad: tensor([-0.2406,  1.3622])\n",
      "Epoch: 461, Loss: 8.540478706359863 \n",
      "              Params: tensor([ 3.9560, -9.3133])\n",
      "              Grad: tensor([-0.2402,  1.3599])\n",
      "Epoch: 462, Loss: 8.5214262008667 \n",
      "              Params: tensor([ 3.9584, -9.3269])\n",
      "              Grad: tensor([-0.2398,  1.3576])\n",
      "Epoch: 463, Loss: 8.502436637878418 \n",
      "              Params: tensor([ 3.9608, -9.3404])\n",
      "              Grad: tensor([-0.2394,  1.3553])\n",
      "Epoch: 464, Loss: 8.483516693115234 \n",
      "              Params: tensor([ 3.9632, -9.3539])\n",
      "              Grad: tensor([-0.2390,  1.3530])\n",
      "Epoch: 465, Loss: 8.464652061462402 \n",
      "              Params: tensor([ 3.9656, -9.3674])\n",
      "              Grad: tensor([-0.2386,  1.3507])\n",
      "Epoch: 466, Loss: 8.445858001708984 \n",
      "              Params: tensor([ 3.9679, -9.3809])\n",
      "              Grad: tensor([-0.2382,  1.3484])\n",
      "Epoch: 467, Loss: 8.427127838134766 \n",
      "              Params: tensor([ 3.9703, -9.3944])\n",
      "              Grad: tensor([-0.2378,  1.3461])\n",
      "Epoch: 468, Loss: 8.408453941345215 \n",
      "              Params: tensor([ 3.9727, -9.4078])\n",
      "              Grad: tensor([-0.2374,  1.3438])\n",
      "Epoch: 469, Loss: 8.389847755432129 \n",
      "              Params: tensor([ 3.9751, -9.4212])\n",
      "              Grad: tensor([-0.2370,  1.3415])\n",
      "Epoch: 470, Loss: 8.371305465698242 \n",
      "              Params: tensor([ 3.9774, -9.4346])\n",
      "              Grad: tensor([-0.2366,  1.3392])\n",
      "Epoch: 471, Loss: 8.352828025817871 \n",
      "              Params: tensor([ 3.9798, -9.4480])\n",
      "              Grad: tensor([-0.2362,  1.3370])\n",
      "Epoch: 472, Loss: 8.3344087600708 \n",
      "              Params: tensor([ 3.9822, -9.4614])\n",
      "              Grad: tensor([-0.2358,  1.3347])\n",
      "Epoch: 473, Loss: 8.316054344177246 \n",
      "              Params: tensor([ 3.9845, -9.4747])\n",
      "              Grad: tensor([-0.2354,  1.3324])\n",
      "Epoch: 474, Loss: 8.29776382446289 \n",
      "              Params: tensor([ 3.9869, -9.4880])\n",
      "              Grad: tensor([-0.2350,  1.3301])\n",
      "Epoch: 475, Loss: 8.279534339904785 \n",
      "              Params: tensor([ 3.9892, -9.5013])\n",
      "              Grad: tensor([-0.2346,  1.3279])\n",
      "Epoch: 476, Loss: 8.261368751525879 \n",
      "              Params: tensor([ 3.9915, -9.5145])\n",
      "              Grad: tensor([-0.2342,  1.3256])\n",
      "Epoch: 477, Loss: 8.24325942993164 \n",
      "              Params: tensor([ 3.9939, -9.5277])\n",
      "              Grad: tensor([-0.2338,  1.3234])\n",
      "Epoch: 478, Loss: 8.225213050842285 \n",
      "              Params: tensor([ 3.9962, -9.5410])\n",
      "              Grad: tensor([-0.2334,  1.3211])\n",
      "Epoch: 479, Loss: 8.207230567932129 \n",
      "              Params: tensor([ 3.9985, -9.5541])\n",
      "              Grad: tensor([-0.2330,  1.3189])\n",
      "Epoch: 480, Loss: 8.189310073852539 \n",
      "              Params: tensor([ 4.0009, -9.5673])\n",
      "              Grad: tensor([-0.2326,  1.3166])\n",
      "Epoch: 481, Loss: 8.171451568603516 \n",
      "              Params: tensor([ 4.0032, -9.5805])\n",
      "              Grad: tensor([-0.2322,  1.3144])\n",
      "Epoch: 482, Loss: 8.153647422790527 \n",
      "              Params: tensor([ 4.0055, -9.5936])\n",
      "              Grad: tensor([-0.2318,  1.3122])\n",
      "Epoch: 483, Loss: 8.135906219482422 \n",
      "              Params: tensor([ 4.0078, -9.6067])\n",
      "              Grad: tensor([-0.2314,  1.3100])\n",
      "Epoch: 484, Loss: 8.118226051330566 \n",
      "              Params: tensor([ 4.0101, -9.6198])\n",
      "              Grad: tensor([-0.2310,  1.3077])\n",
      "Epoch: 485, Loss: 8.100606918334961 \n",
      "              Params: tensor([ 4.0124, -9.6328])\n",
      "              Grad: tensor([-0.2306,  1.3055])\n",
      "Epoch: 486, Loss: 8.08304500579834 \n",
      "              Params: tensor([ 4.0147, -9.6458])\n",
      "              Grad: tensor([-0.2302,  1.3033])\n",
      "Epoch: 487, Loss: 8.065547943115234 \n",
      "              Params: tensor([ 4.0170, -9.6589])\n",
      "              Grad: tensor([-0.2298,  1.3011])\n",
      "Epoch: 488, Loss: 8.048104286193848 \n",
      "              Params: tensor([ 4.0193, -9.6718])\n",
      "              Grad: tensor([-0.2295,  1.2989])\n",
      "Epoch: 489, Loss: 8.030723571777344 \n",
      "              Params: tensor([ 4.0216, -9.6848])\n",
      "              Grad: tensor([-0.2291,  1.2967])\n",
      "Epoch: 490, Loss: 8.01340103149414 \n",
      "              Params: tensor([ 4.0239, -9.6978])\n",
      "              Grad: tensor([-0.2287,  1.2945])\n",
      "Epoch: 491, Loss: 7.996136665344238 \n",
      "              Params: tensor([ 4.0262, -9.7107])\n",
      "              Grad: tensor([-0.2283,  1.2923])\n",
      "Epoch: 492, Loss: 7.97892951965332 \n",
      "              Params: tensor([ 4.0285, -9.7236])\n",
      "              Grad: tensor([-0.2279,  1.2901])\n",
      "Epoch: 493, Loss: 7.961782932281494 \n",
      "              Params: tensor([ 4.0308, -9.7365])\n",
      "              Grad: tensor([-0.2275,  1.2879])\n",
      "Epoch: 494, Loss: 7.944689750671387 \n",
      "              Params: tensor([ 4.0330, -9.7493])\n",
      "              Grad: tensor([-0.2271,  1.2857])\n",
      "Epoch: 495, Loss: 7.9276628494262695 \n",
      "              Params: tensor([ 4.0353, -9.7621])\n",
      "              Grad: tensor([-0.2267,  1.2835])\n",
      "Epoch: 496, Loss: 7.9106903076171875 \n",
      "              Params: tensor([ 4.0376, -9.7750])\n",
      "              Grad: tensor([-0.2263,  1.2813])\n",
      "Epoch: 497, Loss: 7.893775463104248 \n",
      "              Params: tensor([ 4.0398, -9.7878])\n",
      "              Grad: tensor([-0.2260,  1.2791])\n",
      "Epoch: 498, Loss: 7.876915454864502 \n",
      "              Params: tensor([ 4.0421, -9.8005])\n",
      "              Grad: tensor([-0.2256,  1.2770])\n",
      "Epoch: 499, Loss: 7.860115051269531 \n",
      "              Params: tensor([ 4.0443, -9.8133])\n",
      "              Grad: tensor([-0.2252,  1.2748])\n",
      "Epoch: 500, Loss: 7.843369007110596 \n",
      "              Params: tensor([ 4.0466, -9.8260])\n",
      "              Grad: tensor([-0.2248,  1.2726])\n",
      "Epoch: 501, Loss: 7.8266825675964355 \n",
      "              Params: tensor([ 4.0488, -9.8387])\n",
      "              Grad: tensor([-0.2244,  1.2705])\n",
      "Epoch: 502, Loss: 7.810052871704102 \n",
      "              Params: tensor([ 4.0511, -9.8514])\n",
      "              Grad: tensor([-0.2241,  1.2683])\n",
      "Epoch: 503, Loss: 7.793481349945068 \n",
      "              Params: tensor([ 4.0533, -9.8640])\n",
      "              Grad: tensor([-0.2237,  1.2662])\n",
      "Epoch: 504, Loss: 7.776961803436279 \n",
      "              Params: tensor([ 4.0555, -9.8767])\n",
      "              Grad: tensor([-0.2233,  1.2640])\n",
      "Epoch: 505, Loss: 7.760497570037842 \n",
      "              Params: tensor([ 4.0578, -9.8893])\n",
      "              Grad: tensor([-0.2229,  1.2619])\n",
      "Epoch: 506, Loss: 7.744091987609863 \n",
      "              Params: tensor([ 4.0600, -9.9019])\n",
      "              Grad: tensor([-0.2225,  1.2597])\n",
      "Epoch: 507, Loss: 7.7277445793151855 \n",
      "              Params: tensor([ 4.0622, -9.9145])\n",
      "              Grad: tensor([-0.2222,  1.2576])\n",
      "Epoch: 508, Loss: 7.711448669433594 \n",
      "              Params: tensor([ 4.0644, -9.9270])\n",
      "              Grad: tensor([-0.2218,  1.2554])\n",
      "Epoch: 509, Loss: 7.695211410522461 \n",
      "              Params: tensor([ 4.0666, -9.9396])\n",
      "              Grad: tensor([-0.2214,  1.2533])\n",
      "Epoch: 510, Loss: 7.679023742675781 \n",
      "              Params: tensor([ 4.0688, -9.9521])\n",
      "              Grad: tensor([-0.2210,  1.2512])\n",
      "Epoch: 511, Loss: 7.662895679473877 \n",
      "              Params: tensor([ 4.0710, -9.9646])\n",
      "              Grad: tensor([-0.2207,  1.2490])\n",
      "Epoch: 512, Loss: 7.646819591522217 \n",
      "              Params: tensor([ 4.0733, -9.9770])\n",
      "              Grad: tensor([-0.2203,  1.2469])\n",
      "Epoch: 513, Loss: 7.630802631378174 \n",
      "              Params: tensor([ 4.0754, -9.9895])\n",
      "              Grad: tensor([-0.2199,  1.2448])\n",
      "Epoch: 514, Loss: 7.614835739135742 \n",
      "              Params: tensor([  4.0776, -10.0019])\n",
      "              Grad: tensor([-0.2195,  1.2427])\n",
      "Epoch: 515, Loss: 7.59892463684082 \n",
      "              Params: tensor([  4.0798, -10.0143])\n",
      "              Grad: tensor([-0.2192,  1.2406])\n",
      "Epoch: 516, Loss: 7.58306884765625 \n",
      "              Params: tensor([  4.0820, -10.0267])\n",
      "              Grad: tensor([-0.2188,  1.2385])\n",
      "Epoch: 517, Loss: 7.567265033721924 \n",
      "              Params: tensor([  4.0842, -10.0391])\n",
      "              Grad: tensor([-0.2184,  1.2364])\n",
      "Epoch: 518, Loss: 7.551515102386475 \n",
      "              Params: tensor([  4.0864, -10.0514])\n",
      "              Grad: tensor([-0.2180,  1.2343])\n",
      "Epoch: 519, Loss: 7.535818099975586 \n",
      "              Params: tensor([  4.0886, -10.0637])\n",
      "              Grad: tensor([-0.2177,  1.2322])\n",
      "Epoch: 520, Loss: 7.520176410675049 \n",
      "              Params: tensor([  4.0907, -10.0760])\n",
      "              Grad: tensor([-0.2173,  1.2301])\n",
      "Epoch: 521, Loss: 7.504587173461914 \n",
      "              Params: tensor([  4.0929, -10.0883])\n",
      "              Grad: tensor([-0.2169,  1.2280])\n",
      "Epoch: 522, Loss: 7.489048480987549 \n",
      "              Params: tensor([  4.0951, -10.1006])\n",
      "              Grad: tensor([-0.2165,  1.2259])\n",
      "Epoch: 523, Loss: 7.473565578460693 \n",
      "              Params: tensor([  4.0972, -10.1128])\n",
      "              Grad: tensor([-0.2162,  1.2238])\n",
      "Epoch: 524, Loss: 7.458134651184082 \n",
      "              Params: tensor([  4.0994, -10.1250])\n",
      "              Grad: tensor([-0.2158,  1.2217])\n",
      "Epoch: 525, Loss: 7.442750453948975 \n",
      "              Params: tensor([  4.1015, -10.1372])\n",
      "              Grad: tensor([-0.2155,  1.2197])\n",
      "Epoch: 526, Loss: 7.427427291870117 \n",
      "              Params: tensor([  4.1037, -10.1494])\n",
      "              Grad: tensor([-0.2151,  1.2176])\n",
      "Epoch: 527, Loss: 7.412152290344238 \n",
      "              Params: tensor([  4.1058, -10.1616])\n",
      "              Grad: tensor([-0.2147,  1.2155])\n",
      "Epoch: 528, Loss: 7.396928310394287 \n",
      "              Params: tensor([  4.1080, -10.1737])\n",
      "              Grad: tensor([-0.2144,  1.2135])\n",
      "Epoch: 529, Loss: 7.381756782531738 \n",
      "              Params: tensor([  4.1101, -10.1858])\n",
      "              Grad: tensor([-0.2140,  1.2114])\n",
      "Epoch: 530, Loss: 7.366636753082275 \n",
      "              Params: tensor([  4.1123, -10.1979])\n",
      "              Grad: tensor([-0.2136,  1.2093])\n",
      "Epoch: 531, Loss: 7.351567268371582 \n",
      "              Params: tensor([  4.1144, -10.2100])\n",
      "              Grad: tensor([-0.2133,  1.2073])\n",
      "Epoch: 532, Loss: 7.336549282073975 \n",
      "              Params: tensor([  4.1165, -10.2220])\n",
      "              Grad: tensor([-0.2129,  1.2052])\n",
      "Epoch: 533, Loss: 7.3215837478637695 \n",
      "              Params: tensor([  4.1187, -10.2340])\n",
      "              Grad: tensor([-0.2125,  1.2032])\n",
      "Epoch: 534, Loss: 7.306671142578125 \n",
      "              Params: tensor([  4.1208, -10.2461])\n",
      "              Grad: tensor([-0.2122,  1.2012])\n",
      "Epoch: 535, Loss: 7.291804313659668 \n",
      "              Params: tensor([  4.1229, -10.2581])\n",
      "              Grad: tensor([-0.2118,  1.1991])\n",
      "Epoch: 536, Loss: 7.276988506317139 \n",
      "              Params: tensor([  4.1250, -10.2700])\n",
      "              Grad: tensor([-0.2115,  1.1971])\n",
      "Epoch: 537, Loss: 7.262226581573486 \n",
      "              Params: tensor([  4.1271, -10.2820])\n",
      "              Grad: tensor([-0.2111,  1.1950])\n",
      "Epoch: 538, Loss: 7.247512340545654 \n",
      "              Params: tensor([  4.1292, -10.2939])\n",
      "              Grad: tensor([-0.2108,  1.1930])\n",
      "Epoch: 539, Loss: 7.232844829559326 \n",
      "              Params: tensor([  4.1313, -10.3058])\n",
      "              Grad: tensor([-0.2104,  1.1910])\n",
      "Epoch: 540, Loss: 7.218230724334717 \n",
      "              Params: tensor([  4.1334, -10.3177])\n",
      "              Grad: tensor([-0.2100,  1.1890])\n",
      "Epoch: 541, Loss: 7.203665256500244 \n",
      "              Params: tensor([  4.1355, -10.3296])\n",
      "              Grad: tensor([-0.2097,  1.1869])\n",
      "Epoch: 542, Loss: 7.189150810241699 \n",
      "              Params: tensor([  4.1376, -10.3414])\n",
      "              Grad: tensor([-0.2093,  1.1849])\n",
      "Epoch: 543, Loss: 7.1746826171875 \n",
      "              Params: tensor([  4.1397, -10.3533])\n",
      "              Grad: tensor([-0.2090,  1.1829])\n",
      "Epoch: 544, Loss: 7.160265922546387 \n",
      "              Params: tensor([  4.1418, -10.3651])\n",
      "              Grad: tensor([-0.2086,  1.1809])\n",
      "Epoch: 545, Loss: 7.145896911621094 \n",
      "              Params: tensor([  4.1439, -10.3769])\n",
      "              Grad: tensor([-0.2083,  1.1789])\n",
      "Epoch: 546, Loss: 7.131580829620361 \n",
      "              Params: tensor([  4.1460, -10.3886])\n",
      "              Grad: tensor([-0.2079,  1.1769])\n",
      "Epoch: 547, Loss: 7.117304801940918 \n",
      "              Params: tensor([  4.1480, -10.4004])\n",
      "              Grad: tensor([-0.2075,  1.1749])\n",
      "Epoch: 548, Loss: 7.103082656860352 \n",
      "              Params: tensor([  4.1501, -10.4121])\n",
      "              Grad: tensor([-0.2072,  1.1729])\n",
      "Epoch: 549, Loss: 7.088911056518555 \n",
      "              Params: tensor([  4.1522, -10.4238])\n",
      "              Grad: tensor([-0.2068,  1.1709])\n",
      "Epoch: 550, Loss: 7.074785232543945 \n",
      "              Params: tensor([  4.1542, -10.4355])\n",
      "              Grad: tensor([-0.2065,  1.1689])\n",
      "Epoch: 551, Loss: 7.060707092285156 \n",
      "              Params: tensor([  4.1563, -10.4472])\n",
      "              Grad: tensor([-0.2062,  1.1669])\n",
      "Epoch: 552, Loss: 7.046676158905029 \n",
      "              Params: tensor([  4.1584, -10.4588])\n",
      "              Grad: tensor([-0.2058,  1.1649])\n",
      "Epoch: 553, Loss: 7.032695293426514 \n",
      "              Params: tensor([  4.1604, -10.4704])\n",
      "              Grad: tensor([-0.2054,  1.1630])\n",
      "Epoch: 554, Loss: 7.018754959106445 \n",
      "              Params: tensor([  4.1625, -10.4821])\n",
      "              Grad: tensor([-0.2051,  1.1610])\n",
      "Epoch: 555, Loss: 7.004870414733887 \n",
      "              Params: tensor([  4.1645, -10.4936])\n",
      "              Grad: tensor([-0.2047,  1.1590])\n",
      "Epoch: 556, Loss: 6.991028308868408 \n",
      "              Params: tensor([  4.1666, -10.5052])\n",
      "              Grad: tensor([-0.2044,  1.1571])\n",
      "Epoch: 557, Loss: 6.977232456207275 \n",
      "              Params: tensor([  4.1686, -10.5168])\n",
      "              Grad: tensor([-0.2041,  1.1551])\n",
      "Epoch: 558, Loss: 6.96348762512207 \n",
      "              Params: tensor([  4.1706, -10.5283])\n",
      "              Grad: tensor([-0.2037,  1.1531])\n",
      "Epoch: 559, Loss: 6.94978666305542 \n",
      "              Params: tensor([  4.1727, -10.5398])\n",
      "              Grad: tensor([-0.2034,  1.1512])\n",
      "Epoch: 560, Loss: 6.9361348152160645 \n",
      "              Params: tensor([  4.1747, -10.5513])\n",
      "              Grad: tensor([-0.2030,  1.1492])\n",
      "Epoch: 561, Loss: 6.922528266906738 \n",
      "              Params: tensor([  4.1767, -10.5628])\n",
      "              Grad: tensor([-0.2027,  1.1473])\n",
      "Epoch: 562, Loss: 6.908966541290283 \n",
      "              Params: tensor([  4.1787, -10.5742])\n",
      "              Grad: tensor([-0.2023,  1.1453])\n",
      "Epoch: 563, Loss: 6.895452499389648 \n",
      "              Params: tensor([  4.1808, -10.5857])\n",
      "              Grad: tensor([-0.2020,  1.1434])\n",
      "Epoch: 564, Loss: 6.8819804191589355 \n",
      "              Params: tensor([  4.1828, -10.5971])\n",
      "              Grad: tensor([-0.2016,  1.1414])\n",
      "Epoch: 565, Loss: 6.868558883666992 \n",
      "              Params: tensor([  4.1848, -10.6085])\n",
      "              Grad: tensor([-0.2013,  1.1395])\n",
      "Epoch: 566, Loss: 6.855180263519287 \n",
      "              Params: tensor([  4.1868, -10.6198])\n",
      "              Grad: tensor([-0.2010,  1.1375])\n",
      "Epoch: 567, Loss: 6.841848373413086 \n",
      "              Params: tensor([  4.1888, -10.6312])\n",
      "              Grad: tensor([-0.2006,  1.1356])\n",
      "Epoch: 568, Loss: 6.828561305999756 \n",
      "              Params: tensor([  4.1908, -10.6425])\n",
      "              Grad: tensor([-0.2003,  1.1337])\n",
      "Epoch: 569, Loss: 6.815318584442139 \n",
      "              Params: tensor([  4.1928, -10.6539])\n",
      "              Grad: tensor([-0.1999,  1.1318])\n",
      "Epoch: 570, Loss: 6.802118301391602 \n",
      "              Params: tensor([  4.1948, -10.6652])\n",
      "              Grad: tensor([-0.1996,  1.1298])\n",
      "Epoch: 571, Loss: 6.788968086242676 \n",
      "              Params: tensor([  4.1968, -10.6764])\n",
      "              Grad: tensor([-0.1993,  1.1279])\n",
      "Epoch: 572, Loss: 6.7758636474609375 \n",
      "              Params: tensor([  4.1988, -10.6877])\n",
      "              Grad: tensor([-0.1989,  1.1260])\n",
      "Epoch: 573, Loss: 6.7627973556518555 \n",
      "              Params: tensor([  4.2008, -10.6989])\n",
      "              Grad: tensor([-0.1986,  1.1241])\n",
      "Epoch: 574, Loss: 6.749778747558594 \n",
      "              Params: tensor([  4.2028, -10.7102])\n",
      "              Grad: tensor([-0.1982,  1.1222])\n",
      "Epoch: 575, Loss: 6.736804008483887 \n",
      "              Params: tensor([  4.2047, -10.7214])\n",
      "              Grad: tensor([-0.1979,  1.1203])\n",
      "Epoch: 576, Loss: 6.723875522613525 \n",
      "              Params: tensor([  4.2067, -10.7325])\n",
      "              Grad: tensor([-0.1976,  1.1184])\n",
      "Epoch: 577, Loss: 6.710986614227295 \n",
      "              Params: tensor([  4.2087, -10.7437])\n",
      "              Grad: tensor([-0.1972,  1.1165])\n",
      "Epoch: 578, Loss: 6.698141574859619 \n",
      "              Params: tensor([  4.2107, -10.7549])\n",
      "              Grad: tensor([-0.1969,  1.1146])\n",
      "Epoch: 579, Loss: 6.68534517288208 \n",
      "              Params: tensor([  4.2126, -10.7660])\n",
      "              Grad: tensor([-0.1966,  1.1127])\n",
      "Epoch: 580, Loss: 6.67258882522583 \n",
      "              Params: tensor([  4.2146, -10.7771])\n",
      "              Grad: tensor([-0.1962,  1.1108])\n",
      "Epoch: 581, Loss: 6.6598734855651855 \n",
      "              Params: tensor([  4.2165, -10.7882])\n",
      "              Grad: tensor([-0.1959,  1.1089])\n",
      "Epoch: 582, Loss: 6.647207260131836 \n",
      "              Params: tensor([  4.2185, -10.7992])\n",
      "              Grad: tensor([-0.1956,  1.1070])\n",
      "Epoch: 583, Loss: 6.634577751159668 \n",
      "              Params: tensor([  4.2204, -10.8103])\n",
      "              Grad: tensor([-0.1952,  1.1051])\n",
      "Epoch: 584, Loss: 6.621994495391846 \n",
      "              Params: tensor([  4.2224, -10.8213])\n",
      "              Grad: tensor([-0.1949,  1.1033])\n",
      "Epoch: 585, Loss: 6.609454154968262 \n",
      "              Params: tensor([  4.2243, -10.8323])\n",
      "              Grad: tensor([-0.1946,  1.1014])\n",
      "Epoch: 586, Loss: 6.59695291519165 \n",
      "              Params: tensor([  4.2263, -10.8433])\n",
      "              Grad: tensor([-0.1942,  1.0995])\n",
      "Epoch: 587, Loss: 6.584498882293701 \n",
      "              Params: tensor([  4.2282, -10.8543])\n",
      "              Grad: tensor([-0.1939,  1.0976])\n",
      "Epoch: 588, Loss: 6.572086811065674 \n",
      "              Params: tensor([  4.2302, -10.8653])\n",
      "              Grad: tensor([-0.1936,  1.0958])\n",
      "Epoch: 589, Loss: 6.5597124099731445 \n",
      "              Params: tensor([  4.2321, -10.8762])\n",
      "              Grad: tensor([-0.1932,  1.0939])\n",
      "Epoch: 590, Loss: 6.547384262084961 \n",
      "              Params: tensor([  4.2340, -10.8871])\n",
      "              Grad: tensor([-0.1929,  1.0921])\n",
      "Epoch: 591, Loss: 6.535097122192383 \n",
      "              Params: tensor([  4.2359, -10.8980])\n",
      "              Grad: tensor([-0.1926,  1.0902])\n",
      "Epoch: 592, Loss: 6.522850513458252 \n",
      "              Params: tensor([  4.2379, -10.9089])\n",
      "              Grad: tensor([-0.1923,  1.0884])\n",
      "Epoch: 593, Loss: 6.510645866394043 \n",
      "              Params: tensor([  4.2398, -10.9198])\n",
      "              Grad: tensor([-0.1919,  1.0865])\n",
      "Epoch: 594, Loss: 6.4984822273254395 \n",
      "              Params: tensor([  4.2417, -10.9306])\n",
      "              Grad: tensor([-0.1916,  1.0847])\n",
      "Epoch: 595, Loss: 6.486360549926758 \n",
      "              Params: tensor([  4.2436, -10.9415])\n",
      "              Grad: tensor([-0.1913,  1.0828])\n",
      "Epoch: 596, Loss: 6.4742817878723145 \n",
      "              Params: tensor([  4.2455, -10.9523])\n",
      "              Grad: tensor([-0.1910,  1.0810])\n",
      "Epoch: 597, Loss: 6.462240695953369 \n",
      "              Params: tensor([  4.2474, -10.9631])\n",
      "              Grad: tensor([-0.1906,  1.0791])\n",
      "Epoch: 598, Loss: 6.45024299621582 \n",
      "              Params: tensor([  4.2493, -10.9738])\n",
      "              Grad: tensor([-0.1903,  1.0773])\n",
      "Epoch: 599, Loss: 6.438284397125244 \n",
      "              Params: tensor([  4.2512, -10.9846])\n",
      "              Grad: tensor([-0.1900,  1.0755])\n",
      "Epoch: 600, Loss: 6.426368236541748 \n",
      "              Params: tensor([  4.2531, -10.9953])\n",
      "              Grad: tensor([-0.1897,  1.0737])\n",
      "Epoch: 601, Loss: 6.41448974609375 \n",
      "              Params: tensor([  4.2550, -11.0060])\n",
      "              Grad: tensor([-0.1893,  1.0718])\n",
      "Epoch: 602, Loss: 6.402653217315674 \n",
      "              Params: tensor([  4.2569, -11.0167])\n",
      "              Grad: tensor([-0.1890,  1.0700])\n",
      "Epoch: 603, Loss: 6.3908586502075195 \n",
      "              Params: tensor([  4.2588, -11.0274])\n",
      "              Grad: tensor([-0.1887,  1.0682])\n",
      "Epoch: 604, Loss: 6.37910270690918 \n",
      "              Params: tensor([  4.2607, -11.0381])\n",
      "              Grad: tensor([-0.1884,  1.0664])\n",
      "Epoch: 605, Loss: 6.367385387420654 \n",
      "              Params: tensor([  4.2626, -11.0487])\n",
      "              Grad: tensor([-0.1880,  1.0646])\n",
      "Epoch: 606, Loss: 6.355705738067627 \n",
      "              Params: tensor([  4.2644, -11.0594])\n",
      "              Grad: tensor([-0.1877,  1.0628])\n",
      "Epoch: 607, Loss: 6.3440704345703125 \n",
      "              Params: tensor([  4.2663, -11.0700])\n",
      "              Grad: tensor([-0.1874,  1.0609])\n",
      "Epoch: 608, Loss: 6.332472324371338 \n",
      "              Params: tensor([  4.2682, -11.0806])\n",
      "              Grad: tensor([-0.1871,  1.0591])\n",
      "Epoch: 609, Loss: 6.320911884307861 \n",
      "              Params: tensor([  4.2701, -11.0911])\n",
      "              Grad: tensor([-0.1868,  1.0573])\n",
      "Epoch: 610, Loss: 6.309394836425781 \n",
      "              Params: tensor([  4.2719, -11.1017])\n",
      "              Grad: tensor([-0.1865,  1.0555])\n",
      "Epoch: 611, Loss: 6.297914505004883 \n",
      "              Params: tensor([  4.2738, -11.1122])\n",
      "              Grad: tensor([-0.1861,  1.0538])\n",
      "Epoch: 612, Loss: 6.286472797393799 \n",
      "              Params: tensor([  4.2756, -11.1227])\n",
      "              Grad: tensor([-0.1858,  1.0520])\n",
      "Epoch: 613, Loss: 6.275073528289795 \n",
      "              Params: tensor([  4.2775, -11.1333])\n",
      "              Grad: tensor([-0.1855,  1.0502])\n",
      "Epoch: 614, Loss: 6.263708114624023 \n",
      "              Params: tensor([  4.2794, -11.1437])\n",
      "              Grad: tensor([-0.1852,  1.0484])\n",
      "Epoch: 615, Loss: 6.252382278442383 \n",
      "              Params: tensor([  4.2812, -11.1542])\n",
      "              Grad: tensor([-0.1849,  1.0466])\n",
      "Epoch: 616, Loss: 6.241097927093506 \n",
      "              Params: tensor([  4.2830, -11.1646])\n",
      "              Grad: tensor([-0.1846,  1.0448])\n",
      "Epoch: 617, Loss: 6.229849338531494 \n",
      "              Params: tensor([  4.2849, -11.1751])\n",
      "              Grad: tensor([-0.1843,  1.0431])\n",
      "Epoch: 618, Loss: 6.218638896942139 \n",
      "              Params: tensor([  4.2867, -11.1855])\n",
      "              Grad: tensor([-0.1840,  1.0413])\n",
      "Epoch: 619, Loss: 6.207470417022705 \n",
      "              Params: tensor([  4.2886, -11.1959])\n",
      "              Grad: tensor([-0.1836,  1.0395])\n",
      "Epoch: 620, Loss: 6.196334362030029 \n",
      "              Params: tensor([  4.2904, -11.2063])\n",
      "              Grad: tensor([-0.1833,  1.0378])\n",
      "Epoch: 621, Loss: 6.185240268707275 \n",
      "              Params: tensor([  4.2922, -11.2166])\n",
      "              Grad: tensor([-0.1830,  1.0360])\n",
      "Epoch: 622, Loss: 6.174180507659912 \n",
      "              Params: tensor([  4.2941, -11.2270])\n",
      "              Grad: tensor([-0.1827,  1.0342])\n",
      "Epoch: 623, Loss: 6.163159370422363 \n",
      "              Params: tensor([  4.2959, -11.2373])\n",
      "              Grad: tensor([-0.1824,  1.0325])\n",
      "Epoch: 624, Loss: 6.152177333831787 \n",
      "              Params: tensor([  4.2977, -11.2476])\n",
      "              Grad: tensor([-0.1821,  1.0307])\n",
      "Epoch: 625, Loss: 6.141229629516602 \n",
      "              Params: tensor([  4.2995, -11.2579])\n",
      "              Grad: tensor([-0.1818,  1.0290])\n",
      "Epoch: 626, Loss: 6.130321979522705 \n",
      "              Params: tensor([  4.3013, -11.2682])\n",
      "              Grad: tensor([-0.1815,  1.0272])\n",
      "Epoch: 627, Loss: 6.119447708129883 \n",
      "              Params: tensor([  4.3031, -11.2784])\n",
      "              Grad: tensor([-0.1811,  1.0255])\n",
      "Epoch: 628, Loss: 6.108613967895508 \n",
      "              Params: tensor([  4.3050, -11.2887])\n",
      "              Grad: tensor([-0.1808,  1.0237])\n",
      "Epoch: 629, Loss: 6.097814559936523 \n",
      "              Params: tensor([  4.3068, -11.2989])\n",
      "              Grad: tensor([-0.1805,  1.0220])\n",
      "Epoch: 630, Loss: 6.087054252624512 \n",
      "              Params: tensor([  4.3086, -11.3091])\n",
      "              Grad: tensor([-0.1802,  1.0203])\n",
      "Epoch: 631, Loss: 6.076329231262207 \n",
      "              Params: tensor([  4.3104, -11.3193])\n",
      "              Grad: tensor([-0.1799,  1.0185])\n",
      "Epoch: 632, Loss: 6.065643787384033 \n",
      "              Params: tensor([  4.3122, -11.3294])\n",
      "              Grad: tensor([-0.1796,  1.0168])\n",
      "Epoch: 633, Loss: 6.054988384246826 \n",
      "              Params: tensor([  4.3139, -11.3396])\n",
      "              Grad: tensor([-0.1793,  1.0151])\n",
      "Epoch: 634, Loss: 6.044372081756592 \n",
      "              Params: tensor([  4.3157, -11.3497])\n",
      "              Grad: tensor([-0.1790,  1.0133])\n",
      "Epoch: 635, Loss: 6.033793926239014 \n",
      "              Params: tensor([  4.3175, -11.3598])\n",
      "              Grad: tensor([-0.1787,  1.0116])\n",
      "Epoch: 636, Loss: 6.023247241973877 \n",
      "              Params: tensor([  4.3193, -11.3699])\n",
      "              Grad: tensor([-0.1784,  1.0099])\n",
      "Epoch: 637, Loss: 6.01273775100708 \n",
      "              Params: tensor([  4.3211, -11.3800])\n",
      "              Grad: tensor([-0.1781,  1.0082])\n",
      "Epoch: 638, Loss: 6.002264022827148 \n",
      "              Params: tensor([  4.3229, -11.3901])\n",
      "              Grad: tensor([-0.1778,  1.0065])\n",
      "Epoch: 639, Loss: 5.991828441619873 \n",
      "              Params: tensor([  4.3246, -11.4001])\n",
      "              Grad: tensor([-0.1775,  1.0048])\n",
      "Epoch: 640, Loss: 5.9814252853393555 \n",
      "              Params: tensor([  4.3264, -11.4102])\n",
      "              Grad: tensor([-0.1772,  1.0031])\n",
      "Epoch: 641, Loss: 5.971058368682861 \n",
      "              Params: tensor([  4.3282, -11.4202])\n",
      "              Grad: tensor([-0.1769,  1.0014])\n",
      "Epoch: 642, Loss: 5.960727214813232 \n",
      "              Params: tensor([  4.3300, -11.4302])\n",
      "              Grad: tensor([-0.1766,  0.9997])\n",
      "Epoch: 643, Loss: 5.950432300567627 \n",
      "              Params: tensor([  4.3317, -11.4401])\n",
      "              Grad: tensor([-0.1763,  0.9980])\n",
      "Epoch: 644, Loss: 5.940170764923096 \n",
      "              Params: tensor([  4.3335, -11.4501])\n",
      "              Grad: tensor([-0.1760,  0.9963])\n",
      "Epoch: 645, Loss: 5.929944038391113 \n",
      "              Params: tensor([  4.3352, -11.4601])\n",
      "              Grad: tensor([-0.1757,  0.9946])\n",
      "Epoch: 646, Loss: 5.91975212097168 \n",
      "              Params: tensor([  4.3370, -11.4700])\n",
      "              Grad: tensor([-0.1754,  0.9929])\n",
      "Epoch: 647, Loss: 5.9095964431762695 \n",
      "              Params: tensor([  4.3387, -11.4799])\n",
      "              Grad: tensor([-0.1751,  0.9912])\n",
      "Epoch: 648, Loss: 5.899472236633301 \n",
      "              Params: tensor([  4.3405, -11.4898])\n",
      "              Grad: tensor([-0.1748,  0.9895])\n",
      "Epoch: 649, Loss: 5.889383316040039 \n",
      "              Params: tensor([  4.3422, -11.4997])\n",
      "              Grad: tensor([-0.1745,  0.9878])\n",
      "Epoch: 650, Loss: 5.879326343536377 \n",
      "              Params: tensor([  4.3440, -11.5095])\n",
      "              Grad: tensor([-0.1742,  0.9862])\n",
      "Epoch: 651, Loss: 5.86931037902832 \n",
      "              Params: tensor([  4.3457, -11.5194])\n",
      "              Grad: tensor([-0.1739,  0.9845])\n",
      "Epoch: 652, Loss: 5.8593220710754395 \n",
      "              Params: tensor([  4.3474, -11.5292])\n",
      "              Grad: tensor([-0.1736,  0.9828])\n",
      "Epoch: 653, Loss: 5.849374294281006 \n",
      "              Params: tensor([  4.3492, -11.5390])\n",
      "              Grad: tensor([-0.1733,  0.9811])\n",
      "Epoch: 654, Loss: 5.839453220367432 \n",
      "              Params: tensor([  4.3509, -11.5488])\n",
      "              Grad: tensor([-0.1730,  0.9795])\n",
      "Epoch: 655, Loss: 5.8295698165893555 \n",
      "              Params: tensor([  4.3526, -11.5586])\n",
      "              Grad: tensor([-0.1727,  0.9778])\n",
      "Epoch: 656, Loss: 5.819717884063721 \n",
      "              Params: tensor([  4.3544, -11.5683])\n",
      "              Grad: tensor([-0.1724,  0.9761])\n",
      "Epoch: 657, Loss: 5.809900760650635 \n",
      "              Params: tensor([  4.3561, -11.5781])\n",
      "              Grad: tensor([-0.1722,  0.9745])\n",
      "Epoch: 658, Loss: 5.800116062164307 \n",
      "              Params: tensor([  4.3578, -11.5878])\n",
      "              Grad: tensor([-0.1719,  0.9728])\n",
      "Epoch: 659, Loss: 5.7903666496276855 \n",
      "              Params: tensor([  4.3595, -11.5975])\n",
      "              Grad: tensor([-0.1716,  0.9712])\n",
      "Epoch: 660, Loss: 5.780646324157715 \n",
      "              Params: tensor([  4.3612, -11.6072])\n",
      "              Grad: tensor([-0.1713,  0.9695])\n",
      "Epoch: 661, Loss: 5.770962238311768 \n",
      "              Params: tensor([  4.3629, -11.6169])\n",
      "              Grad: tensor([-0.1710,  0.9679])\n",
      "Epoch: 662, Loss: 5.7613115310668945 \n",
      "              Params: tensor([  4.3646, -11.6266])\n",
      "              Grad: tensor([-0.1707,  0.9662])\n",
      "Epoch: 663, Loss: 5.7516937255859375 \n",
      "              Params: tensor([  4.3664, -11.6362])\n",
      "              Grad: tensor([-0.1704,  0.9646])\n",
      "Epoch: 664, Loss: 5.742105484008789 \n",
      "              Params: tensor([  4.3681, -11.6458])\n",
      "              Grad: tensor([-0.1701,  0.9630])\n",
      "Epoch: 665, Loss: 5.732549667358398 \n",
      "              Params: tensor([  4.3697, -11.6555])\n",
      "              Grad: tensor([-0.1698,  0.9613])\n",
      "Epoch: 666, Loss: 5.723031044006348 \n",
      "              Params: tensor([  4.3714, -11.6651])\n",
      "              Grad: tensor([-0.1695,  0.9597])\n",
      "Epoch: 667, Loss: 5.7135396003723145 \n",
      "              Params: tensor([  4.3731, -11.6746])\n",
      "              Grad: tensor([-0.1692,  0.9581])\n",
      "Epoch: 668, Loss: 5.704083442687988 \n",
      "              Params: tensor([  4.3748, -11.6842])\n",
      "              Grad: tensor([-0.1690,  0.9564])\n",
      "Epoch: 669, Loss: 5.6946587562561035 \n",
      "              Params: tensor([  4.3765, -11.6937])\n",
      "              Grad: tensor([-0.1687,  0.9548])\n",
      "Epoch: 670, Loss: 5.68526554107666 \n",
      "              Params: tensor([  4.3782, -11.7033])\n",
      "              Grad: tensor([-0.1684,  0.9532])\n",
      "Epoch: 671, Loss: 5.675903797149658 \n",
      "              Params: tensor([  4.3799, -11.7128])\n",
      "              Grad: tensor([-0.1681,  0.9516])\n",
      "Epoch: 672, Loss: 5.6665730476379395 \n",
      "              Params: tensor([  4.3816, -11.7223])\n",
      "              Grad: tensor([-0.1678,  0.9499])\n",
      "Epoch: 673, Loss: 5.6572771072387695 \n",
      "              Params: tensor([  4.3832, -11.7318])\n",
      "              Grad: tensor([-0.1675,  0.9483])\n",
      "Epoch: 674, Loss: 5.648009777069092 \n",
      "              Params: tensor([  4.3849, -11.7412])\n",
      "              Grad: tensor([-0.1673,  0.9467])\n",
      "Epoch: 675, Loss: 5.6387763023376465 \n",
      "              Params: tensor([  4.3866, -11.7507])\n",
      "              Grad: tensor([-0.1670,  0.9451])\n",
      "Epoch: 676, Loss: 5.629574298858643 \n",
      "              Params: tensor([  4.3882, -11.7601])\n",
      "              Grad: tensor([-0.1667,  0.9435])\n",
      "Epoch: 677, Loss: 5.6204023361206055 \n",
      "              Params: tensor([  4.3899, -11.7696])\n",
      "              Grad: tensor([-0.1664,  0.9419])\n",
      "Epoch: 678, Loss: 5.611259937286377 \n",
      "              Params: tensor([  4.3916, -11.7790])\n",
      "              Grad: tensor([-0.1661,  0.9403])\n",
      "Epoch: 679, Loss: 5.602148532867432 \n",
      "              Params: tensor([  4.3932, -11.7883])\n",
      "              Grad: tensor([-0.1658,  0.9387])\n",
      "Epoch: 680, Loss: 5.593070983886719 \n",
      "              Params: tensor([  4.3949, -11.7977])\n",
      "              Grad: tensor([-0.1656,  0.9371])\n",
      "Epoch: 681, Loss: 5.584022045135498 \n",
      "              Params: tensor([  4.3965, -11.8071])\n",
      "              Grad: tensor([-0.1653,  0.9355])\n",
      "Epoch: 682, Loss: 5.575005054473877 \n",
      "              Params: tensor([  4.3982, -11.8164])\n",
      "              Grad: tensor([-0.1650,  0.9339])\n",
      "Epoch: 683, Loss: 5.566019058227539 \n",
      "              Params: tensor([  4.3998, -11.8257])\n",
      "              Grad: tensor([-0.1647,  0.9323])\n",
      "Epoch: 684, Loss: 5.557063102722168 \n",
      "              Params: tensor([  4.4015, -11.8350])\n",
      "              Grad: tensor([-0.1644,  0.9308])\n",
      "Epoch: 685, Loss: 5.548136234283447 \n",
      "              Params: tensor([  4.4031, -11.8443])\n",
      "              Grad: tensor([-0.1641,  0.9292])\n",
      "Epoch: 686, Loss: 5.539241313934326 \n",
      "              Params: tensor([  4.4048, -11.8536])\n",
      "              Grad: tensor([-0.1639,  0.9276])\n",
      "Epoch: 687, Loss: 5.530376434326172 \n",
      "              Params: tensor([  4.4064, -11.8629])\n",
      "              Grad: tensor([-0.1636,  0.9260])\n",
      "Epoch: 688, Loss: 5.521539688110352 \n",
      "              Params: tensor([  4.4080, -11.8721])\n",
      "              Grad: tensor([-0.1633,  0.9245])\n",
      "Epoch: 689, Loss: 5.5127339363098145 \n",
      "              Params: tensor([  4.4097, -11.8813])\n",
      "              Grad: tensor([-0.1630,  0.9229])\n",
      "Epoch: 690, Loss: 5.503957748413086 \n",
      "              Params: tensor([  4.4113, -11.8906])\n",
      "              Grad: tensor([-0.1628,  0.9213])\n",
      "Epoch: 691, Loss: 5.495211601257324 \n",
      "              Params: tensor([  4.4129, -11.8998])\n",
      "              Grad: tensor([-0.1625,  0.9197])\n",
      "Epoch: 692, Loss: 5.4864959716796875 \n",
      "              Params: tensor([  4.4145, -11.9089])\n",
      "              Grad: tensor([-0.1622,  0.9182])\n",
      "Epoch: 693, Loss: 5.477807998657227 \n",
      "              Params: tensor([  4.4161, -11.9181])\n",
      "              Grad: tensor([-0.1619,  0.9166])\n",
      "Epoch: 694, Loss: 5.469152450561523 \n",
      "              Params: tensor([  4.4178, -11.9272])\n",
      "              Grad: tensor([-0.1617,  0.9151])\n",
      "Epoch: 695, Loss: 5.460525035858154 \n",
      "              Params: tensor([  4.4194, -11.9364])\n",
      "              Grad: tensor([-0.1614,  0.9135])\n",
      "Epoch: 696, Loss: 5.451927661895752 \n",
      "              Params: tensor([  4.4210, -11.9455])\n",
      "              Grad: tensor([-0.1611,  0.9120])\n",
      "Epoch: 697, Loss: 5.443358421325684 \n",
      "              Params: tensor([  4.4226, -11.9546])\n",
      "              Grad: tensor([-0.1608,  0.9104])\n",
      "Epoch: 698, Loss: 5.434819221496582 \n",
      "              Params: tensor([  4.4242, -11.9637])\n",
      "              Grad: tensor([-0.1605,  0.9089])\n",
      "Epoch: 699, Loss: 5.426309108734131 \n",
      "              Params: tensor([  4.4258, -11.9728])\n",
      "              Grad: tensor([-0.1603,  0.9073])\n",
      "Epoch: 700, Loss: 5.4178266525268555 \n",
      "              Params: tensor([  4.4274, -11.9818])\n",
      "              Grad: tensor([-0.1600,  0.9058])\n",
      "Epoch: 701, Loss: 5.409372329711914 \n",
      "              Params: tensor([  4.4290, -11.9909])\n",
      "              Grad: tensor([-0.1597,  0.9042])\n",
      "Epoch: 702, Loss: 5.400949478149414 \n",
      "              Params: tensor([  4.4306, -11.9999])\n",
      "              Grad: tensor([-0.1595,  0.9027])\n",
      "Epoch: 703, Loss: 5.392550468444824 \n",
      "              Params: tensor([  4.4322, -12.0089])\n",
      "              Grad: tensor([-0.1592,  0.9012])\n",
      "Epoch: 704, Loss: 5.38418436050415 \n",
      "              Params: tensor([  4.4338, -12.0179])\n",
      "              Grad: tensor([-0.1589,  0.8996])\n",
      "Epoch: 705, Loss: 5.3758463859558105 \n",
      "              Params: tensor([  4.4354, -12.0269])\n",
      "              Grad: tensor([-0.1586,  0.8981])\n",
      "Epoch: 706, Loss: 5.367536544799805 \n",
      "              Params: tensor([  4.4369, -12.0359])\n",
      "              Grad: tensor([-0.1584,  0.8966])\n",
      "Epoch: 707, Loss: 5.359253406524658 \n",
      "              Params: tensor([  4.4385, -12.0448])\n",
      "              Grad: tensor([-0.1581,  0.8951])\n",
      "Epoch: 708, Loss: 5.350998878479004 \n",
      "              Params: tensor([  4.4401, -12.0537])\n",
      "              Grad: tensor([-0.1578,  0.8935])\n",
      "Epoch: 709, Loss: 5.342771530151367 \n",
      "              Params: tensor([  4.4417, -12.0627])\n",
      "              Grad: tensor([-0.1576,  0.8920])\n",
      "Epoch: 710, Loss: 5.334575176239014 \n",
      "              Params: tensor([  4.4433, -12.0716])\n",
      "              Grad: tensor([-0.1573,  0.8905])\n",
      "Epoch: 711, Loss: 5.326402187347412 \n",
      "              Params: tensor([  4.4448, -12.0805])\n",
      "              Grad: tensor([-0.1570,  0.8890])\n",
      "Epoch: 712, Loss: 5.3182597160339355 \n",
      "              Params: tensor([  4.4464, -12.0893])\n",
      "              Grad: tensor([-0.1568,  0.8875])\n",
      "Epoch: 713, Loss: 5.310144424438477 \n",
      "              Params: tensor([  4.4480, -12.0982])\n",
      "              Grad: tensor([-0.1565,  0.8860])\n",
      "Epoch: 714, Loss: 5.3020548820495605 \n",
      "              Params: tensor([  4.4495, -12.1070])\n",
      "              Grad: tensor([-0.1562,  0.8845])\n",
      "Epoch: 715, Loss: 5.293994426727295 \n",
      "              Params: tensor([  4.4511, -12.1159])\n",
      "              Grad: tensor([-0.1560,  0.8830])\n",
      "Epoch: 716, Loss: 5.285963535308838 \n",
      "              Params: tensor([  4.4526, -12.1247])\n",
      "              Grad: tensor([-0.1557,  0.8815])\n",
      "Epoch: 717, Loss: 5.277958393096924 \n",
      "              Params: tensor([  4.4542, -12.1335])\n",
      "              Grad: tensor([-0.1555,  0.8800])\n",
      "Epoch: 718, Loss: 5.269979476928711 \n",
      "              Params: tensor([  4.4557, -12.1423])\n",
      "              Grad: tensor([-0.1552,  0.8785])\n",
      "Epoch: 719, Loss: 5.262026786804199 \n",
      "              Params: tensor([  4.4573, -12.1510])\n",
      "              Grad: tensor([-0.1549,  0.8770])\n",
      "Epoch: 720, Loss: 5.25410270690918 \n",
      "              Params: tensor([  4.4588, -12.1598])\n",
      "              Grad: tensor([-0.1547,  0.8755])\n",
      "Epoch: 721, Loss: 5.2462053298950195 \n",
      "              Params: tensor([  4.4604, -12.1685])\n",
      "              Grad: tensor([-0.1544,  0.8740])\n",
      "Epoch: 722, Loss: 5.238335132598877 \n",
      "              Params: tensor([  4.4619, -12.1773])\n",
      "              Grad: tensor([-0.1541,  0.8725])\n",
      "Epoch: 723, Loss: 5.230491638183594 \n",
      "              Params: tensor([  4.4635, -12.1860])\n",
      "              Grad: tensor([-0.1539,  0.8710])\n",
      "Epoch: 724, Loss: 5.2226738929748535 \n",
      "              Params: tensor([  4.4650, -12.1947])\n",
      "              Grad: tensor([-0.1536,  0.8696])\n",
      "Epoch: 725, Loss: 5.21488094329834 \n",
      "              Params: tensor([  4.4665, -12.2033])\n",
      "              Grad: tensor([-0.1533,  0.8681])\n",
      "Epoch: 726, Loss: 5.207119941711426 \n",
      "              Params: tensor([  4.4681, -12.2120])\n",
      "              Grad: tensor([-0.1531,  0.8666])\n",
      "Epoch: 727, Loss: 5.199380874633789 \n",
      "              Params: tensor([  4.4696, -12.2207])\n",
      "              Grad: tensor([-0.1528,  0.8651])\n",
      "Epoch: 728, Loss: 5.191669940948486 \n",
      "              Params: tensor([  4.4711, -12.2293])\n",
      "              Grad: tensor([-0.1526,  0.8637])\n",
      "Epoch: 729, Loss: 5.183984756469727 \n",
      "              Params: tensor([  4.4726, -12.2379])\n",
      "              Grad: tensor([-0.1523,  0.8622])\n",
      "Epoch: 730, Loss: 5.176324367523193 \n",
      "              Params: tensor([  4.4742, -12.2465])\n",
      "              Grad: tensor([-0.1520,  0.8607])\n",
      "Epoch: 731, Loss: 5.16868782043457 \n",
      "              Params: tensor([  4.4757, -12.2551])\n",
      "              Grad: tensor([-0.1518,  0.8593])\n",
      "Epoch: 732, Loss: 5.161084175109863 \n",
      "              Params: tensor([  4.4772, -12.2637])\n",
      "              Grad: tensor([-0.1515,  0.8578])\n",
      "Epoch: 733, Loss: 5.153499603271484 \n",
      "              Params: tensor([  4.4787, -12.2723])\n",
      "              Grad: tensor([-0.1513,  0.8564])\n",
      "Epoch: 734, Loss: 5.145944118499756 \n",
      "              Params: tensor([  4.4802, -12.2808])\n",
      "              Grad: tensor([-0.1510,  0.8549])\n",
      "Epoch: 735, Loss: 5.138412952423096 \n",
      "              Params: tensor([  4.4817, -12.2893])\n",
      "              Grad: tensor([-0.1508,  0.8535])\n",
      "Epoch: 736, Loss: 5.1309099197387695 \n",
      "              Params: tensor([  4.4832, -12.2979])\n",
      "              Grad: tensor([-0.1505,  0.8520])\n",
      "Epoch: 737, Loss: 5.123427867889404 \n",
      "              Params: tensor([  4.4847, -12.3064])\n",
      "              Grad: tensor([-0.1502,  0.8506])\n",
      "Epoch: 738, Loss: 5.115977764129639 \n",
      "              Params: tensor([  4.4862, -12.3149])\n",
      "              Grad: tensor([-0.1500,  0.8491])\n",
      "Epoch: 739, Loss: 5.108546733856201 \n",
      "              Params: tensor([  4.4877, -12.3233])\n",
      "              Grad: tensor([-0.1497,  0.8477])\n",
      "Epoch: 740, Loss: 5.1011433601379395 \n",
      "              Params: tensor([  4.4892, -12.3318])\n",
      "              Grad: tensor([-0.1495,  0.8462])\n",
      "Epoch: 741, Loss: 5.0937652587890625 \n",
      "              Params: tensor([  4.4907, -12.3402])\n",
      "              Grad: tensor([-0.1492,  0.8448])\n",
      "Epoch: 742, Loss: 5.086413860321045 \n",
      "              Params: tensor([  4.4922, -12.3487])\n",
      "              Grad: tensor([-0.1490,  0.8434])\n",
      "Epoch: 743, Loss: 5.079085826873779 \n",
      "              Params: tensor([  4.4937, -12.3571])\n",
      "              Grad: tensor([-0.1487,  0.8419])\n",
      "Epoch: 744, Loss: 5.071781158447266 \n",
      "              Params: tensor([  4.4952, -12.3655])\n",
      "              Grad: tensor([-0.1485,  0.8405])\n",
      "Epoch: 745, Loss: 5.064504623413086 \n",
      "              Params: tensor([  4.4967, -12.3739])\n",
      "              Grad: tensor([-0.1482,  0.8391])\n",
      "Epoch: 746, Loss: 5.057246685028076 \n",
      "              Params: tensor([  4.4981, -12.3823])\n",
      "              Grad: tensor([-0.1480,  0.8376])\n",
      "Epoch: 747, Loss: 5.050021171569824 \n",
      "              Params: tensor([  4.4996, -12.3906])\n",
      "              Grad: tensor([-0.1477,  0.8362])\n",
      "Epoch: 748, Loss: 5.042816638946533 \n",
      "              Params: tensor([  4.5011, -12.3990])\n",
      "              Grad: tensor([-0.1475,  0.8348])\n",
      "Epoch: 749, Loss: 5.035635948181152 \n",
      "              Params: tensor([  4.5026, -12.4073])\n",
      "              Grad: tensor([-0.1472,  0.8334])\n",
      "Epoch: 750, Loss: 5.028476238250732 \n",
      "              Params: tensor([  4.5040, -12.4156])\n",
      "              Grad: tensor([-0.1470,  0.8320])\n",
      "Epoch: 751, Loss: 5.021346092224121 \n",
      "              Params: tensor([  4.5055, -12.4239])\n",
      "              Grad: tensor([-0.1467,  0.8305])\n",
      "Epoch: 752, Loss: 5.01423978805542 \n",
      "              Params: tensor([  4.5070, -12.4322])\n",
      "              Grad: tensor([-0.1465,  0.8291])\n",
      "Epoch: 753, Loss: 5.007157325744629 \n",
      "              Params: tensor([  4.5084, -12.4405])\n",
      "              Grad: tensor([-0.1462,  0.8277])\n",
      "Epoch: 754, Loss: 5.000098705291748 \n",
      "              Params: tensor([  4.5099, -12.4488])\n",
      "              Grad: tensor([-0.1460,  0.8263])\n",
      "Epoch: 755, Loss: 4.9930644035339355 \n",
      "              Params: tensor([  4.5113, -12.4570])\n",
      "              Grad: tensor([-0.1457,  0.8249])\n",
      "Epoch: 756, Loss: 4.986050605773926 \n",
      "              Params: tensor([  4.5128, -12.4653])\n",
      "              Grad: tensor([-0.1455,  0.8235])\n",
      "Epoch: 757, Loss: 4.979064464569092 \n",
      "              Params: tensor([  4.5143, -12.4735])\n",
      "              Grad: tensor([-0.1452,  0.8221])\n",
      "Epoch: 758, Loss: 4.972100257873535 \n",
      "              Params: tensor([  4.5157, -12.4817])\n",
      "              Grad: tensor([-0.1450,  0.8207])\n",
      "Epoch: 759, Loss: 4.965158939361572 \n",
      "              Params: tensor([  4.5172, -12.4899])\n",
      "              Grad: tensor([-0.1447,  0.8193])\n",
      "Epoch: 760, Loss: 4.958244800567627 \n",
      "              Params: tensor([  4.5186, -12.4981])\n",
      "              Grad: tensor([-0.1445,  0.8179])\n",
      "Epoch: 761, Loss: 4.951350688934326 \n",
      "              Params: tensor([  4.5200, -12.5062])\n",
      "              Grad: tensor([-0.1443,  0.8165])\n",
      "Epoch: 762, Loss: 4.944478988647461 \n",
      "              Params: tensor([  4.5215, -12.5144])\n",
      "              Grad: tensor([-0.1440,  0.8152])\n",
      "Epoch: 763, Loss: 4.9376325607299805 \n",
      "              Params: tensor([  4.5229, -12.5225])\n",
      "              Grad: tensor([-0.1438,  0.8138])\n",
      "Epoch: 764, Loss: 4.930812358856201 \n",
      "              Params: tensor([  4.5244, -12.5306])\n",
      "              Grad: tensor([-0.1435,  0.8124])\n",
      "Epoch: 765, Loss: 4.924009323120117 \n",
      "              Params: tensor([  4.5258, -12.5387])\n",
      "              Grad: tensor([-0.1433,  0.8110])\n",
      "Epoch: 766, Loss: 4.917233943939209 \n",
      "              Params: tensor([  4.5272, -12.5468])\n",
      "              Grad: tensor([-0.1430,  0.8096])\n",
      "Epoch: 767, Loss: 4.91048002243042 \n",
      "              Params: tensor([  4.5286, -12.5549])\n",
      "              Grad: tensor([-0.1428,  0.8083])\n",
      "Epoch: 768, Loss: 4.903748989105225 \n",
      "              Params: tensor([  4.5301, -12.5630])\n",
      "              Grad: tensor([-0.1426,  0.8069])\n",
      "Epoch: 769, Loss: 4.897039890289307 \n",
      "              Params: tensor([  4.5315, -12.5711])\n",
      "              Grad: tensor([-0.1423,  0.8055])\n",
      "Epoch: 770, Loss: 4.890355587005615 \n",
      "              Params: tensor([  4.5329, -12.5791])\n",
      "              Grad: tensor([-0.1420,  0.8042])\n",
      "Epoch: 771, Loss: 4.883691787719727 \n",
      "              Params: tensor([  4.5343, -12.5871])\n",
      "              Grad: tensor([-0.1418,  0.8028])\n",
      "Epoch: 772, Loss: 4.877051830291748 \n",
      "              Params: tensor([  4.5357, -12.5951])\n",
      "              Grad: tensor([-0.1416,  0.8014])\n",
      "Epoch: 773, Loss: 4.87043571472168 \n",
      "              Params: tensor([  4.5372, -12.6031])\n",
      "              Grad: tensor([-0.1413,  0.8001])\n",
      "Epoch: 774, Loss: 4.8638386726379395 \n",
      "              Params: tensor([  4.5386, -12.6111])\n",
      "              Grad: tensor([-0.1411,  0.7987])\n",
      "Epoch: 775, Loss: 4.8572678565979 \n",
      "              Params: tensor([  4.5400, -12.6191])\n",
      "              Grad: tensor([-0.1408,  0.7973])\n",
      "Epoch: 776, Loss: 4.850717544555664 \n",
      "              Params: tensor([  4.5414, -12.6271])\n",
      "              Grad: tensor([-0.1406,  0.7960])\n",
      "Epoch: 777, Loss: 4.844189167022705 \n",
      "              Params: tensor([  4.5428, -12.6350])\n",
      "              Grad: tensor([-0.1404,  0.7946])\n",
      "Epoch: 778, Loss: 4.837683200836182 \n",
      "              Params: tensor([  4.5442, -12.6429])\n",
      "              Grad: tensor([-0.1401,  0.7933])\n",
      "Epoch: 779, Loss: 4.831196308135986 \n",
      "              Params: tensor([  4.5456, -12.6509])\n",
      "              Grad: tensor([-0.1399,  0.7919])\n",
      "Epoch: 780, Loss: 4.824736595153809 \n",
      "              Params: tensor([  4.5470, -12.6588])\n",
      "              Grad: tensor([-0.1397,  0.7906])\n",
      "Epoch: 781, Loss: 4.818297863006592 \n",
      "              Params: tensor([  4.5484, -12.6667])\n",
      "              Grad: tensor([-0.1394,  0.7893])\n",
      "Epoch: 782, Loss: 4.8118791580200195 \n",
      "              Params: tensor([  4.5498, -12.6745])\n",
      "              Grad: tensor([-0.1392,  0.7879])\n",
      "Epoch: 783, Loss: 4.80548095703125 \n",
      "              Params: tensor([  4.5512, -12.6824])\n",
      "              Grad: tensor([-0.1389,  0.7866])\n",
      "Epoch: 784, Loss: 4.799106121063232 \n",
      "              Params: tensor([  4.5525, -12.6902])\n",
      "              Grad: tensor([-0.1387,  0.7852])\n",
      "Epoch: 785, Loss: 4.792754650115967 \n",
      "              Params: tensor([  4.5539, -12.6981])\n",
      "              Grad: tensor([-0.1385,  0.7839])\n",
      "Epoch: 786, Loss: 4.786421775817871 \n",
      "              Params: tensor([  4.5553, -12.7059])\n",
      "              Grad: tensor([-0.1383,  0.7826])\n",
      "Epoch: 787, Loss: 4.780111789703369 \n",
      "              Params: tensor([  4.5567, -12.7137])\n",
      "              Grad: tensor([-0.1380,  0.7812])\n",
      "Epoch: 788, Loss: 4.7738237380981445 \n",
      "              Params: tensor([  4.5581, -12.7215])\n",
      "              Grad: tensor([-0.1378,  0.7799])\n",
      "Epoch: 789, Loss: 4.7675580978393555 \n",
      "              Params: tensor([  4.5594, -12.7293])\n",
      "              Grad: tensor([-0.1375,  0.7786])\n",
      "Epoch: 790, Loss: 4.7613115310668945 \n",
      "              Params: tensor([  4.5608, -12.7371])\n",
      "              Grad: tensor([-0.1373,  0.7773])\n",
      "Epoch: 791, Loss: 4.755086898803711 \n",
      "              Params: tensor([  4.5622, -12.7448])\n",
      "              Grad: tensor([-0.1371,  0.7759])\n",
      "Epoch: 792, Loss: 4.748885154724121 \n",
      "              Params: tensor([  4.5636, -12.7526])\n",
      "              Grad: tensor([-0.1368,  0.7746])\n",
      "Epoch: 793, Loss: 4.742700099945068 \n",
      "              Params: tensor([  4.5649, -12.7603])\n",
      "              Grad: tensor([-0.1366,  0.7733])\n",
      "Epoch: 794, Loss: 4.736537456512451 \n",
      "              Params: tensor([  4.5663, -12.7680])\n",
      "              Grad: tensor([-0.1364,  0.7720])\n",
      "Epoch: 795, Loss: 4.730396747589111 \n",
      "              Params: tensor([  4.5677, -12.7758])\n",
      "              Grad: tensor([-0.1361,  0.7707])\n",
      "Epoch: 796, Loss: 4.724279403686523 \n",
      "              Params: tensor([  4.5690, -12.7834])\n",
      "              Grad: tensor([-0.1359,  0.7694])\n",
      "Epoch: 797, Loss: 4.718181133270264 \n",
      "              Params: tensor([  4.5704, -12.7911])\n",
      "              Grad: tensor([-0.1357,  0.7681])\n",
      "Epoch: 798, Loss: 4.712100982666016 \n",
      "              Params: tensor([  4.5717, -12.7988])\n",
      "              Grad: tensor([-0.1354,  0.7668])\n",
      "Epoch: 799, Loss: 4.706046104431152 \n",
      "              Params: tensor([  4.5731, -12.8064])\n",
      "              Grad: tensor([-0.1352,  0.7655])\n",
      "Epoch: 800, Loss: 4.700008869171143 \n",
      "              Params: tensor([  4.5744, -12.8141])\n",
      "              Grad: tensor([-0.1350,  0.7642])\n",
      "Epoch: 801, Loss: 4.6939897537231445 \n",
      "              Params: tensor([  4.5758, -12.8217])\n",
      "              Grad: tensor([-0.1347,  0.7629])\n",
      "Epoch: 802, Loss: 4.687995433807373 \n",
      "              Params: tensor([  4.5771, -12.8293])\n",
      "              Grad: tensor([-0.1345,  0.7616])\n",
      "Epoch: 803, Loss: 4.6820197105407715 \n",
      "              Params: tensor([  4.5785, -12.8369])\n",
      "              Grad: tensor([-0.1343,  0.7603])\n",
      "Epoch: 804, Loss: 4.676063060760498 \n",
      "              Params: tensor([  4.5798, -12.8445])\n",
      "              Grad: tensor([-0.1341,  0.7590])\n",
      "Epoch: 805, Loss: 4.670130252838135 \n",
      "              Params: tensor([  4.5811, -12.8521])\n",
      "              Grad: tensor([-0.1338,  0.7577])\n",
      "Epoch: 806, Loss: 4.664214134216309 \n",
      "              Params: tensor([  4.5825, -12.8597])\n",
      "              Grad: tensor([-0.1336,  0.7564])\n",
      "Epoch: 807, Loss: 4.658319473266602 \n",
      "              Params: tensor([  4.5838, -12.8672])\n",
      "              Grad: tensor([-0.1334,  0.7551])\n",
      "Epoch: 808, Loss: 4.652444839477539 \n",
      "              Params: tensor([  4.5851, -12.8748])\n",
      "              Grad: tensor([-0.1332,  0.7538])\n",
      "Epoch: 809, Loss: 4.646592140197754 \n",
      "              Params: tensor([  4.5865, -12.8823])\n",
      "              Grad: tensor([-0.1330,  0.7526])\n",
      "Epoch: 810, Loss: 4.640753746032715 \n",
      "              Params: tensor([  4.5878, -12.8898])\n",
      "              Grad: tensor([-0.1327,  0.7513])\n",
      "Epoch: 811, Loss: 4.634937763214111 \n",
      "              Params: tensor([  4.5891, -12.8973])\n",
      "              Grad: tensor([-0.1325,  0.7500])\n",
      "Epoch: 812, Loss: 4.6291422843933105 \n",
      "              Params: tensor([  4.5904, -12.9048])\n",
      "              Grad: tensor([-0.1323,  0.7487])\n",
      "Epoch: 813, Loss: 4.6233673095703125 \n",
      "              Params: tensor([  4.5918, -12.9123])\n",
      "              Grad: tensor([-0.1320,  0.7475])\n",
      "Epoch: 814, Loss: 4.617611408233643 \n",
      "              Params: tensor([  4.5931, -12.9197])\n",
      "              Grad: tensor([-0.1318,  0.7462])\n",
      "Epoch: 815, Loss: 4.61187219619751 \n",
      "              Params: tensor([  4.5944, -12.9272])\n",
      "              Grad: tensor([-0.1316,  0.7449])\n",
      "Epoch: 816, Loss: 4.606155872344971 \n",
      "              Params: tensor([  4.5957, -12.9346])\n",
      "              Grad: tensor([-0.1314,  0.7437])\n",
      "Epoch: 817, Loss: 4.600458145141602 \n",
      "              Params: tensor([  4.5970, -12.9420])\n",
      "              Grad: tensor([-0.1311,  0.7424])\n",
      "Epoch: 818, Loss: 4.594779968261719 \n",
      "              Params: tensor([  4.5983, -12.9494])\n",
      "              Grad: tensor([-0.1309,  0.7411])\n",
      "Epoch: 819, Loss: 4.5891194343566895 \n",
      "              Params: tensor([  4.5996, -12.9568])\n",
      "              Grad: tensor([-0.1307,  0.7399])\n",
      "Epoch: 820, Loss: 4.583479404449463 \n",
      "              Params: tensor([  4.6009, -12.9642])\n",
      "              Grad: tensor([-0.1305,  0.7386])\n",
      "Epoch: 821, Loss: 4.577857494354248 \n",
      "              Params: tensor([  4.6022, -12.9716])\n",
      "              Grad: tensor([-0.1303,  0.7374])\n",
      "Epoch: 822, Loss: 4.572255611419678 \n",
      "              Params: tensor([  4.6035, -12.9790])\n",
      "              Grad: tensor([-0.1300,  0.7361])\n",
      "Epoch: 823, Loss: 4.566675186157227 \n",
      "              Params: tensor([  4.6048, -12.9863])\n",
      "              Grad: tensor([-0.1298,  0.7349])\n",
      "Epoch: 824, Loss: 4.561108112335205 \n",
      "              Params: tensor([  4.6061, -12.9936])\n",
      "              Grad: tensor([-0.1296,  0.7336])\n",
      "Epoch: 825, Loss: 4.555565357208252 \n",
      "              Params: tensor([  4.6074, -13.0010])\n",
      "              Grad: tensor([-0.1294,  0.7324])\n",
      "Epoch: 826, Loss: 4.550038814544678 \n",
      "              Params: tensor([  4.6087, -13.0083])\n",
      "              Grad: tensor([-0.1292,  0.7311])\n",
      "Epoch: 827, Loss: 4.544533729553223 \n",
      "              Params: tensor([  4.6100, -13.0156])\n",
      "              Grad: tensor([-0.1289,  0.7299])\n",
      "Epoch: 828, Loss: 4.539044380187988 \n",
      "              Params: tensor([  4.6113, -13.0229])\n",
      "              Grad: tensor([-0.1287,  0.7286])\n",
      "Epoch: 829, Loss: 4.53357458114624 \n",
      "              Params: tensor([  4.6126, -13.0301])\n",
      "              Grad: tensor([-0.1285,  0.7274])\n",
      "Epoch: 830, Loss: 4.528122425079346 \n",
      "              Params: tensor([  4.6139, -13.0374])\n",
      "              Grad: tensor([-0.1283,  0.7262])\n",
      "Epoch: 831, Loss: 4.522690773010254 \n",
      "              Params: tensor([  4.6152, -13.0446])\n",
      "              Grad: tensor([-0.1280,  0.7249])\n",
      "Epoch: 832, Loss: 4.517275810241699 \n",
      "              Params: tensor([  4.6164, -13.0519])\n",
      "              Grad: tensor([-0.1278,  0.7237])\n",
      "Epoch: 833, Loss: 4.5118794441223145 \n",
      "              Params: tensor([  4.6177, -13.0591])\n",
      "              Grad: tensor([-0.1276,  0.7225])\n",
      "Epoch: 834, Loss: 4.506504535675049 \n",
      "              Params: tensor([  4.6190, -13.0663])\n",
      "              Grad: tensor([-0.1274,  0.7212])\n",
      "Epoch: 835, Loss: 4.501140594482422 \n",
      "              Params: tensor([  4.6203, -13.0735])\n",
      "              Grad: tensor([-0.1272,  0.7200])\n",
      "Epoch: 836, Loss: 4.495800971984863 \n",
      "              Params: tensor([  4.6215, -13.0807])\n",
      "              Grad: tensor([-0.1270,  0.7188])\n",
      "Epoch: 837, Loss: 4.490474700927734 \n",
      "              Params: tensor([  4.6228, -13.0879])\n",
      "              Grad: tensor([-0.1268,  0.7176])\n",
      "Epoch: 838, Loss: 4.485169410705566 \n",
      "              Params: tensor([  4.6241, -13.0950])\n",
      "              Grad: tensor([-0.1266,  0.7163])\n",
      "Epoch: 839, Loss: 4.479884147644043 \n",
      "              Params: tensor([  4.6253, -13.1022])\n",
      "              Grad: tensor([-0.1263,  0.7151])\n",
      "Epoch: 840, Loss: 4.474613189697266 \n",
      "              Params: tensor([  4.6266, -13.1093])\n",
      "              Grad: tensor([-0.1261,  0.7139])\n",
      "Epoch: 841, Loss: 4.469363689422607 \n",
      "              Params: tensor([  4.6278, -13.1165])\n",
      "              Grad: tensor([-0.1259,  0.7127])\n",
      "Epoch: 842, Loss: 4.46412992477417 \n",
      "              Params: tensor([  4.6291, -13.1236])\n",
      "              Grad: tensor([-0.1257,  0.7115])\n",
      "Epoch: 843, Loss: 4.4589128494262695 \n",
      "              Params: tensor([  4.6304, -13.1307])\n",
      "              Grad: tensor([-0.1255,  0.7103])\n",
      "Epoch: 844, Loss: 4.453715801239014 \n",
      "              Params: tensor([  4.6316, -13.1378])\n",
      "              Grad: tensor([-0.1253,  0.7091])\n",
      "Epoch: 845, Loss: 4.448534965515137 \n",
      "              Params: tensor([  4.6329, -13.1449])\n",
      "              Grad: tensor([-0.1250,  0.7079])\n",
      "Epoch: 846, Loss: 4.4433722496032715 \n",
      "              Params: tensor([  4.6341, -13.1519])\n",
      "              Grad: tensor([-0.1249,  0.7067])\n",
      "Epoch: 847, Loss: 4.438226222991943 \n",
      "              Params: tensor([  4.6353, -13.1590])\n",
      "              Grad: tensor([-0.1246,  0.7055])\n",
      "Epoch: 848, Loss: 4.433098793029785 \n",
      "              Params: tensor([  4.6366, -13.1660])\n",
      "              Grad: tensor([-0.1244,  0.7043])\n",
      "Epoch: 849, Loss: 4.427990436553955 \n",
      "              Params: tensor([  4.6378, -13.1730])\n",
      "              Grad: tensor([-0.1242,  0.7031])\n",
      "Epoch: 850, Loss: 4.422896862030029 \n",
      "              Params: tensor([  4.6391, -13.1801])\n",
      "              Grad: tensor([-0.1240,  0.7019])\n",
      "Epoch: 851, Loss: 4.417819499969482 \n",
      "              Params: tensor([  4.6403, -13.1871])\n",
      "              Grad: tensor([-0.1238,  0.7007])\n",
      "Epoch: 852, Loss: 4.41276216506958 \n",
      "              Params: tensor([  4.6415, -13.1941])\n",
      "              Grad: tensor([-0.1236,  0.6995])\n",
      "Epoch: 853, Loss: 4.407720565795898 \n",
      "              Params: tensor([  4.6428, -13.2010])\n",
      "              Grad: tensor([-0.1234,  0.6983])\n",
      "Epoch: 854, Loss: 4.402697563171387 \n",
      "              Params: tensor([  4.6440, -13.2080])\n",
      "              Grad: tensor([-0.1232,  0.6971])\n",
      "Epoch: 855, Loss: 4.397687911987305 \n",
      "              Params: tensor([  4.6452, -13.2150])\n",
      "              Grad: tensor([-0.1229,  0.6959])\n",
      "Epoch: 856, Loss: 4.392696857452393 \n",
      "              Params: tensor([  4.6465, -13.2219])\n",
      "              Grad: tensor([-0.1227,  0.6948])\n",
      "Epoch: 857, Loss: 4.387725353240967 \n",
      "              Params: tensor([  4.6477, -13.2289])\n",
      "              Grad: tensor([-0.1225,  0.6936])\n",
      "Epoch: 858, Loss: 4.382769584655762 \n",
      "              Params: tensor([  4.6489, -13.2358])\n",
      "              Grad: tensor([-0.1223,  0.6924])\n",
      "Epoch: 859, Loss: 4.377828121185303 \n",
      "              Params: tensor([  4.6501, -13.2427])\n",
      "              Grad: tensor([-0.1221,  0.6912])\n",
      "Epoch: 860, Loss: 4.3729047775268555 \n",
      "              Params: tensor([  4.6514, -13.2496])\n",
      "              Grad: tensor([-0.1219,  0.6901])\n",
      "Epoch: 861, Loss: 4.36799955368042 \n",
      "              Params: tensor([  4.6526, -13.2565])\n",
      "              Grad: tensor([-0.1217,  0.6889])\n",
      "Epoch: 862, Loss: 4.3631110191345215 \n",
      "              Params: tensor([  4.6538, -13.2634])\n",
      "              Grad: tensor([-0.1215,  0.6877])\n",
      "Epoch: 863, Loss: 4.3582377433776855 \n",
      "              Params: tensor([  4.6550, -13.2702])\n",
      "              Grad: tensor([-0.1213,  0.6865])\n",
      "Epoch: 864, Loss: 4.3533830642700195 \n",
      "              Params: tensor([  4.6562, -13.2771])\n",
      "              Grad: tensor([-0.1211,  0.6854])\n",
      "Epoch: 865, Loss: 4.348541736602783 \n",
      "              Params: tensor([  4.6574, -13.2839])\n",
      "              Grad: tensor([-0.1209,  0.6842])\n",
      "Epoch: 866, Loss: 4.343716144561768 \n",
      "              Params: tensor([  4.6586, -13.2908])\n",
      "              Grad: tensor([-0.1207,  0.6830])\n",
      "Epoch: 867, Loss: 4.338911056518555 \n",
      "              Params: tensor([  4.6598, -13.2976])\n",
      "              Grad: tensor([-0.1205,  0.6819])\n",
      "Epoch: 868, Loss: 4.334120273590088 \n",
      "              Params: tensor([  4.6610, -13.3044])\n",
      "              Grad: tensor([-0.1203,  0.6807])\n",
      "Epoch: 869, Loss: 4.329345226287842 \n",
      "              Params: tensor([  4.6622, -13.3112])\n",
      "              Grad: tensor([-0.1201,  0.6796])\n",
      "Epoch: 870, Loss: 4.324588298797607 \n",
      "              Params: tensor([  4.6634, -13.3180])\n",
      "              Grad: tensor([-0.1198,  0.6784])\n",
      "Epoch: 871, Loss: 4.319845676422119 \n",
      "              Params: tensor([  4.6646, -13.3247])\n",
      "              Grad: tensor([-0.1196,  0.6773])\n",
      "Epoch: 872, Loss: 4.315117359161377 \n",
      "              Params: tensor([  4.6658, -13.3315])\n",
      "              Grad: tensor([-0.1195,  0.6761])\n",
      "Epoch: 873, Loss: 4.310409069061279 \n",
      "              Params: tensor([  4.6670, -13.3382])\n",
      "              Grad: tensor([-0.1192,  0.6750])\n",
      "Epoch: 874, Loss: 4.305714130401611 \n",
      "              Params: tensor([  4.6682, -13.3450])\n",
      "              Grad: tensor([-0.1190,  0.6738])\n",
      "Epoch: 875, Loss: 4.3010358810424805 \n",
      "              Params: tensor([  4.6694, -13.3517])\n",
      "              Grad: tensor([-0.1188,  0.6727])\n",
      "Epoch: 876, Loss: 4.296375751495361 \n",
      "              Params: tensor([  4.6706, -13.3584])\n",
      "              Grad: tensor([-0.1186,  0.6715])\n",
      "Epoch: 877, Loss: 4.291726589202881 \n",
      "              Params: tensor([  4.6718, -13.3651])\n",
      "              Grad: tensor([-0.1184,  0.6704])\n",
      "Epoch: 878, Loss: 4.287097930908203 \n",
      "              Params: tensor([  4.6730, -13.3718])\n",
      "              Grad: tensor([-0.1182,  0.6693])\n",
      "Epoch: 879, Loss: 4.282481670379639 \n",
      "              Params: tensor([  4.6741, -13.3785])\n",
      "              Grad: tensor([-0.1180,  0.6681])\n",
      "Epoch: 880, Loss: 4.277881622314453 \n",
      "              Params: tensor([  4.6753, -13.3852])\n",
      "              Grad: tensor([-0.1178,  0.6670])\n",
      "Epoch: 881, Loss: 4.273299217224121 \n",
      "              Params: tensor([  4.6765, -13.3918])\n",
      "              Grad: tensor([-0.1176,  0.6658])\n",
      "Epoch: 882, Loss: 4.268731594085693 \n",
      "              Params: tensor([  4.6777, -13.3985])\n",
      "              Grad: tensor([-0.1174,  0.6647])\n",
      "Epoch: 883, Loss: 4.2641777992248535 \n",
      "              Params: tensor([  4.6788, -13.4051])\n",
      "              Grad: tensor([-0.1172,  0.6636])\n",
      "Epoch: 884, Loss: 4.259643077850342 \n",
      "              Params: tensor([  4.6800, -13.4117])\n",
      "              Grad: tensor([-0.1170,  0.6625])\n",
      "Epoch: 885, Loss: 4.255119800567627 \n",
      "              Params: tensor([  4.6812, -13.4184])\n",
      "              Grad: tensor([-0.1168,  0.6613])\n",
      "Epoch: 886, Loss: 4.250613689422607 \n",
      "              Params: tensor([  4.6823, -13.4250])\n",
      "              Grad: tensor([-0.1166,  0.6602])\n",
      "Epoch: 887, Loss: 4.246123790740967 \n",
      "              Params: tensor([  4.6835, -13.4316])\n",
      "              Grad: tensor([-0.1164,  0.6591])\n",
      "Epoch: 888, Loss: 4.241647720336914 \n",
      "              Params: tensor([  4.6847, -13.4381])\n",
      "              Grad: tensor([-0.1162,  0.6580])\n",
      "Epoch: 889, Loss: 4.237185001373291 \n",
      "              Params: tensor([  4.6858, -13.4447])\n",
      "              Grad: tensor([-0.1160,  0.6569])\n",
      "Epoch: 890, Loss: 4.2327399253845215 \n",
      "              Params: tensor([  4.6870, -13.4513])\n",
      "              Grad: tensor([-0.1158,  0.6557])\n",
      "Epoch: 891, Loss: 4.228307723999023 \n",
      "              Params: tensor([  4.6881, -13.4578])\n",
      "              Grad: tensor([-0.1157,  0.6546])\n",
      "Epoch: 892, Loss: 4.223895072937012 \n",
      "              Params: tensor([  4.6893, -13.4643])\n",
      "              Grad: tensor([-0.1154,  0.6535])\n",
      "Epoch: 893, Loss: 4.219494342803955 \n",
      "              Params: tensor([  4.6904, -13.4709])\n",
      "              Grad: tensor([-0.1153,  0.6524])\n",
      "Epoch: 894, Loss: 4.215109348297119 \n",
      "              Params: tensor([  4.6916, -13.4774])\n",
      "              Grad: tensor([-0.1151,  0.6513])\n",
      "Epoch: 895, Loss: 4.210737228393555 \n",
      "              Params: tensor([  4.6927, -13.4839])\n",
      "              Grad: tensor([-0.1148,  0.6502])\n",
      "Epoch: 896, Loss: 4.206383228302002 \n",
      "              Params: tensor([  4.6939, -13.4904])\n",
      "              Grad: tensor([-0.1147,  0.6491])\n",
      "Epoch: 897, Loss: 4.202042579650879 \n",
      "              Params: tensor([  4.6950, -13.4968])\n",
      "              Grad: tensor([-0.1145,  0.6480])\n",
      "Epoch: 898, Loss: 4.1977152824401855 \n",
      "              Params: tensor([  4.6962, -13.5033])\n",
      "              Grad: tensor([-0.1143,  0.6469])\n",
      "Epoch: 899, Loss: 4.1934051513671875 \n",
      "              Params: tensor([  4.6973, -13.5098])\n",
      "              Grad: tensor([-0.1141,  0.6458])\n",
      "Epoch: 900, Loss: 4.189108371734619 \n",
      "              Params: tensor([  4.6985, -13.5162])\n",
      "              Grad: tensor([-0.1139,  0.6447])\n",
      "Epoch: 901, Loss: 4.184825420379639 \n",
      "              Params: tensor([  4.6996, -13.5227])\n",
      "              Grad: tensor([-0.1137,  0.6436])\n",
      "Epoch: 902, Loss: 4.180559158325195 \n",
      "              Params: tensor([  4.7007, -13.5291])\n",
      "              Grad: tensor([-0.1135,  0.6425])\n",
      "Epoch: 903, Loss: 4.176304817199707 \n",
      "              Params: tensor([  4.7019, -13.5355])\n",
      "              Grad: tensor([-0.1133,  0.6414])\n",
      "Epoch: 904, Loss: 4.172065258026123 \n",
      "              Params: tensor([  4.7030, -13.5419])\n",
      "              Grad: tensor([-0.1131,  0.6403])\n",
      "Epoch: 905, Loss: 4.167842388153076 \n",
      "              Params: tensor([  4.7041, -13.5483])\n",
      "              Grad: tensor([-0.1129,  0.6392])\n",
      "Epoch: 906, Loss: 4.163630485534668 \n",
      "              Params: tensor([  4.7053, -13.5547])\n",
      "              Grad: tensor([-0.1127,  0.6381])\n",
      "Epoch: 907, Loss: 4.159435749053955 \n",
      "              Params: tensor([  4.7064, -13.5610])\n",
      "              Grad: tensor([-0.1125,  0.6371])\n",
      "Epoch: 908, Loss: 4.155252933502197 \n",
      "              Params: tensor([  4.7075, -13.5674])\n",
      "              Grad: tensor([-0.1124,  0.6360])\n",
      "Epoch: 909, Loss: 4.151086330413818 \n",
      "              Params: tensor([  4.7086, -13.5738])\n",
      "              Grad: tensor([-0.1122,  0.6349])\n",
      "Epoch: 910, Loss: 4.1469340324401855 \n",
      "              Params: tensor([  4.7097, -13.5801])\n",
      "              Grad: tensor([-0.1120,  0.6338])\n",
      "Epoch: 911, Loss: 4.142794132232666 \n",
      "              Params: tensor([  4.7109, -13.5864])\n",
      "              Grad: tensor([-0.1118,  0.6327])\n",
      "Epoch: 912, Loss: 4.138669013977051 \n",
      "              Params: tensor([  4.7120, -13.5927])\n",
      "              Grad: tensor([-0.1116,  0.6317])\n",
      "Epoch: 913, Loss: 4.13455867767334 \n",
      "              Params: tensor([  4.7131, -13.5990])\n",
      "              Grad: tensor([-0.1114,  0.6306])\n",
      "Epoch: 914, Loss: 4.130464553833008 \n",
      "              Params: tensor([  4.7142, -13.6053])\n",
      "              Grad: tensor([-0.1112,  0.6295])\n",
      "Epoch: 915, Loss: 4.126377582550049 \n",
      "              Params: tensor([  4.7153, -13.6116])\n",
      "              Grad: tensor([-0.1110,  0.6284])\n",
      "Epoch: 916, Loss: 4.122309684753418 \n",
      "              Params: tensor([  4.7164, -13.6179])\n",
      "              Grad: tensor([-0.1108,  0.6274])\n",
      "Epoch: 917, Loss: 4.118252754211426 \n",
      "              Params: tensor([  4.7175, -13.6242])\n",
      "              Grad: tensor([-0.1107,  0.6263])\n",
      "Epoch: 918, Loss: 4.114212512969971 \n",
      "              Params: tensor([  4.7186, -13.6304])\n",
      "              Grad: tensor([-0.1104,  0.6253])\n",
      "Epoch: 919, Loss: 4.110184192657471 \n",
      "              Params: tensor([  4.7197, -13.6367])\n",
      "              Grad: tensor([-0.1103,  0.6242])\n",
      "Epoch: 920, Loss: 4.106170177459717 \n",
      "              Params: tensor([  4.7208, -13.6429])\n",
      "              Grad: tensor([-0.1101,  0.6231])\n",
      "Epoch: 921, Loss: 4.102170944213867 \n",
      "              Params: tensor([  4.7219, -13.6491])\n",
      "              Grad: tensor([-0.1099,  0.6221])\n",
      "Epoch: 922, Loss: 4.098180770874023 \n",
      "              Params: tensor([  4.7230, -13.6553])\n",
      "              Grad: tensor([-0.1097,  0.6210])\n",
      "Epoch: 923, Loss: 4.09420919418335 \n",
      "              Params: tensor([  4.7241, -13.6615])\n",
      "              Grad: tensor([-0.1095,  0.6200])\n",
      "Epoch: 924, Loss: 4.090250015258789 \n",
      "              Params: tensor([  4.7252, -13.6677])\n",
      "              Grad: tensor([-0.1093,  0.6189])\n",
      "Epoch: 925, Loss: 4.086300373077393 \n",
      "              Params: tensor([  4.7263, -13.6739])\n",
      "              Grad: tensor([-0.1091,  0.6179])\n",
      "Epoch: 926, Loss: 4.082366466522217 \n",
      "              Params: tensor([  4.7274, -13.6800])\n",
      "              Grad: tensor([-0.1090,  0.6168])\n",
      "Epoch: 927, Loss: 4.078448295593262 \n",
      "              Params: tensor([  4.7285, -13.6862])\n",
      "              Grad: tensor([-0.1088,  0.6158])\n",
      "Epoch: 928, Loss: 4.074540138244629 \n",
      "              Params: tensor([  4.7296, -13.6924])\n",
      "              Grad: tensor([-0.1086,  0.6147])\n",
      "Epoch: 929, Loss: 4.07064962387085 \n",
      "              Params: tensor([  4.7307, -13.6985])\n",
      "              Grad: tensor([-0.1084,  0.6137])\n",
      "Epoch: 930, Loss: 4.066768646240234 \n",
      "              Params: tensor([  4.7317, -13.7046])\n",
      "              Grad: tensor([-0.1082,  0.6126])\n",
      "Epoch: 931, Loss: 4.062900066375732 \n",
      "              Params: tensor([  4.7328, -13.7107])\n",
      "              Grad: tensor([-0.1080,  0.6116])\n",
      "Epoch: 932, Loss: 4.059047222137451 \n",
      "              Params: tensor([  4.7339, -13.7168])\n",
      "              Grad: tensor([-0.1079,  0.6105])\n",
      "Epoch: 933, Loss: 4.055204391479492 \n",
      "              Params: tensor([  4.7350, -13.7229])\n",
      "              Grad: tensor([-0.1077,  0.6095])\n",
      "Epoch: 934, Loss: 4.05137825012207 \n",
      "              Params: tensor([  4.7360, -13.7290])\n",
      "              Grad: tensor([-0.1075,  0.6085])\n",
      "Epoch: 935, Loss: 4.047563552856445 \n",
      "              Params: tensor([  4.7371, -13.7351])\n",
      "              Grad: tensor([-0.1073,  0.6074])\n",
      "Epoch: 936, Loss: 4.043761730194092 \n",
      "              Params: tensor([  4.7382, -13.7412])\n",
      "              Grad: tensor([-0.1071,  0.6064])\n",
      "Epoch: 937, Loss: 4.039972305297852 \n",
      "              Params: tensor([  4.7393, -13.7472])\n",
      "              Grad: tensor([-0.1069,  0.6054])\n",
      "Epoch: 938, Loss: 4.036196708679199 \n",
      "              Params: tensor([  4.7403, -13.7533])\n",
      "              Grad: tensor([-0.1068,  0.6043])\n",
      "Epoch: 939, Loss: 4.032433032989502 \n",
      "              Params: tensor([  4.7414, -13.7593])\n",
      "              Grad: tensor([-0.1066,  0.6033])\n",
      "Epoch: 940, Loss: 4.028685092926025 \n",
      "              Params: tensor([  4.7425, -13.7653])\n",
      "              Grad: tensor([-0.1064,  0.6023])\n",
      "Epoch: 941, Loss: 4.024947166442871 \n",
      "              Params: tensor([  4.7435, -13.7713])\n",
      "              Grad: tensor([-0.1062,  0.6013])\n",
      "Epoch: 942, Loss: 4.021220684051514 \n",
      "              Params: tensor([  4.7446, -13.7773])\n",
      "              Grad: tensor([-0.1060,  0.6003])\n",
      "Epoch: 943, Loss: 4.017508029937744 \n",
      "              Params: tensor([  4.7456, -13.7833])\n",
      "              Grad: tensor([-0.1058,  0.5992])\n",
      "Epoch: 944, Loss: 4.013808727264404 \n",
      "              Params: tensor([  4.7467, -13.7893])\n",
      "              Grad: tensor([-0.1057,  0.5982])\n",
      "Epoch: 945, Loss: 4.010122776031494 \n",
      "              Params: tensor([  4.7478, -13.7953])\n",
      "              Grad: tensor([-0.1055,  0.5972])\n",
      "Epoch: 946, Loss: 4.006446361541748 \n",
      "              Params: tensor([  4.7488, -13.8012])\n",
      "              Grad: tensor([-0.1053,  0.5962])\n",
      "Epoch: 947, Loss: 4.002786159515381 \n",
      "              Params: tensor([  4.7499, -13.8072])\n",
      "              Grad: tensor([-0.1051,  0.5952])\n",
      "Epoch: 948, Loss: 3.9991350173950195 \n",
      "              Params: tensor([  4.7509, -13.8131])\n",
      "              Grad: tensor([-0.1050,  0.5942])\n",
      "Epoch: 949, Loss: 3.995497941970825 \n",
      "              Params: tensor([  4.7520, -13.8191])\n",
      "              Grad: tensor([-0.1048,  0.5931])\n",
      "Epoch: 950, Loss: 3.9918737411499023 \n",
      "              Params: tensor([  4.7530, -13.8250])\n",
      "              Grad: tensor([-0.1046,  0.5921])\n",
      "Epoch: 951, Loss: 3.988260507583618 \n",
      "              Params: tensor([  4.7540, -13.8309])\n",
      "              Grad: tensor([-0.1044,  0.5911])\n",
      "Epoch: 952, Loss: 3.9846603870391846 \n",
      "              Params: tensor([  4.7551, -13.8368])\n",
      "              Grad: tensor([-0.1042,  0.5901])\n",
      "Epoch: 953, Loss: 3.9810731410980225 \n",
      "              Params: tensor([  4.7561, -13.8427])\n",
      "              Grad: tensor([-0.1041,  0.5891])\n",
      "Epoch: 954, Loss: 3.9774956703186035 \n",
      "              Params: tensor([  4.7572, -13.8486])\n",
      "              Grad: tensor([-0.1039,  0.5881])\n",
      "Epoch: 955, Loss: 3.973931312561035 \n",
      "              Params: tensor([  4.7582, -13.8544])\n",
      "              Grad: tensor([-0.1037,  0.5871])\n",
      "Epoch: 956, Loss: 3.970381021499634 \n",
      "              Params: tensor([  4.7592, -13.8603])\n",
      "              Grad: tensor([-0.1035,  0.5861])\n",
      "Epoch: 957, Loss: 3.9668405055999756 \n",
      "              Params: tensor([  4.7603, -13.8661])\n",
      "              Grad: tensor([-0.1034,  0.5851])\n",
      "Epoch: 958, Loss: 3.963313102722168 \n",
      "              Params: tensor([  4.7613, -13.8720])\n",
      "              Grad: tensor([-0.1032,  0.5841])\n",
      "Epoch: 959, Loss: 3.9597959518432617 \n",
      "              Params: tensor([  4.7623, -13.8778])\n",
      "              Grad: tensor([-0.1030,  0.5831])\n",
      "Epoch: 960, Loss: 3.9562947750091553 \n",
      "              Params: tensor([  4.7634, -13.8836])\n",
      "              Grad: tensor([-0.1028,  0.5822])\n",
      "Epoch: 961, Loss: 3.952801465988159 \n",
      "              Params: tensor([  4.7644, -13.8895])\n",
      "              Grad: tensor([-0.1026,  0.5812])\n",
      "Epoch: 962, Loss: 3.9493227005004883 \n",
      "              Params: tensor([  4.7654, -13.8953])\n",
      "              Grad: tensor([-0.1025,  0.5802])\n",
      "Epoch: 963, Loss: 3.945855140686035 \n",
      "              Params: tensor([  4.7664, -13.9010])\n",
      "              Grad: tensor([-0.1023,  0.5792])\n",
      "Epoch: 964, Loss: 3.9423980712890625 \n",
      "              Params: tensor([  4.7675, -13.9068])\n",
      "              Grad: tensor([-0.1021,  0.5782])\n",
      "Epoch: 965, Loss: 3.9389536380767822 \n",
      "              Params: tensor([  4.7685, -13.9126])\n",
      "              Grad: tensor([-0.1020,  0.5772])\n",
      "Epoch: 966, Loss: 3.9355204105377197 \n",
      "              Params: tensor([  4.7695, -13.9184])\n",
      "              Grad: tensor([-0.1018,  0.5762])\n",
      "Epoch: 967, Loss: 3.932095527648926 \n",
      "              Params: tensor([  4.7705, -13.9241])\n",
      "              Grad: tensor([-0.1016,  0.5753])\n",
      "Epoch: 968, Loss: 3.9286880493164062 \n",
      "              Params: tensor([  4.7715, -13.9299])\n",
      "              Grad: tensor([-0.1015,  0.5743])\n",
      "Epoch: 969, Loss: 3.9252915382385254 \n",
      "              Params: tensor([  4.7725, -13.9356])\n",
      "              Grad: tensor([-0.1013,  0.5733])\n",
      "Epoch: 970, Loss: 3.921905517578125 \n",
      "              Params: tensor([  4.7736, -13.9413])\n",
      "              Grad: tensor([-0.1011,  0.5723])\n",
      "Epoch: 971, Loss: 3.918527364730835 \n",
      "              Params: tensor([  4.7746, -13.9470])\n",
      "              Grad: tensor([-0.1009,  0.5714])\n",
      "Epoch: 972, Loss: 3.915165901184082 \n",
      "              Params: tensor([  4.7756, -13.9527])\n",
      "              Grad: tensor([-0.1008,  0.5704])\n",
      "Epoch: 973, Loss: 3.9118154048919678 \n",
      "              Params: tensor([  4.7766, -13.9584])\n",
      "              Grad: tensor([-0.1006,  0.5694])\n",
      "Epoch: 974, Loss: 3.9084739685058594 \n",
      "              Params: tensor([  4.7776, -13.9641])\n",
      "              Grad: tensor([-0.1004,  0.5685])\n",
      "Epoch: 975, Loss: 3.9051434993743896 \n",
      "              Params: tensor([  4.7786, -13.9698])\n",
      "              Grad: tensor([-0.1003,  0.5675])\n",
      "Epoch: 976, Loss: 3.901824712753296 \n",
      "              Params: tensor([  4.7796, -13.9755])\n",
      "              Grad: tensor([-0.1001,  0.5665])\n",
      "Epoch: 977, Loss: 3.898517370223999 \n",
      "              Params: tensor([  4.7806, -13.9811])\n",
      "              Grad: tensor([-0.0999,  0.5656])\n",
      "Epoch: 978, Loss: 3.895221710205078 \n",
      "              Params: tensor([  4.7816, -13.9868])\n",
      "              Grad: tensor([-0.0997,  0.5646])\n",
      "Epoch: 979, Loss: 3.891935110092163 \n",
      "              Params: tensor([  4.7826, -13.9924])\n",
      "              Grad: tensor([-0.0996,  0.5637])\n",
      "Epoch: 980, Loss: 3.8886640071868896 \n",
      "              Params: tensor([  4.7836, -13.9980])\n",
      "              Grad: tensor([-0.0994,  0.5627])\n",
      "Epoch: 981, Loss: 3.8854005336761475 \n",
      "              Params: tensor([  4.7846, -14.0036])\n",
      "              Grad: tensor([-0.0992,  0.5617])\n",
      "Epoch: 982, Loss: 3.882150411605835 \n",
      "              Params: tensor([  4.7856, -14.0092])\n",
      "              Grad: tensor([-0.0991,  0.5608])\n",
      "Epoch: 983, Loss: 3.8789103031158447 \n",
      "              Params: tensor([  4.7865, -14.0148])\n",
      "              Grad: tensor([-0.0989,  0.5598])\n",
      "Epoch: 984, Loss: 3.875680446624756 \n",
      "              Params: tensor([  4.7875, -14.0204])\n",
      "              Grad: tensor([-0.0987,  0.5589])\n",
      "Epoch: 985, Loss: 3.8724632263183594 \n",
      "              Params: tensor([  4.7885, -14.0260])\n",
      "              Grad: tensor([-0.0986,  0.5579])\n",
      "Epoch: 986, Loss: 3.869255781173706 \n",
      "              Params: tensor([  4.7895, -14.0316])\n",
      "              Grad: tensor([-0.0984,  0.5570])\n",
      "Epoch: 987, Loss: 3.866060495376587 \n",
      "              Params: tensor([  4.7905, -14.0371])\n",
      "              Grad: tensor([-0.0982,  0.5560])\n",
      "Epoch: 988, Loss: 3.862872362136841 \n",
      "              Params: tensor([  4.7915, -14.0427])\n",
      "              Grad: tensor([-0.0981,  0.5551])\n",
      "Epoch: 989, Loss: 3.859699010848999 \n",
      "              Params: tensor([  4.7924, -14.0482])\n",
      "              Grad: tensor([-0.0979,  0.5541])\n",
      "Epoch: 990, Loss: 3.8565351963043213 \n",
      "              Params: tensor([  4.7934, -14.0538])\n",
      "              Grad: tensor([-0.0978,  0.5532])\n",
      "Epoch: 991, Loss: 3.8533811569213867 \n",
      "              Params: tensor([  4.7944, -14.0593])\n",
      "              Grad: tensor([-0.0976,  0.5523])\n",
      "Epoch: 992, Loss: 3.8502371311187744 \n",
      "              Params: tensor([  4.7954, -14.0648])\n",
      "              Grad: tensor([-0.0974,  0.5513])\n",
      "Epoch: 993, Loss: 3.847109079360962 \n",
      "              Params: tensor([  4.7963, -14.0703])\n",
      "              Grad: tensor([-0.0973,  0.5504])\n",
      "Epoch: 994, Loss: 3.8439841270446777 \n",
      "              Params: tensor([  4.7973, -14.0758])\n",
      "              Grad: tensor([-0.0971,  0.5495])\n",
      "Epoch: 995, Loss: 3.8408761024475098 \n",
      "              Params: tensor([  4.7983, -14.0813])\n",
      "              Grad: tensor([-0.0969,  0.5485])\n",
      "Epoch: 996, Loss: 3.837775468826294 \n",
      "              Params: tensor([  4.7992, -14.0868])\n",
      "              Grad: tensor([-0.0967,  0.5476])\n",
      "Epoch: 997, Loss: 3.8346855640411377 \n",
      "              Params: tensor([  4.8002, -14.0922])\n",
      "              Grad: tensor([-0.0966,  0.5467])\n",
      "Epoch: 998, Loss: 3.831606388092041 \n",
      "              Params: tensor([  4.8012, -14.0977])\n",
      "              Grad: tensor([-0.0964,  0.5457])\n",
      "Epoch: 999, Loss: 3.828537940979004 \n",
      "              Params: tensor([  4.8021, -14.1031])\n",
      "              Grad: tensor([-0.0962,  0.5448])\n",
      "Epoch: 1000, Loss: 3.8254828453063965 \n",
      "              Params: tensor([  4.8031, -14.1086])\n",
      "              Grad: tensor([-0.0961,  0.5439])\n",
      "Epoch: 1001, Loss: 3.82243275642395 \n",
      "              Params: tensor([  4.8041, -14.1140])\n",
      "              Grad: tensor([-0.0959,  0.5430])\n",
      "Epoch: 1002, Loss: 3.8193979263305664 \n",
      "              Params: tensor([  4.8050, -14.1194])\n",
      "              Grad: tensor([-0.0957,  0.5420])\n",
      "Epoch: 1003, Loss: 3.816368818283081 \n",
      "              Params: tensor([  4.8060, -14.1248])\n",
      "              Grad: tensor([-0.0956,  0.5411])\n",
      "Epoch: 1004, Loss: 3.8133504390716553 \n",
      "              Params: tensor([  4.8069, -14.1302])\n",
      "              Grad: tensor([-0.0954,  0.5402])\n",
      "Epoch: 1005, Loss: 3.8103437423706055 \n",
      "              Params: tensor([  4.8079, -14.1356])\n",
      "              Grad: tensor([-0.0953,  0.5393])\n",
      "Epoch: 1006, Loss: 3.8073484897613525 \n",
      "              Params: tensor([  4.8088, -14.1410])\n",
      "              Grad: tensor([-0.0951,  0.5384])\n",
      "Epoch: 1007, Loss: 3.8043596744537354 \n",
      "              Params: tensor([  4.8098, -14.1464])\n",
      "              Grad: tensor([-0.0949,  0.5375])\n",
      "Epoch: 1008, Loss: 3.8013837337493896 \n",
      "              Params: tensor([  4.8107, -14.1518])\n",
      "              Grad: tensor([-0.0948,  0.5365])\n",
      "Epoch: 1009, Loss: 3.7984206676483154 \n",
      "              Params: tensor([  4.8117, -14.1571])\n",
      "              Grad: tensor([-0.0946,  0.5356])\n",
      "Epoch: 1010, Loss: 3.7954649925231934 \n",
      "              Params: tensor([  4.8126, -14.1625])\n",
      "              Grad: tensor([-0.0945,  0.5347])\n",
      "Epoch: 1011, Loss: 3.792518377304077 \n",
      "              Params: tensor([  4.8136, -14.1678])\n",
      "              Grad: tensor([-0.0943,  0.5338])\n",
      "Epoch: 1012, Loss: 3.789584159851074 \n",
      "              Params: tensor([  4.8145, -14.1731])\n",
      "              Grad: tensor([-0.0942,  0.5329])\n",
      "Epoch: 1013, Loss: 3.7866575717926025 \n",
      "              Params: tensor([  4.8154, -14.1784])\n",
      "              Grad: tensor([-0.0940,  0.5320])\n",
      "Epoch: 1014, Loss: 3.7837395668029785 \n",
      "              Params: tensor([  4.8164, -14.1838])\n",
      "              Grad: tensor([-0.0938,  0.5311])\n",
      "Epoch: 1015, Loss: 3.780831813812256 \n",
      "              Params: tensor([  4.8173, -14.1891])\n",
      "              Grad: tensor([-0.0937,  0.5302])\n",
      "Epoch: 1016, Loss: 3.7779388427734375 \n",
      "              Params: tensor([  4.8183, -14.1943])\n",
      "              Grad: tensor([-0.0935,  0.5293])\n",
      "Epoch: 1017, Loss: 3.775052785873413 \n",
      "              Params: tensor([  4.8192, -14.1996])\n",
      "              Grad: tensor([-0.0933,  0.5284])\n",
      "Epoch: 1018, Loss: 3.7721731662750244 \n",
      "              Params: tensor([  4.8201, -14.2049])\n",
      "              Grad: tensor([-0.0932,  0.5275])\n",
      "Epoch: 1019, Loss: 3.769310712814331 \n",
      "              Params: tensor([  4.8210, -14.2102])\n",
      "              Grad: tensor([-0.0930,  0.5266])\n",
      "Epoch: 1020, Loss: 3.7664504051208496 \n",
      "              Params: tensor([  4.8220, -14.2154])\n",
      "              Grad: tensor([-0.0929,  0.5257])\n",
      "Epoch: 1021, Loss: 3.7636024951934814 \n",
      "              Params: tensor([  4.8229, -14.2207])\n",
      "              Grad: tensor([-0.0927,  0.5248])\n",
      "Epoch: 1022, Loss: 3.760765790939331 \n",
      "              Params: tensor([  4.8238, -14.2259])\n",
      "              Grad: tensor([-0.0926,  0.5239])\n",
      "Epoch: 1023, Loss: 3.7579362392425537 \n",
      "              Params: tensor([  4.8248, -14.2311])\n",
      "              Grad: tensor([-0.0924,  0.5230])\n",
      "Epoch: 1024, Loss: 3.755117654800415 \n",
      "              Params: tensor([  4.8257, -14.2364])\n",
      "              Grad: tensor([-0.0922,  0.5221])\n",
      "Epoch: 1025, Loss: 3.7523093223571777 \n",
      "              Params: tensor([  4.8266, -14.2416])\n",
      "              Grad: tensor([-0.0921,  0.5213])\n",
      "Epoch: 1026, Loss: 3.7495110034942627 \n",
      "              Params: tensor([  4.8275, -14.2468])\n",
      "              Grad: tensor([-0.0919,  0.5204])\n",
      "Epoch: 1027, Loss: 3.7467222213745117 \n",
      "              Params: tensor([  4.8284, -14.2520])\n",
      "              Grad: tensor([-0.0918,  0.5195])\n",
      "Epoch: 1028, Loss: 3.7439401149749756 \n",
      "              Params: tensor([  4.8293, -14.2572])\n",
      "              Grad: tensor([-0.0916,  0.5186])\n",
      "Epoch: 1029, Loss: 3.7411692142486572 \n",
      "              Params: tensor([  4.8303, -14.2623])\n",
      "              Grad: tensor([-0.0915,  0.5177])\n",
      "Epoch: 1030, Loss: 3.7384068965911865 \n",
      "              Params: tensor([  4.8312, -14.2675])\n",
      "              Grad: tensor([-0.0913,  0.5168])\n",
      "Epoch: 1031, Loss: 3.7356557846069336 \n",
      "              Params: tensor([  4.8321, -14.2727])\n",
      "              Grad: tensor([-0.0912,  0.5160])\n",
      "Epoch: 1032, Loss: 3.7329137325286865 \n",
      "              Params: tensor([  4.8330, -14.2778])\n",
      "              Grad: tensor([-0.0910,  0.5151])\n",
      "Epoch: 1033, Loss: 3.7301812171936035 \n",
      "              Params: tensor([  4.8339, -14.2830])\n",
      "              Grad: tensor([-0.0908,  0.5142])\n",
      "Epoch: 1034, Loss: 3.7274558544158936 \n",
      "              Params: tensor([  4.8348, -14.2881])\n",
      "              Grad: tensor([-0.0907,  0.5133])\n",
      "Epoch: 1035, Loss: 3.7247400283813477 \n",
      "              Params: tensor([  4.8357, -14.2932])\n",
      "              Grad: tensor([-0.0905,  0.5125])\n",
      "Epoch: 1036, Loss: 3.722034454345703 \n",
      "              Params: tensor([  4.8366, -14.2983])\n",
      "              Grad: tensor([-0.0904,  0.5116])\n",
      "Epoch: 1037, Loss: 3.7193374633789062 \n",
      "              Params: tensor([  4.8375, -14.3034])\n",
      "              Grad: tensor([-0.0902,  0.5107])\n",
      "Epoch: 1038, Loss: 3.71665096282959 \n",
      "              Params: tensor([  4.8384, -14.3085])\n",
      "              Grad: tensor([-0.0901,  0.5099])\n",
      "Epoch: 1039, Loss: 3.7139716148376465 \n",
      "              Params: tensor([  4.8393, -14.3136])\n",
      "              Grad: tensor([-0.0899,  0.5090])\n",
      "Epoch: 1040, Loss: 3.711301565170288 \n",
      "              Params: tensor([  4.8402, -14.3187])\n",
      "              Grad: tensor([-0.0898,  0.5081])\n",
      "Epoch: 1041, Loss: 3.708643913269043 \n",
      "              Params: tensor([  4.8411, -14.3238])\n",
      "              Grad: tensor([-0.0896,  0.5073])\n",
      "Epoch: 1042, Loss: 3.7059905529022217 \n",
      "              Params: tensor([  4.8420, -14.3288])\n",
      "              Grad: tensor([-0.0895,  0.5064])\n",
      "Epoch: 1043, Loss: 3.7033514976501465 \n",
      "              Params: tensor([  4.8429, -14.3339])\n",
      "              Grad: tensor([-0.0893,  0.5055])\n",
      "Epoch: 1044, Loss: 3.7007157802581787 \n",
      "              Params: tensor([  4.8438, -14.3390])\n",
      "              Grad: tensor([-0.0892,  0.5047])\n",
      "Epoch: 1045, Loss: 3.6980912685394287 \n",
      "              Params: tensor([  4.8447, -14.3440])\n",
      "              Grad: tensor([-0.0890,  0.5038])\n",
      "Epoch: 1046, Loss: 3.6954762935638428 \n",
      "              Params: tensor([  4.8456, -14.3490])\n",
      "              Grad: tensor([-0.0888,  0.5030])\n",
      "Epoch: 1047, Loss: 3.692868947982788 \n",
      "              Params: tensor([  4.8465, -14.3540])\n",
      "              Grad: tensor([-0.0887,  0.5021])\n",
      "Epoch: 1048, Loss: 3.690272569656372 \n",
      "              Params: tensor([  4.8473, -14.3591])\n",
      "              Grad: tensor([-0.0886,  0.5013])\n",
      "Epoch: 1049, Loss: 3.687682867050171 \n",
      "              Params: tensor([  4.8482, -14.3641])\n",
      "              Grad: tensor([-0.0884,  0.5004])\n",
      "Epoch: 1050, Loss: 3.6851041316986084 \n",
      "              Params: tensor([  4.8491, -14.3691])\n",
      "              Grad: tensor([-0.0882,  0.4996])\n",
      "Epoch: 1051, Loss: 3.6825318336486816 \n",
      "              Params: tensor([  4.8500, -14.3740])\n",
      "              Grad: tensor([-0.0881,  0.4987])\n",
      "Epoch: 1052, Loss: 3.67996883392334 \n",
      "              Params: tensor([  4.8509, -14.3790])\n",
      "              Grad: tensor([-0.0879,  0.4979])\n",
      "Epoch: 1053, Loss: 3.677417039871216 \n",
      "              Params: tensor([  4.8518, -14.3840])\n",
      "              Grad: tensor([-0.0878,  0.4970])\n",
      "Epoch: 1054, Loss: 3.6748709678649902 \n",
      "              Params: tensor([  4.8526, -14.3889])\n",
      "              Grad: tensor([-0.0877,  0.4962])\n",
      "Epoch: 1055, Loss: 3.672334909439087 \n",
      "              Params: tensor([  4.8535, -14.3939])\n",
      "              Grad: tensor([-0.0875,  0.4953])\n",
      "Epoch: 1056, Loss: 3.669804334640503 \n",
      "              Params: tensor([  4.8544, -14.3988])\n",
      "              Grad: tensor([-0.0873,  0.4945])\n",
      "Epoch: 1057, Loss: 3.6672868728637695 \n",
      "              Params: tensor([  4.8552, -14.4038])\n",
      "              Grad: tensor([-0.0872,  0.4936])\n",
      "Epoch: 1058, Loss: 3.6647748947143555 \n",
      "              Params: tensor([  4.8561, -14.4087])\n",
      "              Grad: tensor([-0.0870,  0.4928])\n",
      "Epoch: 1059, Loss: 3.6622731685638428 \n",
      "              Params: tensor([  4.8570, -14.4136])\n",
      "              Grad: tensor([-0.0869,  0.4920])\n",
      "Epoch: 1060, Loss: 3.659777879714966 \n",
      "              Params: tensor([  4.8579, -14.4185])\n",
      "              Grad: tensor([-0.0868,  0.4911])\n",
      "Epoch: 1061, Loss: 3.657294988632202 \n",
      "              Params: tensor([  4.8587, -14.4234])\n",
      "              Grad: tensor([-0.0866,  0.4903])\n",
      "Epoch: 1062, Loss: 3.654815912246704 \n",
      "              Params: tensor([  4.8596, -14.4283])\n",
      "              Grad: tensor([-0.0865,  0.4895])\n",
      "Epoch: 1063, Loss: 3.6523489952087402 \n",
      "              Params: tensor([  4.8604, -14.4332])\n",
      "              Grad: tensor([-0.0863,  0.4886])\n",
      "Epoch: 1064, Loss: 3.6498892307281494 \n",
      "              Params: tensor([  4.8613, -14.4381])\n",
      "              Grad: tensor([-0.0862,  0.4878])\n",
      "Epoch: 1065, Loss: 3.647437334060669 \n",
      "              Params: tensor([  4.8622, -14.4430])\n",
      "              Grad: tensor([-0.0860,  0.4870])\n",
      "Epoch: 1066, Loss: 3.644991397857666 \n",
      "              Params: tensor([  4.8630, -14.4478])\n",
      "              Grad: tensor([-0.0859,  0.4862])\n",
      "Epoch: 1067, Loss: 3.642559289932251 \n",
      "              Params: tensor([  4.8639, -14.4527])\n",
      "              Grad: tensor([-0.0857,  0.4853])\n",
      "Epoch: 1068, Loss: 3.640131950378418 \n",
      "              Params: tensor([  4.8647, -14.4575])\n",
      "              Grad: tensor([-0.0856,  0.4845])\n",
      "Epoch: 1069, Loss: 3.6377112865448 \n",
      "              Params: tensor([  4.8656, -14.4624])\n",
      "              Grad: tensor([-0.0854,  0.4837])\n",
      "Epoch: 1070, Loss: 3.6353020668029785 \n",
      "              Params: tensor([  4.8665, -14.4672])\n",
      "              Grad: tensor([-0.0853,  0.4829])\n",
      "Epoch: 1071, Loss: 3.6329023838043213 \n",
      "              Params: tensor([  4.8673, -14.4720])\n",
      "              Grad: tensor([-0.0851,  0.4820])\n",
      "Epoch: 1072, Loss: 3.6305084228515625 \n",
      "              Params: tensor([  4.8682, -14.4768])\n",
      "              Grad: tensor([-0.0850,  0.4812])\n",
      "Epoch: 1073, Loss: 3.6281189918518066 \n",
      "              Params: tensor([  4.8690, -14.4816])\n",
      "              Grad: tensor([-0.0849,  0.4804])\n",
      "Epoch: 1074, Loss: 3.6257410049438477 \n",
      "              Params: tensor([  4.8698, -14.4864])\n",
      "              Grad: tensor([-0.0847,  0.4796])\n",
      "Epoch: 1075, Loss: 3.623373508453369 \n",
      "              Params: tensor([  4.8707, -14.4912])\n",
      "              Grad: tensor([-0.0846,  0.4788])\n",
      "Epoch: 1076, Loss: 3.6210100650787354 \n",
      "              Params: tensor([  4.8715, -14.4960])\n",
      "              Grad: tensor([-0.0844,  0.4780])\n",
      "Epoch: 1077, Loss: 3.6186585426330566 \n",
      "              Params: tensor([  4.8724, -14.5008])\n",
      "              Grad: tensor([-0.0843,  0.4771])\n",
      "Epoch: 1078, Loss: 3.6163108348846436 \n",
      "              Params: tensor([  4.8732, -14.5055])\n",
      "              Grad: tensor([-0.0841,  0.4763])\n",
      "Epoch: 1079, Loss: 3.6139731407165527 \n",
      "              Params: tensor([  4.8741, -14.5103])\n",
      "              Grad: tensor([-0.0840,  0.4755])\n",
      "Epoch: 1080, Loss: 3.611642599105835 \n",
      "              Params: tensor([  4.8749, -14.5150])\n",
      "              Grad: tensor([-0.0839,  0.4747])\n",
      "Epoch: 1081, Loss: 3.609320640563965 \n",
      "              Params: tensor([  4.8757, -14.5198])\n",
      "              Grad: tensor([-0.0837,  0.4739])\n",
      "Epoch: 1082, Loss: 3.6070075035095215 \n",
      "              Params: tensor([  4.8766, -14.5245])\n",
      "              Grad: tensor([-0.0836,  0.4731])\n",
      "Epoch: 1083, Loss: 3.604701042175293 \n",
      "              Params: tensor([  4.8774, -14.5292])\n",
      "              Grad: tensor([-0.0834,  0.4723])\n",
      "Epoch: 1084, Loss: 3.602403402328491 \n",
      "              Params: tensor([  4.8782, -14.5339])\n",
      "              Grad: tensor([-0.0833,  0.4715])\n",
      "Epoch: 1085, Loss: 3.6001136302948 \n",
      "              Params: tensor([  4.8791, -14.5387])\n",
      "              Grad: tensor([-0.0832,  0.4707])\n",
      "Epoch: 1086, Loss: 3.5978310108184814 \n",
      "              Params: tensor([  4.8799, -14.5434])\n",
      "              Grad: tensor([-0.0830,  0.4699])\n",
      "Epoch: 1087, Loss: 3.595552921295166 \n",
      "              Params: tensor([  4.8807, -14.5480])\n",
      "              Grad: tensor([-0.0829,  0.4691])\n",
      "Epoch: 1088, Loss: 3.593287467956543 \n",
      "              Params: tensor([  4.8816, -14.5527])\n",
      "              Grad: tensor([-0.0827,  0.4683])\n",
      "Epoch: 1089, Loss: 3.5910298824310303 \n",
      "              Params: tensor([  4.8824, -14.5574])\n",
      "              Grad: tensor([-0.0826,  0.4675])\n",
      "Epoch: 1090, Loss: 3.5887763500213623 \n",
      "              Params: tensor([  4.8832, -14.5621])\n",
      "              Grad: tensor([-0.0824,  0.4667])\n",
      "Epoch: 1091, Loss: 3.586533546447754 \n",
      "              Params: tensor([  4.8840, -14.5667])\n",
      "              Grad: tensor([-0.0823,  0.4659])\n",
      "Epoch: 1092, Loss: 3.584294319152832 \n",
      "              Params: tensor([  4.8849, -14.5714])\n",
      "              Grad: tensor([-0.0822,  0.4651])\n",
      "Epoch: 1093, Loss: 3.5820672512054443 \n",
      "              Params: tensor([  4.8857, -14.5760])\n",
      "              Grad: tensor([-0.0820,  0.4643])\n",
      "Epoch: 1094, Loss: 3.579845428466797 \n",
      "              Params: tensor([  4.8865, -14.5807])\n",
      "              Grad: tensor([-0.0819,  0.4636])\n",
      "Epoch: 1095, Loss: 3.5776307582855225 \n",
      "              Params: tensor([  4.8873, -14.5853])\n",
      "              Grad: tensor([-0.0818,  0.4628])\n",
      "Epoch: 1096, Loss: 3.5754239559173584 \n",
      "              Params: tensor([  4.8881, -14.5899])\n",
      "              Grad: tensor([-0.0816,  0.4620])\n",
      "Epoch: 1097, Loss: 3.573225498199463 \n",
      "              Params: tensor([  4.8889, -14.5945])\n",
      "              Grad: tensor([-0.0815,  0.4612])\n",
      "Epoch: 1098, Loss: 3.5710349082946777 \n",
      "              Params: tensor([  4.8898, -14.5991])\n",
      "              Grad: tensor([-0.0813,  0.4604])\n",
      "Epoch: 1099, Loss: 3.568847894668579 \n",
      "              Params: tensor([  4.8906, -14.6037])\n",
      "              Grad: tensor([-0.0812,  0.4596])\n",
      "Epoch: 1100, Loss: 3.5666725635528564 \n",
      "              Params: tensor([  4.8914, -14.6083])\n",
      "              Grad: tensor([-0.0810,  0.4588])\n",
      "Epoch: 1101, Loss: 3.5645058155059814 \n",
      "              Params: tensor([  4.8922, -14.6129])\n",
      "              Grad: tensor([-0.0809,  0.4581])\n",
      "Epoch: 1102, Loss: 3.56234073638916 \n",
      "              Params: tensor([  4.8930, -14.6175])\n",
      "              Grad: tensor([-0.0808,  0.4573])\n",
      "Epoch: 1103, Loss: 3.5601847171783447 \n",
      "              Params: tensor([  4.8938, -14.6220])\n",
      "              Grad: tensor([-0.0806,  0.4565])\n",
      "Epoch: 1104, Loss: 3.558039903640747 \n",
      "              Params: tensor([  4.8946, -14.6266])\n",
      "              Grad: tensor([-0.0805,  0.4557])\n",
      "Epoch: 1105, Loss: 3.555901288986206 \n",
      "              Params: tensor([  4.8954, -14.6311])\n",
      "              Grad: tensor([-0.0804,  0.4550])\n",
      "Epoch: 1106, Loss: 3.553767204284668 \n",
      "              Params: tensor([  4.8962, -14.6357])\n",
      "              Grad: tensor([-0.0802,  0.4542])\n",
      "Epoch: 1107, Loss: 3.551640510559082 \n",
      "              Params: tensor([  4.8970, -14.6402])\n",
      "              Grad: tensor([-0.0801,  0.4534])\n",
      "Epoch: 1108, Loss: 3.5495238304138184 \n",
      "              Params: tensor([  4.8978, -14.6447])\n",
      "              Grad: tensor([-0.0799,  0.4527])\n",
      "Epoch: 1109, Loss: 3.5474114418029785 \n",
      "              Params: tensor([  4.8986, -14.6493])\n",
      "              Grad: tensor([-0.0798,  0.4519])\n",
      "Epoch: 1110, Loss: 3.5453085899353027 \n",
      "              Params: tensor([  4.8994, -14.6538])\n",
      "              Grad: tensor([-0.0797,  0.4511])\n",
      "Epoch: 1111, Loss: 3.543210506439209 \n",
      "              Params: tensor([  4.9002, -14.6583])\n",
      "              Grad: tensor([-0.0796,  0.4503])\n",
      "Epoch: 1112, Loss: 3.541123867034912 \n",
      "              Params: tensor([  4.9010, -14.6628])\n",
      "              Grad: tensor([-0.0794,  0.4496])\n",
      "Epoch: 1113, Loss: 3.53904128074646 \n",
      "              Params: tensor([  4.9018, -14.6673])\n",
      "              Grad: tensor([-0.0793,  0.4488])\n",
      "Epoch: 1114, Loss: 3.536966562271118 \n",
      "              Params: tensor([  4.9026, -14.6717])\n",
      "              Grad: tensor([-0.0791,  0.4481])\n",
      "Epoch: 1115, Loss: 3.5348961353302 \n",
      "              Params: tensor([  4.9034, -14.6762])\n",
      "              Grad: tensor([-0.0790,  0.4473])\n",
      "Epoch: 1116, Loss: 3.532834529876709 \n",
      "              Params: tensor([  4.9042, -14.6807])\n",
      "              Grad: tensor([-0.0789,  0.4465])\n",
      "Epoch: 1117, Loss: 3.530780553817749 \n",
      "              Params: tensor([  4.9049, -14.6851])\n",
      "              Grad: tensor([-0.0787,  0.4458])\n",
      "Epoch: 1118, Loss: 3.528733730316162 \n",
      "              Params: tensor([  4.9057, -14.6896])\n",
      "              Grad: tensor([-0.0786,  0.4450])\n",
      "Epoch: 1119, Loss: 3.52669358253479 \n",
      "              Params: tensor([  4.9065, -14.6940])\n",
      "              Grad: tensor([-0.0785,  0.4443])\n",
      "Epoch: 1120, Loss: 3.5246617794036865 \n",
      "              Params: tensor([  4.9073, -14.6985])\n",
      "              Grad: tensor([-0.0784,  0.4435])\n",
      "Epoch: 1121, Loss: 3.522632598876953 \n",
      "              Params: tensor([  4.9081, -14.7029])\n",
      "              Grad: tensor([-0.0782,  0.4428])\n",
      "Epoch: 1122, Loss: 3.5206143856048584 \n",
      "              Params: tensor([  4.9089, -14.7073])\n",
      "              Grad: tensor([-0.0781,  0.4420])\n",
      "Epoch: 1123, Loss: 3.518601179122925 \n",
      "              Params: tensor([  4.9096, -14.7117])\n",
      "              Grad: tensor([-0.0779,  0.4413])\n",
      "Epoch: 1124, Loss: 3.516594171524048 \n",
      "              Params: tensor([  4.9104, -14.7161])\n",
      "              Grad: tensor([-0.0778,  0.4405])\n",
      "Epoch: 1125, Loss: 3.5145936012268066 \n",
      "              Params: tensor([  4.9112, -14.7205])\n",
      "              Grad: tensor([-0.0777,  0.4398])\n",
      "Epoch: 1126, Loss: 3.5126020908355713 \n",
      "              Params: tensor([  4.9120, -14.7249])\n",
      "              Grad: tensor([-0.0775,  0.4390])\n",
      "Epoch: 1127, Loss: 3.5106186866760254 \n",
      "              Params: tensor([  4.9128, -14.7293])\n",
      "              Grad: tensor([-0.0774,  0.4383])\n",
      "Epoch: 1128, Loss: 3.508636713027954 \n",
      "              Params: tensor([  4.9135, -14.7337])\n",
      "              Grad: tensor([-0.0773,  0.4375])\n",
      "Epoch: 1129, Loss: 3.5066652297973633 \n",
      "              Params: tensor([  4.9143, -14.7380])\n",
      "              Grad: tensor([-0.0772,  0.4368])\n",
      "Epoch: 1130, Loss: 3.5046989917755127 \n",
      "              Params: tensor([  4.9151, -14.7424])\n",
      "              Grad: tensor([-0.0770,  0.4360])\n",
      "Epoch: 1131, Loss: 3.5027406215667725 \n",
      "              Params: tensor([  4.9158, -14.7467])\n",
      "              Grad: tensor([-0.0769,  0.4353])\n",
      "Epoch: 1132, Loss: 3.500788927078247 \n",
      "              Params: tensor([  4.9166, -14.7511])\n",
      "              Grad: tensor([-0.0767,  0.4346])\n",
      "Epoch: 1133, Loss: 3.498843193054199 \n",
      "              Params: tensor([  4.9174, -14.7554])\n",
      "              Grad: tensor([-0.0766,  0.4338])\n",
      "Epoch: 1134, Loss: 3.4969053268432617 \n",
      "              Params: tensor([  4.9181, -14.7598])\n",
      "              Grad: tensor([-0.0765,  0.4331])\n",
      "Epoch: 1135, Loss: 3.494971513748169 \n",
      "              Params: tensor([  4.9189, -14.7641])\n",
      "              Grad: tensor([-0.0764,  0.4323])\n",
      "Epoch: 1136, Loss: 3.4930458068847656 \n",
      "              Params: tensor([  4.9197, -14.7684])\n",
      "              Grad: tensor([-0.0763,  0.4316])\n",
      "Epoch: 1137, Loss: 3.4911270141601562 \n",
      "              Params: tensor([  4.9204, -14.7727])\n",
      "              Grad: tensor([-0.0761,  0.4309])\n",
      "Epoch: 1138, Loss: 3.4892144203186035 \n",
      "              Params: tensor([  4.9212, -14.7770])\n",
      "              Grad: tensor([-0.0760,  0.4301])\n",
      "Epoch: 1139, Loss: 3.487307548522949 \n",
      "              Params: tensor([  4.9219, -14.7813])\n",
      "              Grad: tensor([-0.0759,  0.4294])\n",
      "Epoch: 1140, Loss: 3.485409736633301 \n",
      "              Params: tensor([  4.9227, -14.7856])\n",
      "              Grad: tensor([-0.0757,  0.4287])\n",
      "Epoch: 1141, Loss: 3.4835150241851807 \n",
      "              Params: tensor([  4.9235, -14.7899])\n",
      "              Grad: tensor([-0.0756,  0.4280])\n",
      "Epoch: 1142, Loss: 3.4816269874572754 \n",
      "              Params: tensor([  4.9242, -14.7941])\n",
      "              Grad: tensor([-0.0755,  0.4272])\n",
      "Epoch: 1143, Loss: 3.479745626449585 \n",
      "              Params: tensor([  4.9250, -14.7984])\n",
      "              Grad: tensor([-0.0753,  0.4265])\n",
      "Epoch: 1144, Loss: 3.477871894836426 \n",
      "              Params: tensor([  4.9257, -14.8027])\n",
      "              Grad: tensor([-0.0752,  0.4258])\n",
      "Epoch: 1145, Loss: 3.4760048389434814 \n",
      "              Params: tensor([  4.9265, -14.8069])\n",
      "              Grad: tensor([-0.0751,  0.4250])\n",
      "Epoch: 1146, Loss: 3.4741432666778564 \n",
      "              Params: tensor([  4.9272, -14.8112])\n",
      "              Grad: tensor([-0.0750,  0.4243])\n",
      "Epoch: 1147, Loss: 3.4722883701324463 \n",
      "              Params: tensor([  4.9280, -14.8154])\n",
      "              Grad: tensor([-0.0748,  0.4236])\n",
      "Epoch: 1148, Loss: 3.4704408645629883 \n",
      "              Params: tensor([  4.9287, -14.8196])\n",
      "              Grad: tensor([-0.0747,  0.4229])\n",
      "Epoch: 1149, Loss: 3.468597173690796 \n",
      "              Params: tensor([  4.9295, -14.8238])\n",
      "              Grad: tensor([-0.0746,  0.4222])\n",
      "Epoch: 1150, Loss: 3.466761827468872 \n",
      "              Params: tensor([  4.9302, -14.8281])\n",
      "              Grad: tensor([-0.0745,  0.4215])\n",
      "Epoch: 1151, Loss: 3.4649300575256348 \n",
      "              Params: tensor([  4.9309, -14.8323])\n",
      "              Grad: tensor([-0.0743,  0.4207])\n",
      "Epoch: 1152, Loss: 3.4631049633026123 \n",
      "              Params: tensor([  4.9317, -14.8365])\n",
      "              Grad: tensor([-0.0742,  0.4200])\n",
      "Epoch: 1153, Loss: 3.461289882659912 \n",
      "              Params: tensor([  4.9324, -14.8407])\n",
      "              Grad: tensor([-0.0741,  0.4193])\n",
      "Epoch: 1154, Loss: 3.459477424621582 \n",
      "              Params: tensor([  4.9332, -14.8448])\n",
      "              Grad: tensor([-0.0739,  0.4186])\n",
      "Epoch: 1155, Loss: 3.4576711654663086 \n",
      "              Params: tensor([  4.9339, -14.8490])\n",
      "              Grad: tensor([-0.0738,  0.4179])\n",
      "Epoch: 1156, Loss: 3.4558732509613037 \n",
      "              Params: tensor([  4.9346, -14.8532])\n",
      "              Grad: tensor([-0.0737,  0.4172])\n",
      "Epoch: 1157, Loss: 3.45408034324646 \n",
      "              Params: tensor([  4.9354, -14.8574])\n",
      "              Grad: tensor([-0.0736,  0.4165])\n",
      "Epoch: 1158, Loss: 3.4522926807403564 \n",
      "              Params: tensor([  4.9361, -14.8615])\n",
      "              Grad: tensor([-0.0734,  0.4158])\n",
      "Epoch: 1159, Loss: 3.450512647628784 \n",
      "              Params: tensor([  4.9368, -14.8657])\n",
      "              Grad: tensor([-0.0733,  0.4151])\n",
      "Epoch: 1160, Loss: 3.4487359523773193 \n",
      "              Params: tensor([  4.9376, -14.8698])\n",
      "              Grad: tensor([-0.0732,  0.4143])\n",
      "Epoch: 1161, Loss: 3.446967840194702 \n",
      "              Params: tensor([  4.9383, -14.8739])\n",
      "              Grad: tensor([-0.0731,  0.4136])\n",
      "Epoch: 1162, Loss: 3.445202589035034 \n",
      "              Params: tensor([  4.9390, -14.8781])\n",
      "              Grad: tensor([-0.0730,  0.4129])\n",
      "Epoch: 1163, Loss: 3.4434492588043213 \n",
      "              Params: tensor([  4.9398, -14.8822])\n",
      "              Grad: tensor([-0.0728,  0.4122])\n",
      "Epoch: 1164, Loss: 3.441696882247925 \n",
      "              Params: tensor([  4.9405, -14.8863])\n",
      "              Grad: tensor([-0.0727,  0.4115])\n",
      "Epoch: 1165, Loss: 3.4399521350860596 \n",
      "              Params: tensor([  4.9412, -14.8904])\n",
      "              Grad: tensor([-0.0726,  0.4108])\n",
      "Epoch: 1166, Loss: 3.4382104873657227 \n",
      "              Params: tensor([  4.9419, -14.8945])\n",
      "              Grad: tensor([-0.0725,  0.4101])\n",
      "Epoch: 1167, Loss: 3.436479091644287 \n",
      "              Params: tensor([  4.9427, -14.8986])\n",
      "              Grad: tensor([-0.0723,  0.4094])\n",
      "Epoch: 1168, Loss: 3.434753179550171 \n",
      "              Params: tensor([  4.9434, -14.9027])\n",
      "              Grad: tensor([-0.0722,  0.4087])\n",
      "Epoch: 1169, Loss: 3.433029890060425 \n",
      "              Params: tensor([  4.9441, -14.9068])\n",
      "              Grad: tensor([-0.0721,  0.4081])\n",
      "Epoch: 1170, Loss: 3.431313991546631 \n",
      "              Params: tensor([  4.9448, -14.9109])\n",
      "              Grad: tensor([-0.0720,  0.4074])\n",
      "Epoch: 1171, Loss: 3.429607391357422 \n",
      "              Params: tensor([  4.9455, -14.9149])\n",
      "              Grad: tensor([-0.0719,  0.4067])\n",
      "Epoch: 1172, Loss: 3.427902936935425 \n",
      "              Params: tensor([  4.9463, -14.9190])\n",
      "              Grad: tensor([-0.0717,  0.4060])\n",
      "Epoch: 1173, Loss: 3.426203966140747 \n",
      "              Params: tensor([  4.9470, -14.9230])\n",
      "              Grad: tensor([-0.0716,  0.4053])\n",
      "Epoch: 1174, Loss: 3.4245095252990723 \n",
      "              Params: tensor([  4.9477, -14.9271])\n",
      "              Grad: tensor([-0.0715,  0.4046])\n",
      "Epoch: 1175, Loss: 3.422823190689087 \n",
      "              Params: tensor([  4.9484, -14.9311])\n",
      "              Grad: tensor([-0.0714,  0.4039])\n",
      "Epoch: 1176, Loss: 3.4211440086364746 \n",
      "              Params: tensor([  4.9491, -14.9352])\n",
      "              Grad: tensor([-0.0712,  0.4032])\n",
      "Epoch: 1177, Loss: 3.4194676876068115 \n",
      "              Params: tensor([  4.9498, -14.9392])\n",
      "              Grad: tensor([-0.0711,  0.4025])\n",
      "Epoch: 1178, Loss: 3.417797803878784 \n",
      "              Params: tensor([  4.9505, -14.9432])\n",
      "              Grad: tensor([-0.0710,  0.4019])\n",
      "Epoch: 1179, Loss: 3.4161338806152344 \n",
      "              Params: tensor([  4.9512, -14.9472])\n",
      "              Grad: tensor([-0.0709,  0.4012])\n",
      "Epoch: 1180, Loss: 3.414476156234741 \n",
      "              Params: tensor([  4.9520, -14.9512])\n",
      "              Grad: tensor([-0.0708,  0.4005])\n",
      "Epoch: 1181, Loss: 3.4128243923187256 \n",
      "              Params: tensor([  4.9527, -14.9552])\n",
      "              Grad: tensor([-0.0706,  0.3998])\n",
      "Epoch: 1182, Loss: 3.4111764430999756 \n",
      "              Params: tensor([  4.9534, -14.9592])\n",
      "              Grad: tensor([-0.0705,  0.3991])\n",
      "Epoch: 1183, Loss: 3.409534454345703 \n",
      "              Params: tensor([  4.9541, -14.9632])\n",
      "              Grad: tensor([-0.0704,  0.3985])\n",
      "Epoch: 1184, Loss: 3.4078996181488037 \n",
      "              Params: tensor([  4.9548, -14.9672])\n",
      "              Grad: tensor([-0.0703,  0.3978])\n",
      "Epoch: 1185, Loss: 3.40627121925354 \n",
      "              Params: tensor([  4.9555, -14.9711])\n",
      "              Grad: tensor([-0.0701,  0.3971])\n",
      "Epoch: 1186, Loss: 3.4046454429626465 \n",
      "              Params: tensor([  4.9562, -14.9751])\n",
      "              Grad: tensor([-0.0700,  0.3964])\n",
      "Epoch: 1187, Loss: 3.403024435043335 \n",
      "              Params: tensor([  4.9569, -14.9791])\n",
      "              Grad: tensor([-0.0699,  0.3958])\n",
      "Epoch: 1188, Loss: 3.4014132022857666 \n",
      "              Params: tensor([  4.9576, -14.9830])\n",
      "              Grad: tensor([-0.0698,  0.3951])\n",
      "Epoch: 1189, Loss: 3.3998024463653564 \n",
      "              Params: tensor([  4.9583, -14.9870])\n",
      "              Grad: tensor([-0.0697,  0.3944])\n",
      "Epoch: 1190, Loss: 3.3981995582580566 \n",
      "              Params: tensor([  4.9590, -14.9909])\n",
      "              Grad: tensor([-0.0696,  0.3937])\n",
      "Epoch: 1191, Loss: 3.3966023921966553 \n",
      "              Params: tensor([  4.9597, -14.9948])\n",
      "              Grad: tensor([-0.0694,  0.3931])\n",
      "Epoch: 1192, Loss: 3.3950111865997314 \n",
      "              Params: tensor([  4.9604, -14.9988])\n",
      "              Grad: tensor([-0.0693,  0.3924])\n",
      "Epoch: 1193, Loss: 3.3934249877929688 \n",
      "              Params: tensor([  4.9610, -15.0027])\n",
      "              Grad: tensor([-0.0692,  0.3917])\n",
      "Epoch: 1194, Loss: 3.3918442726135254 \n",
      "              Params: tensor([  4.9617, -15.0066])\n",
      "              Grad: tensor([-0.0691,  0.3911])\n",
      "Epoch: 1195, Loss: 3.3902664184570312 \n",
      "              Params: tensor([  4.9624, -15.0105])\n",
      "              Grad: tensor([-0.0690,  0.3904])\n",
      "Epoch: 1196, Loss: 3.3886969089508057 \n",
      "              Params: tensor([  4.9631, -15.0144])\n",
      "              Grad: tensor([-0.0689,  0.3897])\n",
      "Epoch: 1197, Loss: 3.387131452560425 \n",
      "              Params: tensor([  4.9638, -15.0183])\n",
      "              Grad: tensor([-0.0687,  0.3891])\n",
      "Epoch: 1198, Loss: 3.385570764541626 \n",
      "              Params: tensor([  4.9645, -15.0222])\n",
      "              Grad: tensor([-0.0686,  0.3884])\n",
      "Epoch: 1199, Loss: 3.3840177059173584 \n",
      "              Params: tensor([  4.9652, -15.0260])\n",
      "              Grad: tensor([-0.0685,  0.3878])\n",
      "Epoch: 1200, Loss: 3.3824667930603027 \n",
      "              Params: tensor([  4.9659, -15.0299])\n",
      "              Grad: tensor([-0.0684,  0.3871])\n",
      "Epoch: 1201, Loss: 3.380925178527832 \n",
      "              Params: tensor([  4.9665, -15.0338])\n",
      "              Grad: tensor([-0.0683,  0.3864])\n",
      "Epoch: 1202, Loss: 3.379385471343994 \n",
      "              Params: tensor([  4.9672, -15.0376])\n",
      "              Grad: tensor([-0.0681,  0.3858])\n",
      "Epoch: 1203, Loss: 3.3778510093688965 \n",
      "              Params: tensor([  4.9679, -15.0415])\n",
      "              Grad: tensor([-0.0680,  0.3851])\n",
      "Epoch: 1204, Loss: 3.3763234615325928 \n",
      "              Params: tensor([  4.9686, -15.0453])\n",
      "              Grad: tensor([-0.0679,  0.3845])\n",
      "Epoch: 1205, Loss: 3.374800205230713 \n",
      "              Params: tensor([  4.9693, -15.0492])\n",
      "              Grad: tensor([-0.0678,  0.3838])\n",
      "Epoch: 1206, Loss: 3.373284101486206 \n",
      "              Params: tensor([  4.9699, -15.0530])\n",
      "              Grad: tensor([-0.0677,  0.3832])\n",
      "Epoch: 1207, Loss: 3.3717689514160156 \n",
      "              Params: tensor([  4.9706, -15.0568])\n",
      "              Grad: tensor([-0.0676,  0.3825])\n",
      "Epoch: 1208, Loss: 3.3702611923217773 \n",
      "              Params: tensor([  4.9713, -15.0606])\n",
      "              Grad: tensor([-0.0675,  0.3819])\n",
      "Epoch: 1209, Loss: 3.368760108947754 \n",
      "              Params: tensor([  4.9720, -15.0645])\n",
      "              Grad: tensor([-0.0673,  0.3812])\n",
      "Epoch: 1210, Loss: 3.3672616481781006 \n",
      "              Params: tensor([  4.9726, -15.0683])\n",
      "              Grad: tensor([-0.0672,  0.3806])\n",
      "Epoch: 1211, Loss: 3.3657712936401367 \n",
      "              Params: tensor([  4.9733, -15.0721])\n",
      "              Grad: tensor([-0.0671,  0.3799])\n",
      "Epoch: 1212, Loss: 3.3642821311950684 \n",
      "              Params: tensor([  4.9740, -15.0758])\n",
      "              Grad: tensor([-0.0670,  0.3793])\n",
      "Epoch: 1213, Loss: 3.362799644470215 \n",
      "              Params: tensor([  4.9746, -15.0796])\n",
      "              Grad: tensor([-0.0669,  0.3786])\n",
      "Epoch: 1214, Loss: 3.3613243103027344 \n",
      "              Params: tensor([  4.9753, -15.0834])\n",
      "              Grad: tensor([-0.0668,  0.3780])\n",
      "Epoch: 1215, Loss: 3.3598504066467285 \n",
      "              Params: tensor([  4.9760, -15.0872])\n",
      "              Grad: tensor([-0.0667,  0.3774])\n",
      "Epoch: 1216, Loss: 3.358383893966675 \n",
      "              Params: tensor([  4.9766, -15.0910])\n",
      "              Grad: tensor([-0.0665,  0.3767])\n",
      "Epoch: 1217, Loss: 3.3569211959838867 \n",
      "              Params: tensor([  4.9773, -15.0947])\n",
      "              Grad: tensor([-0.0664,  0.3761])\n",
      "Epoch: 1218, Loss: 3.355463981628418 \n",
      "              Params: tensor([  4.9780, -15.0985])\n",
      "              Grad: tensor([-0.0663,  0.3754])\n",
      "Epoch: 1219, Loss: 3.3540124893188477 \n",
      "              Params: tensor([  4.9786, -15.1022])\n",
      "              Grad: tensor([-0.0662,  0.3748])\n",
      "Epoch: 1220, Loss: 3.3525640964508057 \n",
      "              Params: tensor([  4.9793, -15.1060])\n",
      "              Grad: tensor([-0.0661,  0.3742])\n",
      "Epoch: 1221, Loss: 3.3511223793029785 \n",
      "              Params: tensor([  4.9799, -15.1097])\n",
      "              Grad: tensor([-0.0660,  0.3735])\n",
      "Epoch: 1222, Loss: 3.349684715270996 \n",
      "              Params: tensor([  4.9806, -15.1134])\n",
      "              Grad: tensor([-0.0659,  0.3729])\n",
      "Epoch: 1223, Loss: 3.3482511043548584 \n",
      "              Params: tensor([  4.9813, -15.1171])\n",
      "              Grad: tensor([-0.0657,  0.3723])\n",
      "Epoch: 1224, Loss: 3.3468244075775146 \n",
      "              Params: tensor([  4.9819, -15.1209])\n",
      "              Grad: tensor([-0.0656,  0.3716])\n",
      "Epoch: 1225, Loss: 3.345402717590332 \n",
      "              Params: tensor([  4.9826, -15.1246])\n",
      "              Grad: tensor([-0.0655,  0.3710])\n",
      "Epoch: 1226, Loss: 3.3439817428588867 \n",
      "              Params: tensor([  4.9832, -15.1283])\n",
      "              Grad: tensor([-0.0654,  0.3704])\n",
      "Epoch: 1227, Loss: 3.3425705432891846 \n",
      "              Params: tensor([  4.9839, -15.1320])\n",
      "              Grad: tensor([-0.0653,  0.3697])\n",
      "Epoch: 1228, Loss: 3.3411600589752197 \n",
      "              Params: tensor([  4.9845, -15.1357])\n",
      "              Grad: tensor([-0.0652,  0.3691])\n",
      "Epoch: 1229, Loss: 3.3397583961486816 \n",
      "              Params: tensor([  4.9852, -15.1393])\n",
      "              Grad: tensor([-0.0651,  0.3685])\n",
      "Epoch: 1230, Loss: 3.3383591175079346 \n",
      "              Params: tensor([  4.9858, -15.1430])\n",
      "              Grad: tensor([-0.0650,  0.3679])\n",
      "Epoch: 1231, Loss: 3.3369646072387695 \n",
      "              Params: tensor([  4.9865, -15.1467])\n",
      "              Grad: tensor([-0.0649,  0.3672])\n",
      "Epoch: 1232, Loss: 3.3355767726898193 \n",
      "              Params: tensor([  4.9871, -15.1504])\n",
      "              Grad: tensor([-0.0648,  0.3666])\n",
      "Epoch: 1233, Loss: 3.3341917991638184 \n",
      "              Params: tensor([  4.9878, -15.1540])\n",
      "              Grad: tensor([-0.0646,  0.3660])\n",
      "Epoch: 1234, Loss: 3.332810878753662 \n",
      "              Params: tensor([  4.9884, -15.1577])\n",
      "              Grad: tensor([-0.0645,  0.3654])\n",
      "Epoch: 1235, Loss: 3.3314359188079834 \n",
      "              Params: tensor([  4.9891, -15.1613])\n",
      "              Grad: tensor([-0.0644,  0.3647])\n",
      "Epoch: 1236, Loss: 3.3300652503967285 \n",
      "              Params: tensor([  4.9897, -15.1650])\n",
      "              Grad: tensor([-0.0643,  0.3641])\n",
      "Epoch: 1237, Loss: 3.3286988735198975 \n",
      "              Params: tensor([  4.9904, -15.1686])\n",
      "              Grad: tensor([-0.0642,  0.3635])\n",
      "Epoch: 1238, Loss: 3.327338933944702 \n",
      "              Params: tensor([  4.9910, -15.1722])\n",
      "              Grad: tensor([-0.0641,  0.3629])\n",
      "Epoch: 1239, Loss: 3.325979709625244 \n",
      "              Params: tensor([  4.9916, -15.1759])\n",
      "              Grad: tensor([-0.0640,  0.3623])\n",
      "Epoch: 1240, Loss: 3.324627637863159 \n",
      "              Params: tensor([  4.9923, -15.1795])\n",
      "              Grad: tensor([-0.0639,  0.3617])\n",
      "Epoch: 1241, Loss: 3.32327938079834 \n",
      "              Params: tensor([  4.9929, -15.1831])\n",
      "              Grad: tensor([-0.0638,  0.3610])\n",
      "Epoch: 1242, Loss: 3.321934700012207 \n",
      "              Params: tensor([  4.9936, -15.1867])\n",
      "              Grad: tensor([-0.0637,  0.3604])\n",
      "Epoch: 1243, Loss: 3.3205995559692383 \n",
      "              Params: tensor([  4.9942, -15.1903])\n",
      "              Grad: tensor([-0.0636,  0.3598])\n",
      "Epoch: 1244, Loss: 3.3192641735076904 \n",
      "              Params: tensor([  4.9948, -15.1939])\n",
      "              Grad: tensor([-0.0635,  0.3592])\n",
      "Epoch: 1245, Loss: 3.317934513092041 \n",
      "              Params: tensor([  4.9955, -15.1975])\n",
      "              Grad: tensor([-0.0633,  0.3586])\n",
      "Epoch: 1246, Loss: 3.31661057472229 \n",
      "              Params: tensor([  4.9961, -15.2010])\n",
      "              Grad: tensor([-0.0633,  0.3580])\n",
      "Epoch: 1247, Loss: 3.3152894973754883 \n",
      "              Params: tensor([  4.9967, -15.2046])\n",
      "              Grad: tensor([-0.0631,  0.3574])\n",
      "Epoch: 1248, Loss: 3.3139734268188477 \n",
      "              Params: tensor([  4.9973, -15.2082])\n",
      "              Grad: tensor([-0.0630,  0.3568])\n",
      "Epoch: 1249, Loss: 3.3126630783081055 \n",
      "              Params: tensor([  4.9980, -15.2117])\n",
      "              Grad: tensor([-0.0629,  0.3562])\n",
      "Epoch: 1250, Loss: 3.3113532066345215 \n",
      "              Params: tensor([  4.9986, -15.2153])\n",
      "              Grad: tensor([-0.0628,  0.3556])\n",
      "Epoch: 1251, Loss: 3.3100526332855225 \n",
      "              Params: tensor([  4.9992, -15.2189])\n",
      "              Grad: tensor([-0.0627,  0.3550])\n",
      "Epoch: 1252, Loss: 3.3087563514709473 \n",
      "              Params: tensor([  4.9999, -15.2224])\n",
      "              Grad: tensor([-0.0626,  0.3543])\n",
      "Epoch: 1253, Loss: 3.3074629306793213 \n",
      "              Params: tensor([  5.0005, -15.2259])\n",
      "              Grad: tensor([-0.0625,  0.3537])\n",
      "Epoch: 1254, Loss: 3.3061704635620117 \n",
      "              Params: tensor([  5.0011, -15.2295])\n",
      "              Grad: tensor([-0.0624,  0.3531])\n",
      "Epoch: 1255, Loss: 3.304886817932129 \n",
      "              Params: tensor([  5.0017, -15.2330])\n",
      "              Grad: tensor([-0.0623,  0.3525])\n",
      "Epoch: 1256, Loss: 3.303605079650879 \n",
      "              Params: tensor([  5.0024, -15.2365])\n",
      "              Grad: tensor([-0.0622,  0.3519])\n",
      "Epoch: 1257, Loss: 3.3023290634155273 \n",
      "              Params: tensor([  5.0030, -15.2400])\n",
      "              Grad: tensor([-0.0620,  0.3514])\n",
      "Epoch: 1258, Loss: 3.3010573387145996 \n",
      "              Params: tensor([  5.0036, -15.2435])\n",
      "              Grad: tensor([-0.0620,  0.3508])\n",
      "Epoch: 1259, Loss: 3.299790859222412 \n",
      "              Params: tensor([  5.0042, -15.2470])\n",
      "              Grad: tensor([-0.0619,  0.3502])\n",
      "Epoch: 1260, Loss: 3.298527717590332 \n",
      "              Params: tensor([  5.0048, -15.2505])\n",
      "              Grad: tensor([-0.0618,  0.3496])\n",
      "Epoch: 1261, Loss: 3.297266960144043 \n",
      "              Params: tensor([  5.0054, -15.2540])\n",
      "              Grad: tensor([-0.0616,  0.3490])\n",
      "Epoch: 1262, Loss: 3.296013593673706 \n",
      "              Params: tensor([  5.0061, -15.2575])\n",
      "              Grad: tensor([-0.0615,  0.3484])\n",
      "Epoch: 1263, Loss: 3.294762372970581 \n",
      "              Params: tensor([  5.0067, -15.2610])\n",
      "              Grad: tensor([-0.0614,  0.3478])\n",
      "Epoch: 1264, Loss: 3.2935173511505127 \n",
      "              Params: tensor([  5.0073, -15.2645])\n",
      "              Grad: tensor([-0.0613,  0.3472])\n",
      "Epoch: 1265, Loss: 3.292275905609131 \n",
      "              Params: tensor([  5.0079, -15.2679])\n",
      "              Grad: tensor([-0.0612,  0.3466])\n",
      "Epoch: 1266, Loss: 3.291036367416382 \n",
      "              Params: tensor([  5.0085, -15.2714])\n",
      "              Grad: tensor([-0.0611,  0.3460])\n",
      "Epoch: 1267, Loss: 3.289803981781006 \n",
      "              Params: tensor([  5.0091, -15.2748])\n",
      "              Grad: tensor([-0.0610,  0.3454])\n",
      "Epoch: 1268, Loss: 3.2885732650756836 \n",
      "              Params: tensor([  5.0097, -15.2783])\n",
      "              Grad: tensor([-0.0609,  0.3448])\n",
      "Epoch: 1269, Loss: 3.287346839904785 \n",
      "              Params: tensor([  5.0103, -15.2817])\n",
      "              Grad: tensor([-0.0608,  0.3443])\n",
      "Epoch: 1270, Loss: 3.2861289978027344 \n",
      "              Params: tensor([  5.0109, -15.2852])\n",
      "              Grad: tensor([-0.0607,  0.3437])\n",
      "Epoch: 1271, Loss: 3.2849111557006836 \n",
      "              Params: tensor([  5.0116, -15.2886])\n",
      "              Grad: tensor([-0.0606,  0.3431])\n",
      "Epoch: 1272, Loss: 3.2836976051330566 \n",
      "              Params: tensor([  5.0122, -15.2920])\n",
      "              Grad: tensor([-0.0605,  0.3425])\n",
      "Epoch: 1273, Loss: 3.2824883460998535 \n",
      "              Params: tensor([  5.0128, -15.2954])\n",
      "              Grad: tensor([-0.0604,  0.3419])\n",
      "Epoch: 1274, Loss: 3.2812840938568115 \n",
      "              Params: tensor([  5.0134, -15.2988])\n",
      "              Grad: tensor([-0.0603,  0.3413])\n",
      "Epoch: 1275, Loss: 3.2800850868225098 \n",
      "              Params: tensor([  5.0140, -15.3023])\n",
      "              Grad: tensor([-0.0602,  0.3408])\n",
      "Epoch: 1276, Loss: 3.278887987136841 \n",
      "              Params: tensor([  5.0146, -15.3057])\n",
      "              Grad: tensor([-0.0601,  0.3402])\n",
      "Epoch: 1277, Loss: 3.277696132659912 \n",
      "              Params: tensor([  5.0152, -15.3091])\n",
      "              Grad: tensor([-0.0600,  0.3396])\n",
      "Epoch: 1278, Loss: 3.276506185531616 \n",
      "              Params: tensor([  5.0158, -15.3124])\n",
      "              Grad: tensor([-0.0599,  0.3390])\n",
      "Epoch: 1279, Loss: 3.275322198867798 \n",
      "              Params: tensor([  5.0164, -15.3158])\n",
      "              Grad: tensor([-0.0598,  0.3384])\n",
      "Epoch: 1280, Loss: 3.274141550064087 \n",
      "              Params: tensor([  5.0170, -15.3192])\n",
      "              Grad: tensor([-0.0597,  0.3379])\n",
      "Epoch: 1281, Loss: 3.272967576980591 \n",
      "              Params: tensor([  5.0176, -15.3226])\n",
      "              Grad: tensor([-0.0596,  0.3373])\n",
      "Epoch: 1282, Loss: 3.2717931270599365 \n",
      "              Params: tensor([  5.0182, -15.3259])\n",
      "              Grad: tensor([-0.0595,  0.3367])\n",
      "Epoch: 1283, Loss: 3.270625114440918 \n",
      "              Params: tensor([  5.0187, -15.3293])\n",
      "              Grad: tensor([-0.0594,  0.3362])\n",
      "Epoch: 1284, Loss: 3.269460439682007 \n",
      "              Params: tensor([  5.0193, -15.3327])\n",
      "              Grad: tensor([-0.0593,  0.3356])\n",
      "Epoch: 1285, Loss: 3.268301248550415 \n",
      "              Params: tensor([  5.0199, -15.3360])\n",
      "              Grad: tensor([-0.0592,  0.3350])\n",
      "Epoch: 1286, Loss: 3.267143487930298 \n",
      "              Params: tensor([  5.0205, -15.3394])\n",
      "              Grad: tensor([-0.0591,  0.3344])\n",
      "Epoch: 1287, Loss: 3.265991449356079 \n",
      "              Params: tensor([  5.0211, -15.3427])\n",
      "              Grad: tensor([-0.0590,  0.3339])\n",
      "Epoch: 1288, Loss: 3.2648420333862305 \n",
      "              Params: tensor([  5.0217, -15.3460])\n",
      "              Grad: tensor([-0.0589,  0.3333])\n",
      "Epoch: 1289, Loss: 3.263699531555176 \n",
      "              Params: tensor([  5.0223, -15.3494])\n",
      "              Grad: tensor([-0.0588,  0.3327])\n",
      "Epoch: 1290, Loss: 3.2625558376312256 \n",
      "              Params: tensor([  5.0229, -15.3527])\n",
      "              Grad: tensor([-0.0587,  0.3322])\n",
      "Epoch: 1291, Loss: 3.2614214420318604 \n",
      "              Params: tensor([  5.0235, -15.3560])\n",
      "              Grad: tensor([-0.0586,  0.3316])\n",
      "Epoch: 1292, Loss: 3.260287284851074 \n",
      "              Params: tensor([  5.0240, -15.3593])\n",
      "              Grad: tensor([-0.0585,  0.3311])\n",
      "Epoch: 1293, Loss: 3.259160041809082 \n",
      "              Params: tensor([  5.0246, -15.3626])\n",
      "              Grad: tensor([-0.0584,  0.3305])\n",
      "Epoch: 1294, Loss: 3.258033037185669 \n",
      "              Params: tensor([  5.0252, -15.3659])\n",
      "              Grad: tensor([-0.0583,  0.3299])\n",
      "Epoch: 1295, Loss: 3.256911516189575 \n",
      "              Params: tensor([  5.0258, -15.3692])\n",
      "              Grad: tensor([-0.0582,  0.3294])\n",
      "Epoch: 1296, Loss: 3.255795478820801 \n",
      "              Params: tensor([  5.0264, -15.3725])\n",
      "              Grad: tensor([-0.0581,  0.3288])\n",
      "Epoch: 1297, Loss: 3.254680871963501 \n",
      "              Params: tensor([  5.0270, -15.3758])\n",
      "              Grad: tensor([-0.0580,  0.3282])\n",
      "Epoch: 1298, Loss: 3.2535688877105713 \n",
      "              Params: tensor([  5.0275, -15.3791])\n",
      "              Grad: tensor([-0.0579,  0.3277])\n",
      "Epoch: 1299, Loss: 3.2524619102478027 \n",
      "              Params: tensor([  5.0281, -15.3823])\n",
      "              Grad: tensor([-0.0578,  0.3271])\n",
      "Epoch: 1300, Loss: 3.251361608505249 \n",
      "              Params: tensor([  5.0287, -15.3856])\n",
      "              Grad: tensor([-0.0577,  0.3266])\n",
      "Epoch: 1301, Loss: 3.2502634525299072 \n",
      "              Params: tensor([  5.0293, -15.3888])\n",
      "              Grad: tensor([-0.0576,  0.3260])\n",
      "Epoch: 1302, Loss: 3.2491676807403564 \n",
      "              Params: tensor([  5.0298, -15.3921])\n",
      "              Grad: tensor([-0.0575,  0.3255])\n",
      "Epoch: 1303, Loss: 3.2480766773223877 \n",
      "              Params: tensor([  5.0304, -15.3954])\n",
      "              Grad: tensor([-0.0574,  0.3249])\n",
      "Epoch: 1304, Loss: 3.24698805809021 \n",
      "              Params: tensor([  5.0310, -15.3986])\n",
      "              Grad: tensor([-0.0573,  0.3244])\n",
      "Epoch: 1305, Loss: 3.245903968811035 \n",
      "              Params: tensor([  5.0316, -15.4018])\n",
      "              Grad: tensor([-0.0572,  0.3238])\n",
      "Epoch: 1306, Loss: 3.244824171066284 \n",
      "              Params: tensor([  5.0321, -15.4051])\n",
      "              Grad: tensor([-0.0571,  0.3233])\n",
      "Epoch: 1307, Loss: 3.243746757507324 \n",
      "              Params: tensor([  5.0327, -15.4083])\n",
      "              Grad: tensor([-0.0570,  0.3227])\n",
      "Epoch: 1308, Loss: 3.2426743507385254 \n",
      "              Params: tensor([  5.0333, -15.4115])\n",
      "              Grad: tensor([-0.0569,  0.3222])\n",
      "Epoch: 1309, Loss: 3.2416059970855713 \n",
      "              Params: tensor([  5.0338, -15.4147])\n",
      "              Grad: tensor([-0.0568,  0.3216])\n",
      "Epoch: 1310, Loss: 3.2405378818511963 \n",
      "              Params: tensor([  5.0344, -15.4179])\n",
      "              Grad: tensor([-0.0567,  0.3211])\n",
      "Epoch: 1311, Loss: 3.2394745349884033 \n",
      "              Params: tensor([  5.0350, -15.4211])\n",
      "              Grad: tensor([-0.0566,  0.3205])\n",
      "Epoch: 1312, Loss: 3.2384192943573 \n",
      "              Params: tensor([  5.0355, -15.4243])\n",
      "              Grad: tensor([-0.0565,  0.3200])\n",
      "Epoch: 1313, Loss: 3.237363338470459 \n",
      "              Params: tensor([  5.0361, -15.4275])\n",
      "              Grad: tensor([-0.0564,  0.3194])\n",
      "Epoch: 1314, Loss: 3.236313819885254 \n",
      "              Params: tensor([  5.0367, -15.4307])\n",
      "              Grad: tensor([-0.0563,  0.3189])\n",
      "Epoch: 1315, Loss: 3.235264778137207 \n",
      "              Params: tensor([  5.0372, -15.4339])\n",
      "              Grad: tensor([-0.0562,  0.3184])\n",
      "Epoch: 1316, Loss: 3.2342183589935303 \n",
      "              Params: tensor([  5.0378, -15.4371])\n",
      "              Grad: tensor([-0.0561,  0.3178])\n",
      "Epoch: 1317, Loss: 3.2331793308258057 \n",
      "              Params: tensor([  5.0383, -15.4403])\n",
      "              Grad: tensor([-0.0561,  0.3173])\n",
      "Epoch: 1318, Loss: 3.2321431636810303 \n",
      "              Params: tensor([  5.0389, -15.4434])\n",
      "              Grad: tensor([-0.0560,  0.3167])\n",
      "Epoch: 1319, Loss: 3.2311086654663086 \n",
      "              Params: tensor([  5.0395, -15.4466])\n",
      "              Grad: tensor([-0.0558,  0.3162])\n",
      "Epoch: 1320, Loss: 3.2300782203674316 \n",
      "              Params: tensor([  5.0400, -15.4498])\n",
      "              Grad: tensor([-0.0558,  0.3157])\n",
      "Epoch: 1321, Loss: 3.229050636291504 \n",
      "              Params: tensor([  5.0406, -15.4529])\n",
      "              Grad: tensor([-0.0557,  0.3151])\n",
      "Epoch: 1322, Loss: 3.228027105331421 \n",
      "              Params: tensor([  5.0411, -15.4560])\n",
      "              Grad: tensor([-0.0556,  0.3146])\n",
      "Epoch: 1323, Loss: 3.2270095348358154 \n",
      "              Params: tensor([  5.0417, -15.4592])\n",
      "              Grad: tensor([-0.0555,  0.3141])\n",
      "Epoch: 1324, Loss: 3.225992441177368 \n",
      "              Params: tensor([  5.0422, -15.4623])\n",
      "              Grad: tensor([-0.0554,  0.3135])\n",
      "Epoch: 1325, Loss: 3.2249794006347656 \n",
      "              Params: tensor([  5.0428, -15.4655])\n",
      "              Grad: tensor([-0.0553,  0.3130])\n",
      "Epoch: 1326, Loss: 3.223970890045166 \n",
      "              Params: tensor([  5.0433, -15.4686])\n",
      "              Grad: tensor([-0.0552,  0.3125])\n",
      "Epoch: 1327, Loss: 3.2229654788970947 \n",
      "              Params: tensor([  5.0439, -15.4717])\n",
      "              Grad: tensor([-0.0551,  0.3119])\n",
      "Epoch: 1328, Loss: 3.2219603061676025 \n",
      "              Params: tensor([  5.0444, -15.4748])\n",
      "              Grad: tensor([-0.0550,  0.3114])\n",
      "Epoch: 1329, Loss: 3.2209622859954834 \n",
      "              Params: tensor([  5.0450, -15.4779])\n",
      "              Grad: tensor([-0.0549,  0.3109])\n",
      "Epoch: 1330, Loss: 3.2199668884277344 \n",
      "              Params: tensor([  5.0455, -15.4810])\n",
      "              Grad: tensor([-0.0548,  0.3103])\n",
      "Epoch: 1331, Loss: 3.218975067138672 \n",
      "              Params: tensor([  5.0461, -15.4841])\n",
      "              Grad: tensor([-0.0547,  0.3098])\n",
      "Epoch: 1332, Loss: 3.2179863452911377 \n",
      "              Params: tensor([  5.0466, -15.4872])\n",
      "              Grad: tensor([-0.0546,  0.3093])\n",
      "Epoch: 1333, Loss: 3.2169997692108154 \n",
      "              Params: tensor([  5.0472, -15.4903])\n",
      "              Grad: tensor([-0.0545,  0.3088])\n",
      "Epoch: 1334, Loss: 3.216017246246338 \n",
      "              Params: tensor([  5.0477, -15.4934])\n",
      "              Grad: tensor([-0.0544,  0.3082])\n",
      "Epoch: 1335, Loss: 3.215038299560547 \n",
      "              Params: tensor([  5.0483, -15.4965])\n",
      "              Grad: tensor([-0.0543,  0.3077])\n",
      "Epoch: 1336, Loss: 3.214062452316284 \n",
      "              Params: tensor([  5.0488, -15.4995])\n",
      "              Grad: tensor([-0.0543,  0.3072])\n",
      "Epoch: 1337, Loss: 3.213092088699341 \n",
      "              Params: tensor([  5.0494, -15.5026])\n",
      "              Grad: tensor([-0.0542,  0.3067])\n",
      "Epoch: 1338, Loss: 3.2121224403381348 \n",
      "              Params: tensor([  5.0499, -15.5057])\n",
      "              Grad: tensor([-0.0541,  0.3061])\n",
      "Epoch: 1339, Loss: 3.2111566066741943 \n",
      "              Params: tensor([  5.0504, -15.5087])\n",
      "              Grad: tensor([-0.0540,  0.3056])\n",
      "Epoch: 1340, Loss: 3.2101919651031494 \n",
      "              Params: tensor([  5.0510, -15.5118])\n",
      "              Grad: tensor([-0.0539,  0.3051])\n",
      "Epoch: 1341, Loss: 3.2092347145080566 \n",
      "              Params: tensor([  5.0515, -15.5148])\n",
      "              Grad: tensor([-0.0538,  0.3046])\n",
      "Epoch: 1342, Loss: 3.2082788944244385 \n",
      "              Params: tensor([  5.0521, -15.5179])\n",
      "              Grad: tensor([-0.0537,  0.3041])\n",
      "Epoch: 1343, Loss: 3.2073256969451904 \n",
      "              Params: tensor([  5.0526, -15.5209])\n",
      "              Grad: tensor([-0.0536,  0.3036])\n",
      "Epoch: 1344, Loss: 3.206376075744629 \n",
      "              Params: tensor([  5.0531, -15.5239])\n",
      "              Grad: tensor([-0.0535,  0.3030])\n",
      "Epoch: 1345, Loss: 3.205430030822754 \n",
      "              Params: tensor([  5.0537, -15.5269])\n",
      "              Grad: tensor([-0.0534,  0.3025])\n",
      "Epoch: 1346, Loss: 3.2044878005981445 \n",
      "              Params: tensor([  5.0542, -15.5300])\n",
      "              Grad: tensor([-0.0533,  0.3020])\n",
      "Epoch: 1347, Loss: 3.203547477722168 \n",
      "              Params: tensor([  5.0547, -15.5330])\n",
      "              Grad: tensor([-0.0532,  0.3015])\n",
      "Epoch: 1348, Loss: 3.2026102542877197 \n",
      "              Params: tensor([  5.0553, -15.5360])\n",
      "              Grad: tensor([-0.0532,  0.3010])\n",
      "Epoch: 1349, Loss: 3.2016775608062744 \n",
      "              Params: tensor([  5.0558, -15.5390])\n",
      "              Grad: tensor([-0.0531,  0.3005])\n",
      "Epoch: 1350, Loss: 3.200746774673462 \n",
      "              Params: tensor([  5.0563, -15.5420])\n",
      "              Grad: tensor([-0.0530,  0.3000])\n",
      "Epoch: 1351, Loss: 3.199820041656494 \n",
      "              Params: tensor([  5.0568, -15.5450])\n",
      "              Grad: tensor([-0.0529,  0.2995])\n",
      "Epoch: 1352, Loss: 3.198896884918213 \n",
      "              Params: tensor([  5.0574, -15.5480])\n",
      "              Grad: tensor([-0.0528,  0.2989])\n",
      "Epoch: 1353, Loss: 3.1979761123657227 \n",
      "              Params: tensor([  5.0579, -15.5510])\n",
      "              Grad: tensor([-0.0527,  0.2984])\n",
      "Epoch: 1354, Loss: 3.1970596313476562 \n",
      "              Params: tensor([  5.0584, -15.5539])\n",
      "              Grad: tensor([-0.0526,  0.2979])\n",
      "Epoch: 1355, Loss: 3.196143388748169 \n",
      "              Params: tensor([  5.0590, -15.5569])\n",
      "              Grad: tensor([-0.0525,  0.2974])\n",
      "Epoch: 1356, Loss: 3.1952309608459473 \n",
      "              Params: tensor([  5.0595, -15.5599])\n",
      "              Grad: tensor([-0.0524,  0.2969])\n",
      "Epoch: 1357, Loss: 3.194324254989624 \n",
      "              Params: tensor([  5.0600, -15.5629])\n",
      "              Grad: tensor([-0.0524,  0.2964])\n",
      "Epoch: 1358, Loss: 3.1934196949005127 \n",
      "              Params: tensor([  5.0605, -15.5658])\n",
      "              Grad: tensor([-0.0523,  0.2959])\n",
      "Epoch: 1359, Loss: 3.192516565322876 \n",
      "              Params: tensor([  5.0610, -15.5688])\n",
      "              Grad: tensor([-0.0522,  0.2954])\n",
      "Epoch: 1360, Loss: 3.1916162967681885 \n",
      "              Params: tensor([  5.0616, -15.5717])\n",
      "              Grad: tensor([-0.0521,  0.2949])\n",
      "Epoch: 1361, Loss: 3.190720319747925 \n",
      "              Params: tensor([  5.0621, -15.5747])\n",
      "              Grad: tensor([-0.0520,  0.2944])\n",
      "Epoch: 1362, Loss: 3.189828634262085 \n",
      "              Params: tensor([  5.0626, -15.5776])\n",
      "              Grad: tensor([-0.0519,  0.2939])\n",
      "Epoch: 1363, Loss: 3.1889379024505615 \n",
      "              Params: tensor([  5.0631, -15.5805])\n",
      "              Grad: tensor([-0.0518,  0.2934])\n",
      "Epoch: 1364, Loss: 3.1880507469177246 \n",
      "              Params: tensor([  5.0636, -15.5835])\n",
      "              Grad: tensor([-0.0517,  0.2929])\n",
      "Epoch: 1365, Loss: 3.1871659755706787 \n",
      "              Params: tensor([  5.0642, -15.5864])\n",
      "              Grad: tensor([-0.0516,  0.2924])\n",
      "Epoch: 1366, Loss: 3.1862871646881104 \n",
      "              Params: tensor([  5.0647, -15.5893])\n",
      "              Grad: tensor([-0.0516,  0.2919])\n",
      "Epoch: 1367, Loss: 3.185408592224121 \n",
      "              Params: tensor([  5.0652, -15.5922])\n",
      "              Grad: tensor([-0.0515,  0.2914])\n",
      "Epoch: 1368, Loss: 3.1845343112945557 \n",
      "              Params: tensor([  5.0657, -15.5951])\n",
      "              Grad: tensor([-0.0514,  0.2909])\n",
      "Epoch: 1369, Loss: 3.1836624145507812 \n",
      "              Params: tensor([  5.0662, -15.5980])\n",
      "              Grad: tensor([-0.0513,  0.2904])\n",
      "Epoch: 1370, Loss: 3.1827917098999023 \n",
      "              Params: tensor([  5.0667, -15.6009])\n",
      "              Grad: tensor([-0.0512,  0.2899])\n",
      "Epoch: 1371, Loss: 3.1819252967834473 \n",
      "              Params: tensor([  5.0672, -15.6038])\n",
      "              Grad: tensor([-0.0511,  0.2894])\n",
      "Epoch: 1372, Loss: 3.181062936782837 \n",
      "              Params: tensor([  5.0678, -15.6067])\n",
      "              Grad: tensor([-0.0510,  0.2890])\n",
      "Epoch: 1373, Loss: 3.1802008152008057 \n",
      "              Params: tensor([  5.0683, -15.6096])\n",
      "              Grad: tensor([-0.0509,  0.2885])\n",
      "Epoch: 1374, Loss: 3.1793465614318848 \n",
      "              Params: tensor([  5.0688, -15.6125])\n",
      "              Grad: tensor([-0.0509,  0.2880])\n",
      "Epoch: 1375, Loss: 3.178490400314331 \n",
      "              Params: tensor([  5.0693, -15.6154])\n",
      "              Grad: tensor([-0.0508,  0.2875])\n",
      "Epoch: 1376, Loss: 3.177638053894043 \n",
      "              Params: tensor([  5.0698, -15.6182])\n",
      "              Grad: tensor([-0.0507,  0.2870])\n",
      "Epoch: 1377, Loss: 3.1767892837524414 \n",
      "              Params: tensor([  5.0703, -15.6211])\n",
      "              Grad: tensor([-0.0506,  0.2865])\n",
      "Epoch: 1378, Loss: 3.1759445667266846 \n",
      "              Params: tensor([  5.0708, -15.6240])\n",
      "              Grad: tensor([-0.0505,  0.2860])\n",
      "Epoch: 1379, Loss: 3.1751012802124023 \n",
      "              Params: tensor([  5.0713, -15.6268])\n",
      "              Grad: tensor([-0.0504,  0.2855])\n",
      "Epoch: 1380, Loss: 3.1742615699768066 \n",
      "              Params: tensor([  5.0718, -15.6297])\n",
      "              Grad: tensor([-0.0504,  0.2850])\n",
      "Epoch: 1381, Loss: 3.17342472076416 \n",
      "              Params: tensor([  5.0723, -15.6325])\n",
      "              Grad: tensor([-0.0503,  0.2846])\n",
      "Epoch: 1382, Loss: 3.1725900173187256 \n",
      "              Params: tensor([  5.0728, -15.6353])\n",
      "              Grad: tensor([-0.0502,  0.2841])\n",
      "Epoch: 1383, Loss: 3.1717588901519775 \n",
      "              Params: tensor([  5.0733, -15.6382])\n",
      "              Grad: tensor([-0.0501,  0.2836])\n",
      "Epoch: 1384, Loss: 3.170928716659546 \n",
      "              Params: tensor([  5.0738, -15.6410])\n",
      "              Grad: tensor([-0.0500,  0.2831])\n",
      "Epoch: 1385, Loss: 3.170102834701538 \n",
      "              Params: tensor([  5.0743, -15.6438])\n",
      "              Grad: tensor([-0.0499,  0.2826])\n",
      "Epoch: 1386, Loss: 3.1692795753479004 \n",
      "              Params: tensor([  5.0748, -15.6467])\n",
      "              Grad: tensor([-0.0498,  0.2822])\n",
      "Epoch: 1387, Loss: 3.168461799621582 \n",
      "              Params: tensor([  5.0753, -15.6495])\n",
      "              Grad: tensor([-0.0498,  0.2817])\n",
      "Epoch: 1388, Loss: 3.1676435470581055 \n",
      "              Params: tensor([  5.0758, -15.6523])\n",
      "              Grad: tensor([-0.0497,  0.2812])\n",
      "Epoch: 1389, Loss: 3.166827440261841 \n",
      "              Params: tensor([  5.0763, -15.6551])\n",
      "              Grad: tensor([-0.0496,  0.2807])\n",
      "Epoch: 1390, Loss: 3.1660165786743164 \n",
      "              Params: tensor([  5.0768, -15.6579])\n",
      "              Grad: tensor([-0.0495,  0.2802])\n",
      "Epoch: 1391, Loss: 3.1652066707611084 \n",
      "              Params: tensor([  5.0773, -15.6607])\n",
      "              Grad: tensor([-0.0494,  0.2798])\n",
      "Epoch: 1392, Loss: 3.164400577545166 \n",
      "              Params: tensor([  5.0778, -15.6635])\n",
      "              Grad: tensor([-0.0493,  0.2793])\n",
      "Epoch: 1393, Loss: 3.1635942459106445 \n",
      "              Params: tensor([  5.0783, -15.6663])\n",
      "              Grad: tensor([-0.0492,  0.2788])\n",
      "Epoch: 1394, Loss: 3.162795066833496 \n",
      "              Params: tensor([  5.0788, -15.6691])\n",
      "              Grad: tensor([-0.0492,  0.2783])\n",
      "Epoch: 1395, Loss: 3.161996364593506 \n",
      "              Params: tensor([  5.0793, -15.6718])\n",
      "              Grad: tensor([-0.0491,  0.2779])\n",
      "Epoch: 1396, Loss: 3.161201238632202 \n",
      "              Params: tensor([  5.0797, -15.6746])\n",
      "              Grad: tensor([-0.0490,  0.2774])\n",
      "Epoch: 1397, Loss: 3.160410165786743 \n",
      "              Params: tensor([  5.0802, -15.6774])\n",
      "              Grad: tensor([-0.0489,  0.2769])\n",
      "Epoch: 1398, Loss: 3.1596181392669678 \n",
      "              Params: tensor([  5.0807, -15.6802])\n",
      "              Grad: tensor([-0.0488,  0.2765])\n",
      "Epoch: 1399, Loss: 3.158830404281616 \n",
      "              Params: tensor([  5.0812, -15.6829])\n",
      "              Grad: tensor([-0.0488,  0.2760])\n",
      "Epoch: 1400, Loss: 3.1580464839935303 \n",
      "              Params: tensor([  5.0817, -15.6857])\n",
      "              Grad: tensor([-0.0487,  0.2755])\n",
      "Epoch: 1401, Loss: 3.1572632789611816 \n",
      "              Params: tensor([  5.0822, -15.6884])\n",
      "              Grad: tensor([-0.0486,  0.2751])\n",
      "Epoch: 1402, Loss: 3.156484365463257 \n",
      "              Params: tensor([  5.0827, -15.6912])\n",
      "              Grad: tensor([-0.0485,  0.2746])\n",
      "Epoch: 1403, Loss: 3.155707597732544 \n",
      "              Params: tensor([  5.0832, -15.6939])\n",
      "              Grad: tensor([-0.0484,  0.2741])\n",
      "Epoch: 1404, Loss: 3.154932975769043 \n",
      "              Params: tensor([  5.0836, -15.6966])\n",
      "              Grad: tensor([-0.0483,  0.2736])\n",
      "Epoch: 1405, Loss: 3.1541619300842285 \n",
      "              Params: tensor([  5.0841, -15.6994])\n",
      "              Grad: tensor([-0.0483,  0.2732])\n",
      "Epoch: 1406, Loss: 3.153392791748047 \n",
      "              Params: tensor([  5.0846, -15.7021])\n",
      "              Grad: tensor([-0.0482,  0.2727])\n",
      "Epoch: 1407, Loss: 3.15262770652771 \n",
      "              Params: tensor([  5.0851, -15.7048])\n",
      "              Grad: tensor([-0.0481,  0.2723])\n",
      "Epoch: 1408, Loss: 3.151864528656006 \n",
      "              Params: tensor([  5.0856, -15.7075])\n",
      "              Grad: tensor([-0.0480,  0.2718])\n",
      "Epoch: 1409, Loss: 3.1511011123657227 \n",
      "              Params: tensor([  5.0860, -15.7103])\n",
      "              Grad: tensor([-0.0479,  0.2713])\n",
      "Epoch: 1410, Loss: 3.150343179702759 \n",
      "              Params: tensor([  5.0865, -15.7130])\n",
      "              Grad: tensor([-0.0479,  0.2709])\n",
      "Epoch: 1411, Loss: 3.1495866775512695 \n",
      "              Params: tensor([  5.0870, -15.7157])\n",
      "              Grad: tensor([-0.0478,  0.2704])\n",
      "Epoch: 1412, Loss: 3.1488332748413086 \n",
      "              Params: tensor([  5.0875, -15.7184])\n",
      "              Grad: tensor([-0.0477,  0.2700])\n",
      "Epoch: 1413, Loss: 3.148082733154297 \n",
      "              Params: tensor([  5.0879, -15.7211])\n",
      "              Grad: tensor([-0.0476,  0.2695])\n",
      "Epoch: 1414, Loss: 3.1473350524902344 \n",
      "              Params: tensor([  5.0884, -15.7238])\n",
      "              Grad: tensor([-0.0475,  0.2690])\n",
      "Epoch: 1415, Loss: 3.146588087081909 \n",
      "              Params: tensor([  5.0889, -15.7264])\n",
      "              Grad: tensor([-0.0474,  0.2686])\n",
      "Epoch: 1416, Loss: 3.1458449363708496 \n",
      "              Params: tensor([  5.0894, -15.7291])\n",
      "              Grad: tensor([-0.0474,  0.2681])\n",
      "Epoch: 1417, Loss: 3.1451051235198975 \n",
      "              Params: tensor([  5.0898, -15.7318])\n",
      "              Grad: tensor([-0.0473,  0.2677])\n",
      "Epoch: 1418, Loss: 3.1443674564361572 \n",
      "              Params: tensor([  5.0903, -15.7345])\n",
      "              Grad: tensor([-0.0472,  0.2672])\n",
      "Epoch: 1419, Loss: 3.143630027770996 \n",
      "              Params: tensor([  5.0908, -15.7371])\n",
      "              Grad: tensor([-0.0471,  0.2668])\n",
      "Epoch: 1420, Loss: 3.1428985595703125 \n",
      "              Params: tensor([  5.0913, -15.7398])\n",
      "              Grad: tensor([-0.0470,  0.2663])\n",
      "Epoch: 1421, Loss: 3.1421663761138916 \n",
      "              Params: tensor([  5.0917, -15.7425])\n",
      "              Grad: tensor([-0.0469,  0.2659])\n",
      "Epoch: 1422, Loss: 3.1414389610290527 \n",
      "              Params: tensor([  5.0922, -15.7451])\n",
      "              Grad: tensor([-0.0469,  0.2654])\n",
      "Epoch: 1423, Loss: 3.1407124996185303 \n",
      "              Params: tensor([  5.0927, -15.7478])\n",
      "              Grad: tensor([-0.0468,  0.2649])\n",
      "Epoch: 1424, Loss: 3.139988899230957 \n",
      "              Params: tensor([  5.0931, -15.7504])\n",
      "              Grad: tensor([-0.0467,  0.2645])\n",
      "Epoch: 1425, Loss: 3.1392714977264404 \n",
      "              Params: tensor([  5.0936, -15.7530])\n",
      "              Grad: tensor([-0.0466,  0.2641])\n",
      "Epoch: 1426, Loss: 3.1385505199432373 \n",
      "              Params: tensor([  5.0941, -15.7557])\n",
      "              Grad: tensor([-0.0466,  0.2636])\n",
      "Epoch: 1427, Loss: 3.1378347873687744 \n",
      "              Params: tensor([  5.0945, -15.7583])\n",
      "              Grad: tensor([-0.0465,  0.2632])\n",
      "Epoch: 1428, Loss: 3.1371209621429443 \n",
      "              Params: tensor([  5.0950, -15.7609])\n",
      "              Grad: tensor([-0.0464,  0.2627])\n",
      "Epoch: 1429, Loss: 3.136409044265747 \n",
      "              Params: tensor([  5.0955, -15.7636])\n",
      "              Grad: tensor([-0.0463,  0.2623])\n",
      "Epoch: 1430, Loss: 3.1357016563415527 \n",
      "              Params: tensor([  5.0959, -15.7662])\n",
      "              Grad: tensor([-0.0462,  0.2618])\n",
      "Epoch: 1431, Loss: 3.1349942684173584 \n",
      "              Params: tensor([  5.0964, -15.7688])\n",
      "              Grad: tensor([-0.0461,  0.2614])\n",
      "Epoch: 1432, Loss: 3.134291648864746 \n",
      "              Params: tensor([  5.0968, -15.7714])\n",
      "              Grad: tensor([-0.0461,  0.2609])\n",
      "Epoch: 1433, Loss: 3.13358998298645 \n",
      "              Params: tensor([  5.0973, -15.7740])\n",
      "              Grad: tensor([-0.0460,  0.2605])\n",
      "Epoch: 1434, Loss: 3.1328887939453125 \n",
      "              Params: tensor([  5.0978, -15.7766])\n",
      "              Grad: tensor([-0.0459,  0.2600])\n",
      "Epoch: 1435, Loss: 3.1321942806243896 \n",
      "              Params: tensor([  5.0982, -15.7792])\n",
      "              Grad: tensor([-0.0459,  0.2596])\n",
      "Epoch: 1436, Loss: 3.131500244140625 \n",
      "              Params: tensor([  5.0987, -15.7818])\n",
      "              Grad: tensor([-0.0458,  0.2592])\n",
      "Epoch: 1437, Loss: 3.130810022354126 \n",
      "              Params: tensor([  5.0991, -15.7844])\n",
      "              Grad: tensor([-0.0457,  0.2587])\n",
      "Epoch: 1438, Loss: 3.1301186084747314 \n",
      "              Params: tensor([  5.0996, -15.7870])\n",
      "              Grad: tensor([-0.0456,  0.2583])\n",
      "Epoch: 1439, Loss: 3.129431962966919 \n",
      "              Params: tensor([  5.1000, -15.7895])\n",
      "              Grad: tensor([-0.0455,  0.2578])\n",
      "Epoch: 1440, Loss: 3.1287457942962646 \n",
      "              Params: tensor([  5.1005, -15.7921])\n",
      "              Grad: tensor([-0.0455,  0.2574])\n",
      "Epoch: 1441, Loss: 3.1280641555786133 \n",
      "              Params: tensor([  5.1010, -15.7947])\n",
      "              Grad: tensor([-0.0454,  0.2570])\n",
      "Epoch: 1442, Loss: 3.1273818016052246 \n",
      "              Params: tensor([  5.1014, -15.7973])\n",
      "              Grad: tensor([-0.0453,  0.2565])\n",
      "Epoch: 1443, Loss: 3.1267051696777344 \n",
      "              Params: tensor([  5.1019, -15.7998])\n",
      "              Grad: tensor([-0.0453,  0.2561])\n",
      "Epoch: 1444, Loss: 3.126030206680298 \n",
      "              Params: tensor([  5.1023, -15.8024])\n",
      "              Grad: tensor([-0.0452,  0.2557])\n",
      "Epoch: 1445, Loss: 3.125356435775757 \n",
      "              Params: tensor([  5.1028, -15.8049])\n",
      "              Grad: tensor([-0.0451,  0.2552])\n",
      "Epoch: 1446, Loss: 3.124683380126953 \n",
      "              Params: tensor([  5.1032, -15.8075])\n",
      "              Grad: tensor([-0.0450,  0.2548])\n",
      "Epoch: 1447, Loss: 3.124016046524048 \n",
      "              Params: tensor([  5.1037, -15.8100])\n",
      "              Grad: tensor([-0.0449,  0.2544])\n",
      "Epoch: 1448, Loss: 3.1233489513397217 \n",
      "              Params: tensor([  5.1041, -15.8126])\n",
      "              Grad: tensor([-0.0449,  0.2539])\n",
      "Epoch: 1449, Loss: 3.1226859092712402 \n",
      "              Params: tensor([  5.1046, -15.8151])\n",
      "              Grad: tensor([-0.0448,  0.2535])\n",
      "Epoch: 1450, Loss: 3.1220223903656006 \n",
      "              Params: tensor([  5.1050, -15.8176])\n",
      "              Grad: tensor([-0.0447,  0.2531])\n",
      "Epoch: 1451, Loss: 3.1213622093200684 \n",
      "              Params: tensor([  5.1055, -15.8201])\n",
      "              Grad: tensor([-0.0446,  0.2526])\n",
      "Epoch: 1452, Loss: 3.120706796646118 \n",
      "              Params: tensor([  5.1059, -15.8227])\n",
      "              Grad: tensor([-0.0445,  0.2522])\n",
      "Epoch: 1453, Loss: 3.1200485229492188 \n",
      "              Params: tensor([  5.1063, -15.8252])\n",
      "              Grad: tensor([-0.0445,  0.2518])\n",
      "Epoch: 1454, Loss: 3.119396924972534 \n",
      "              Params: tensor([  5.1068, -15.8277])\n",
      "              Grad: tensor([-0.0444,  0.2513])\n",
      "Epoch: 1455, Loss: 3.118746042251587 \n",
      "              Params: tensor([  5.1072, -15.8302])\n",
      "              Grad: tensor([-0.0443,  0.2509])\n",
      "Epoch: 1456, Loss: 3.1180977821350098 \n",
      "              Params: tensor([  5.1077, -15.8327])\n",
      "              Grad: tensor([-0.0442,  0.2505])\n",
      "Epoch: 1457, Loss: 3.1174514293670654 \n",
      "              Params: tensor([  5.1081, -15.8352])\n",
      "              Grad: tensor([-0.0442,  0.2501])\n",
      "Epoch: 1458, Loss: 3.116805076599121 \n",
      "              Params: tensor([  5.1086, -15.8377])\n",
      "              Grad: tensor([-0.0441,  0.2496])\n",
      "Epoch: 1459, Loss: 3.116163730621338 \n",
      "              Params: tensor([  5.1090, -15.8402])\n",
      "              Grad: tensor([-0.0440,  0.2492])\n",
      "Epoch: 1460, Loss: 3.1155245304107666 \n",
      "              Params: tensor([  5.1094, -15.8427])\n",
      "              Grad: tensor([-0.0439,  0.2488])\n",
      "Epoch: 1461, Loss: 3.1148862838745117 \n",
      "              Params: tensor([  5.1099, -15.8452])\n",
      "              Grad: tensor([-0.0439,  0.2484])\n",
      "Epoch: 1462, Loss: 3.114250898361206 \n",
      "              Params: tensor([  5.1103, -15.8477])\n",
      "              Grad: tensor([-0.0438,  0.2480])\n",
      "Epoch: 1463, Loss: 3.113616704940796 \n",
      "              Params: tensor([  5.1107, -15.8501])\n",
      "              Grad: tensor([-0.0437,  0.2475])\n",
      "Epoch: 1464, Loss: 3.1129846572875977 \n",
      "              Params: tensor([  5.1112, -15.8526])\n",
      "              Grad: tensor([-0.0437,  0.2471])\n",
      "Epoch: 1465, Loss: 3.1123578548431396 \n",
      "              Params: tensor([  5.1116, -15.8551])\n",
      "              Grad: tensor([-0.0436,  0.2467])\n",
      "Epoch: 1466, Loss: 3.1117308139801025 \n",
      "              Params: tensor([  5.1121, -15.8575])\n",
      "              Grad: tensor([-0.0435,  0.2463])\n",
      "Epoch: 1467, Loss: 3.111102819442749 \n",
      "              Params: tensor([  5.1125, -15.8600])\n",
      "              Grad: tensor([-0.0434,  0.2459])\n",
      "Epoch: 1468, Loss: 3.1104841232299805 \n",
      "              Params: tensor([  5.1129, -15.8624])\n",
      "              Grad: tensor([-0.0433,  0.2454])\n",
      "Epoch: 1469, Loss: 3.1098592281341553 \n",
      "              Params: tensor([  5.1134, -15.8649])\n",
      "              Grad: tensor([-0.0433,  0.2450])\n",
      "Epoch: 1470, Loss: 3.1092429161071777 \n",
      "              Params: tensor([  5.1138, -15.8673])\n",
      "              Grad: tensor([-0.0432,  0.2446])\n",
      "Epoch: 1471, Loss: 3.1086266040802 \n",
      "              Params: tensor([  5.1142, -15.8698])\n",
      "              Grad: tensor([-0.0431,  0.2442])\n",
      "Epoch: 1472, Loss: 3.10801100730896 \n",
      "              Params: tensor([  5.1147, -15.8722])\n",
      "              Grad: tensor([-0.0430,  0.2438])\n",
      "Epoch: 1473, Loss: 3.10740065574646 \n",
      "              Params: tensor([  5.1151, -15.8747])\n",
      "              Grad: tensor([-0.0430,  0.2434])\n",
      "Epoch: 1474, Loss: 3.106790781021118 \n",
      "              Params: tensor([  5.1155, -15.8771])\n",
      "              Grad: tensor([-0.0429,  0.2429])\n",
      "Epoch: 1475, Loss: 3.106180429458618 \n",
      "              Params: tensor([  5.1159, -15.8795])\n",
      "              Grad: tensor([-0.0428,  0.2425])\n",
      "Epoch: 1476, Loss: 3.1055753231048584 \n",
      "              Params: tensor([  5.1164, -15.8819])\n",
      "              Grad: tensor([-0.0428,  0.2421])\n",
      "Epoch: 1477, Loss: 3.1049718856811523 \n",
      "              Params: tensor([  5.1168, -15.8843])\n",
      "              Grad: tensor([-0.0427,  0.2417])\n",
      "Epoch: 1478, Loss: 3.1043701171875 \n",
      "              Params: tensor([  5.1172, -15.8868])\n",
      "              Grad: tensor([-0.0426,  0.2413])\n",
      "Epoch: 1479, Loss: 3.1037697792053223 \n",
      "              Params: tensor([  5.1176, -15.8892])\n",
      "              Grad: tensor([-0.0425,  0.2409])\n",
      "Epoch: 1480, Loss: 3.1031720638275146 \n",
      "              Params: tensor([  5.1181, -15.8916])\n",
      "              Grad: tensor([-0.0425,  0.2405])\n",
      "Epoch: 1481, Loss: 3.102576494216919 \n",
      "              Params: tensor([  5.1185, -15.8940])\n",
      "              Grad: tensor([-0.0424,  0.2401])\n",
      "Epoch: 1482, Loss: 3.1019821166992188 \n",
      "              Params: tensor([  5.1189, -15.8964])\n",
      "              Grad: tensor([-0.0423,  0.2397])\n",
      "Epoch: 1483, Loss: 3.1013898849487305 \n",
      "              Params: tensor([  5.1193, -15.8988])\n",
      "              Grad: tensor([-0.0423,  0.2393])\n",
      "Epoch: 1484, Loss: 3.100801706314087 \n",
      "              Params: tensor([  5.1198, -15.9011])\n",
      "              Grad: tensor([-0.0422,  0.2388])\n",
      "Epoch: 1485, Loss: 3.100213050842285 \n",
      "              Params: tensor([  5.1202, -15.9035])\n",
      "              Grad: tensor([-0.0421,  0.2384])\n",
      "Epoch: 1486, Loss: 3.0996274948120117 \n",
      "              Params: tensor([  5.1206, -15.9059])\n",
      "              Grad: tensor([-0.0421,  0.2380])\n",
      "Epoch: 1487, Loss: 3.099043846130371 \n",
      "              Params: tensor([  5.1210, -15.9083])\n",
      "              Grad: tensor([-0.0420,  0.2376])\n",
      "Epoch: 1488, Loss: 3.0984625816345215 \n",
      "              Params: tensor([  5.1214, -15.9107])\n",
      "              Grad: tensor([-0.0419,  0.2372])\n",
      "Epoch: 1489, Loss: 3.097883462905884 \n",
      "              Params: tensor([  5.1219, -15.9130])\n",
      "              Grad: tensor([-0.0418,  0.2368])\n",
      "Epoch: 1490, Loss: 3.097302198410034 \n",
      "              Params: tensor([  5.1223, -15.9154])\n",
      "              Grad: tensor([-0.0418,  0.2364])\n",
      "Epoch: 1491, Loss: 3.096727132797241 \n",
      "              Params: tensor([  5.1227, -15.9178])\n",
      "              Grad: tensor([-0.0417,  0.2360])\n",
      "Epoch: 1492, Loss: 3.0961530208587646 \n",
      "              Params: tensor([  5.1231, -15.9201])\n",
      "              Grad: tensor([-0.0416,  0.2356])\n",
      "Epoch: 1493, Loss: 3.095583200454712 \n",
      "              Params: tensor([  5.1235, -15.9225])\n",
      "              Grad: tensor([-0.0416,  0.2352])\n",
      "Epoch: 1494, Loss: 3.0950112342834473 \n",
      "              Params: tensor([  5.1239, -15.9248])\n",
      "              Grad: tensor([-0.0415,  0.2348])\n",
      "Epoch: 1495, Loss: 3.0944442749023438 \n",
      "              Params: tensor([  5.1244, -15.9272])\n",
      "              Grad: tensor([-0.0414,  0.2344])\n",
      "Epoch: 1496, Loss: 3.093876600265503 \n",
      "              Params: tensor([  5.1248, -15.9295])\n",
      "              Grad: tensor([-0.0413,  0.2340])\n",
      "Epoch: 1497, Loss: 3.0933141708374023 \n",
      "              Params: tensor([  5.1252, -15.9318])\n",
      "              Grad: tensor([-0.0413,  0.2336])\n",
      "Epoch: 1498, Loss: 3.0927505493164062 \n",
      "              Params: tensor([  5.1256, -15.9342])\n",
      "              Grad: tensor([-0.0412,  0.2332])\n",
      "Epoch: 1499, Loss: 3.092191219329834 \n",
      "              Params: tensor([  5.1260, -15.9365])\n",
      "              Grad: tensor([-0.0411,  0.2328])\n",
      "Epoch: 1500, Loss: 3.091630458831787 \n",
      "              Params: tensor([  5.1264, -15.9388])\n",
      "              Grad: tensor([-0.0411,  0.2324])\n",
      "Epoch: 1501, Loss: 3.091074228286743 \n",
      "              Params: tensor([  5.1268, -15.9411])\n",
      "              Grad: tensor([-0.0410,  0.2320])\n",
      "Epoch: 1502, Loss: 3.0905203819274902 \n",
      "              Params: tensor([  5.1272, -15.9435])\n",
      "              Grad: tensor([-0.0409,  0.2317])\n",
      "Epoch: 1503, Loss: 3.089968681335449 \n",
      "              Params: tensor([  5.1276, -15.9458])\n",
      "              Grad: tensor([-0.0408,  0.2313])\n",
      "Epoch: 1504, Loss: 3.0894172191619873 \n",
      "              Params: tensor([  5.1281, -15.9481])\n",
      "              Grad: tensor([-0.0408,  0.2309])\n",
      "Epoch: 1505, Loss: 3.0888671875 \n",
      "              Params: tensor([  5.1285, -15.9504])\n",
      "              Grad: tensor([-0.0407,  0.2305])\n",
      "Epoch: 1506, Loss: 3.088320255279541 \n",
      "              Params: tensor([  5.1289, -15.9527])\n",
      "              Grad: tensor([-0.0406,  0.2301])\n",
      "Epoch: 1507, Loss: 3.0877745151519775 \n",
      "              Params: tensor([  5.1293, -15.9550])\n",
      "              Grad: tensor([-0.0406,  0.2297])\n",
      "Epoch: 1508, Loss: 3.0872323513031006 \n",
      "              Params: tensor([  5.1297, -15.9573])\n",
      "              Grad: tensor([-0.0405,  0.2293])\n",
      "Epoch: 1509, Loss: 3.0866899490356445 \n",
      "              Params: tensor([  5.1301, -15.9596])\n",
      "              Grad: tensor([-0.0404,  0.2289])\n",
      "Epoch: 1510, Loss: 3.0861496925354004 \n",
      "              Params: tensor([  5.1305, -15.9618])\n",
      "              Grad: tensor([-0.0404,  0.2285])\n",
      "Epoch: 1511, Loss: 3.0856118202209473 \n",
      "              Params: tensor([  5.1309, -15.9641])\n",
      "              Grad: tensor([-0.0403,  0.2281])\n",
      "Epoch: 1512, Loss: 3.0850746631622314 \n",
      "              Params: tensor([  5.1313, -15.9664])\n",
      "              Grad: tensor([-0.0402,  0.2277])\n",
      "Epoch: 1513, Loss: 3.0845420360565186 \n",
      "              Params: tensor([  5.1317, -15.9687])\n",
      "              Grad: tensor([-0.0402,  0.2274])\n",
      "Epoch: 1514, Loss: 3.0840089321136475 \n",
      "              Params: tensor([  5.1321, -15.9709])\n",
      "              Grad: tensor([-0.0401,  0.2270])\n",
      "Epoch: 1515, Loss: 3.0834779739379883 \n",
      "              Params: tensor([  5.1325, -15.9732])\n",
      "              Grad: tensor([-0.0400,  0.2266])\n",
      "Epoch: 1516, Loss: 3.0829484462738037 \n",
      "              Params: tensor([  5.1329, -15.9755])\n",
      "              Grad: tensor([-0.0400,  0.2262])\n",
      "Epoch: 1517, Loss: 3.0824220180511475 \n",
      "              Params: tensor([  5.1333, -15.9777])\n",
      "              Grad: tensor([-0.0399,  0.2258])\n",
      "Epoch: 1518, Loss: 3.081897497177124 \n",
      "              Params: tensor([  5.1337, -15.9800])\n",
      "              Grad: tensor([-0.0398,  0.2254])\n",
      "Epoch: 1519, Loss: 3.0813727378845215 \n",
      "              Params: tensor([  5.1341, -15.9822])\n",
      "              Grad: tensor([-0.0398,  0.2250])\n",
      "Epoch: 1520, Loss: 3.080850124359131 \n",
      "              Params: tensor([  5.1345, -15.9845])\n",
      "              Grad: tensor([-0.0397,  0.2247])\n",
      "Epoch: 1521, Loss: 3.080331325531006 \n",
      "              Params: tensor([  5.1349, -15.9867])\n",
      "              Grad: tensor([-0.0396,  0.2243])\n",
      "Epoch: 1522, Loss: 3.079810857772827 \n",
      "              Params: tensor([  5.1353, -15.9890])\n",
      "              Grad: tensor([-0.0396,  0.2239])\n",
      "Epoch: 1523, Loss: 3.079296350479126 \n",
      "              Params: tensor([  5.1357, -15.9912])\n",
      "              Grad: tensor([-0.0395,  0.2235])\n",
      "Epoch: 1524, Loss: 3.0787813663482666 \n",
      "              Params: tensor([  5.1361, -15.9934])\n",
      "              Grad: tensor([-0.0394,  0.2231])\n",
      "Epoch: 1525, Loss: 3.078267812728882 \n",
      "              Params: tensor([  5.1365, -15.9957])\n",
      "              Grad: tensor([-0.0394,  0.2228])\n",
      "Epoch: 1526, Loss: 3.0777578353881836 \n",
      "              Params: tensor([  5.1369, -15.9979])\n",
      "              Grad: tensor([-0.0393,  0.2224])\n",
      "Epoch: 1527, Loss: 3.0772476196289062 \n",
      "              Params: tensor([  5.1372, -16.0001])\n",
      "              Grad: tensor([-0.0392,  0.2220])\n",
      "Epoch: 1528, Loss: 3.0767388343811035 \n",
      "              Params: tensor([  5.1376, -16.0023])\n",
      "              Grad: tensor([-0.0391,  0.2216])\n",
      "Epoch: 1529, Loss: 3.0762319564819336 \n",
      "              Params: tensor([  5.1380, -16.0045])\n",
      "              Grad: tensor([-0.0391,  0.2213])\n",
      "Epoch: 1530, Loss: 3.07572865486145 \n",
      "              Params: tensor([  5.1384, -16.0067])\n",
      "              Grad: tensor([-0.0390,  0.2209])\n",
      "Epoch: 1531, Loss: 3.0752248764038086 \n",
      "              Params: tensor([  5.1388, -16.0089])\n",
      "              Grad: tensor([-0.0390,  0.2205])\n",
      "Epoch: 1532, Loss: 3.074723958969116 \n",
      "              Params: tensor([  5.1392, -16.0111])\n",
      "              Grad: tensor([-0.0389,  0.2201])\n",
      "Epoch: 1533, Loss: 3.0742266178131104 \n",
      "              Params: tensor([  5.1396, -16.0133])\n",
      "              Grad: tensor([-0.0388,  0.2198])\n",
      "Epoch: 1534, Loss: 3.073725938796997 \n",
      "              Params: tensor([  5.1400, -16.0155])\n",
      "              Grad: tensor([-0.0387,  0.2194])\n",
      "Epoch: 1535, Loss: 3.073232412338257 \n",
      "              Params: tensor([  5.1404, -16.0177])\n",
      "              Grad: tensor([-0.0387,  0.2190])\n",
      "Epoch: 1536, Loss: 3.0727386474609375 \n",
      "              Params: tensor([  5.1407, -16.0199])\n",
      "              Grad: tensor([-0.0386,  0.2186])\n",
      "Epoch: 1537, Loss: 3.0722451210021973 \n",
      "              Params: tensor([  5.1411, -16.0221])\n",
      "              Grad: tensor([-0.0385,  0.2183])\n",
      "Epoch: 1538, Loss: 3.0717530250549316 \n",
      "              Params: tensor([  5.1415, -16.0243])\n",
      "              Grad: tensor([-0.0385,  0.2179])\n",
      "Epoch: 1539, Loss: 3.0712647438049316 \n",
      "              Params: tensor([  5.1419, -16.0264])\n",
      "              Grad: tensor([-0.0384,  0.2175])\n",
      "Epoch: 1540, Loss: 3.070777654647827 \n",
      "              Params: tensor([  5.1423, -16.0286])\n",
      "              Grad: tensor([-0.0383,  0.2172])\n",
      "Epoch: 1541, Loss: 3.0702927112579346 \n",
      "              Params: tensor([  5.1427, -16.0308])\n",
      "              Grad: tensor([-0.0383,  0.2168])\n",
      "Epoch: 1542, Loss: 3.069808006286621 \n",
      "              Params: tensor([  5.1430, -16.0330])\n",
      "              Grad: tensor([-0.0382,  0.2164])\n",
      "Epoch: 1543, Loss: 3.0693259239196777 \n",
      "              Params: tensor([  5.1434, -16.0351])\n",
      "              Grad: tensor([-0.0382,  0.2161])\n",
      "Epoch: 1544, Loss: 3.068845272064209 \n",
      "              Params: tensor([  5.1438, -16.0373])\n",
      "              Grad: tensor([-0.0381,  0.2157])\n",
      "Epoch: 1545, Loss: 3.0683655738830566 \n",
      "              Params: tensor([  5.1442, -16.0394])\n",
      "              Grad: tensor([-0.0380,  0.2153])\n",
      "Epoch: 1546, Loss: 3.067887306213379 \n",
      "              Params: tensor([  5.1446, -16.0416])\n",
      "              Grad: tensor([-0.0380,  0.2150])\n",
      "Epoch: 1547, Loss: 3.0674118995666504 \n",
      "              Params: tensor([  5.1449, -16.0437])\n",
      "              Grad: tensor([-0.0379,  0.2146])\n",
      "Epoch: 1548, Loss: 3.066936731338501 \n",
      "              Params: tensor([  5.1453, -16.0459])\n",
      "              Grad: tensor([-0.0378,  0.2142])\n",
      "Epoch: 1549, Loss: 3.0664634704589844 \n",
      "              Params: tensor([  5.1457, -16.0480])\n",
      "              Grad: tensor([-0.0378,  0.2139])\n",
      "Epoch: 1550, Loss: 3.065992832183838 \n",
      "              Params: tensor([  5.1461, -16.0501])\n",
      "              Grad: tensor([-0.0377,  0.2135])\n",
      "Epoch: 1551, Loss: 3.0655243396759033 \n",
      "              Params: tensor([  5.1465, -16.0523])\n",
      "              Grad: tensor([-0.0376,  0.2131])\n",
      "Epoch: 1552, Loss: 3.0650551319122314 \n",
      "              Params: tensor([  5.1468, -16.0544])\n",
      "              Grad: tensor([-0.0376,  0.2128])\n",
      "Epoch: 1553, Loss: 3.0645883083343506 \n",
      "              Params: tensor([  5.1472, -16.0565])\n",
      "              Grad: tensor([-0.0375,  0.2124])\n",
      "Epoch: 1554, Loss: 3.0641226768493652 \n",
      "              Params: tensor([  5.1476, -16.0586])\n",
      "              Grad: tensor([-0.0375,  0.2120])\n",
      "Epoch: 1555, Loss: 3.06365966796875 \n",
      "              Params: tensor([  5.1480, -16.0608])\n",
      "              Grad: tensor([-0.0374,  0.2117])\n",
      "Epoch: 1556, Loss: 3.0631988048553467 \n",
      "              Params: tensor([  5.1483, -16.0629])\n",
      "              Grad: tensor([-0.0373,  0.2113])\n",
      "Epoch: 1557, Loss: 3.0627379417419434 \n",
      "              Params: tensor([  5.1487, -16.0650])\n",
      "              Grad: tensor([-0.0373,  0.2110])\n",
      "Epoch: 1558, Loss: 3.0622804164886475 \n",
      "              Params: tensor([  5.1491, -16.0671])\n",
      "              Grad: tensor([-0.0372,  0.2106])\n",
      "Epoch: 1559, Loss: 3.061821699142456 \n",
      "              Params: tensor([  5.1494, -16.0692])\n",
      "              Grad: tensor([-0.0371,  0.2103])\n",
      "Epoch: 1560, Loss: 3.0613672733306885 \n",
      "              Params: tensor([  5.1498, -16.0713])\n",
      "              Grad: tensor([-0.0371,  0.2099])\n",
      "Epoch: 1561, Loss: 3.0609130859375 \n",
      "              Params: tensor([  5.1502, -16.0734])\n",
      "              Grad: tensor([-0.0370,  0.2095])\n",
      "Epoch: 1562, Loss: 3.0604615211486816 \n",
      "              Params: tensor([  5.1506, -16.0755])\n",
      "              Grad: tensor([-0.0370,  0.2092])\n",
      "Epoch: 1563, Loss: 3.060011386871338 \n",
      "              Params: tensor([  5.1509, -16.0776])\n",
      "              Grad: tensor([-0.0369,  0.2088])\n",
      "Epoch: 1564, Loss: 3.059560537338257 \n",
      "              Params: tensor([  5.1513, -16.0796])\n",
      "              Grad: tensor([-0.0368,  0.2085])\n",
      "Epoch: 1565, Loss: 3.0591139793395996 \n",
      "              Params: tensor([  5.1517, -16.0817])\n",
      "              Grad: tensor([-0.0368,  0.2081])\n",
      "Epoch: 1566, Loss: 3.0586678981781006 \n",
      "              Params: tensor([  5.1520, -16.0838])\n",
      "              Grad: tensor([-0.0367,  0.2078])\n",
      "Epoch: 1567, Loss: 3.0582213401794434 \n",
      "              Params: tensor([  5.1524, -16.0859])\n",
      "              Grad: tensor([-0.0366,  0.2074])\n",
      "Epoch: 1568, Loss: 3.0577805042266846 \n",
      "              Params: tensor([  5.1528, -16.0880])\n",
      "              Grad: tensor([-0.0366,  0.2071])\n",
      "Epoch: 1569, Loss: 3.057337999343872 \n",
      "              Params: tensor([  5.1531, -16.0900])\n",
      "              Grad: tensor([-0.0365,  0.2067])\n",
      "Epoch: 1570, Loss: 3.0568978786468506 \n",
      "              Params: tensor([  5.1535, -16.0921])\n",
      "              Grad: tensor([-0.0364,  0.2064])\n",
      "Epoch: 1571, Loss: 3.0564582347869873 \n",
      "              Params: tensor([  5.1539, -16.0941])\n",
      "              Grad: tensor([-0.0364,  0.2060])\n",
      "Epoch: 1572, Loss: 3.0560190677642822 \n",
      "              Params: tensor([  5.1542, -16.0962])\n",
      "              Grad: tensor([-0.0363,  0.2057])\n",
      "Epoch: 1573, Loss: 3.0555853843688965 \n",
      "              Params: tensor([  5.1546, -16.0983])\n",
      "              Grad: tensor([-0.0363,  0.2053])\n",
      "Epoch: 1574, Loss: 3.0551505088806152 \n",
      "              Params: tensor([  5.1549, -16.1003])\n",
      "              Grad: tensor([-0.0362,  0.2050])\n",
      "Epoch: 1575, Loss: 3.0547170639038086 \n",
      "              Params: tensor([  5.1553, -16.1023])\n",
      "              Grad: tensor([-0.0361,  0.2046])\n",
      "Epoch: 1576, Loss: 3.054286241531372 \n",
      "              Params: tensor([  5.1557, -16.1044])\n",
      "              Grad: tensor([-0.0361,  0.2043])\n",
      "Epoch: 1577, Loss: 3.05385684967041 \n",
      "              Params: tensor([  5.1560, -16.1064])\n",
      "              Grad: tensor([-0.0360,  0.2039])\n",
      "Epoch: 1578, Loss: 3.053427219390869 \n",
      "              Params: tensor([  5.1564, -16.1085])\n",
      "              Grad: tensor([-0.0360,  0.2036])\n",
      "Epoch: 1579, Loss: 3.0530004501342773 \n",
      "              Params: tensor([  5.1567, -16.1105])\n",
      "              Grad: tensor([-0.0359,  0.2032])\n",
      "Epoch: 1580, Loss: 3.0525755882263184 \n",
      "              Params: tensor([  5.1571, -16.1125])\n",
      "              Grad: tensor([-0.0358,  0.2029])\n",
      "Epoch: 1581, Loss: 3.052151679992676 \n",
      "              Params: tensor([  5.1575, -16.1146])\n",
      "              Grad: tensor([-0.0358,  0.2025])\n",
      "Epoch: 1582, Loss: 3.051730155944824 \n",
      "              Params: tensor([  5.1578, -16.1166])\n",
      "              Grad: tensor([-0.0357,  0.2022])\n",
      "Epoch: 1583, Loss: 3.0513062477111816 \n",
      "              Params: tensor([  5.1582, -16.1186])\n",
      "              Grad: tensor([-0.0357,  0.2018])\n",
      "Epoch: 1584, Loss: 3.0508880615234375 \n",
      "              Params: tensor([  5.1585, -16.1206])\n",
      "              Grad: tensor([-0.0356,  0.2015])\n",
      "Epoch: 1585, Loss: 3.0504705905914307 \n",
      "              Params: tensor([  5.1589, -16.1226])\n",
      "              Grad: tensor([-0.0355,  0.2012])\n",
      "Epoch: 1586, Loss: 3.0500524044036865 \n",
      "              Params: tensor([  5.1592, -16.1246])\n",
      "              Grad: tensor([-0.0355,  0.2008])\n",
      "Epoch: 1587, Loss: 3.049638509750366 \n",
      "              Params: tensor([  5.1596, -16.1266])\n",
      "              Grad: tensor([-0.0354,  0.2005])\n",
      "Epoch: 1588, Loss: 3.0492234230041504 \n",
      "              Params: tensor([  5.1599, -16.1286])\n",
      "              Grad: tensor([-0.0354,  0.2001])\n",
      "Epoch: 1589, Loss: 3.0488107204437256 \n",
      "              Params: tensor([  5.1603, -16.1306])\n",
      "              Grad: tensor([-0.0353,  0.1998])\n",
      "Epoch: 1590, Loss: 3.0483977794647217 \n",
      "              Params: tensor([  5.1607, -16.1326])\n",
      "              Grad: tensor([-0.0353,  0.1995])\n",
      "Epoch: 1591, Loss: 3.0479910373687744 \n",
      "              Params: tensor([  5.1610, -16.1346])\n",
      "              Grad: tensor([-0.0352,  0.1991])\n",
      "Epoch: 1592, Loss: 3.047581434249878 \n",
      "              Params: tensor([  5.1614, -16.1366])\n",
      "              Grad: tensor([-0.0351,  0.1988])\n",
      "Epoch: 1593, Loss: 3.047173261642456 \n",
      "              Params: tensor([  5.1617, -16.1386])\n",
      "              Grad: tensor([-0.0351,  0.1984])\n",
      "Epoch: 1594, Loss: 3.0467679500579834 \n",
      "              Params: tensor([  5.1621, -16.1406])\n",
      "              Grad: tensor([-0.0350,  0.1981])\n",
      "Epoch: 1595, Loss: 3.0463624000549316 \n",
      "              Params: tensor([  5.1624, -16.1425])\n",
      "              Grad: tensor([-0.0349,  0.1978])\n",
      "Epoch: 1596, Loss: 3.045959711074829 \n",
      "              Params: tensor([  5.1628, -16.1445])\n",
      "              Grad: tensor([-0.0349,  0.1974])\n",
      "Epoch: 1597, Loss: 3.0455591678619385 \n",
      "              Params: tensor([  5.1631, -16.1465])\n",
      "              Grad: tensor([-0.0348,  0.1971])\n",
      "Epoch: 1598, Loss: 3.0451598167419434 \n",
      "              Params: tensor([  5.1635, -16.1485])\n",
      "              Grad: tensor([-0.0348,  0.1968])\n",
      "Epoch: 1599, Loss: 3.0447587966918945 \n",
      "              Params: tensor([  5.1638, -16.1504])\n",
      "              Grad: tensor([-0.0347,  0.1964])\n",
      "Epoch: 1600, Loss: 3.0443613529205322 \n",
      "              Params: tensor([  5.1641, -16.1524])\n",
      "              Grad: tensor([-0.0346,  0.1961])\n",
      "Epoch: 1601, Loss: 3.043966054916382 \n",
      "              Params: tensor([  5.1645, -16.1543])\n",
      "              Grad: tensor([-0.0346,  0.1958])\n",
      "Epoch: 1602, Loss: 3.0435714721679688 \n",
      "              Params: tensor([  5.1648, -16.1563])\n",
      "              Grad: tensor([-0.0345,  0.1954])\n",
      "Epoch: 1603, Loss: 3.0431761741638184 \n",
      "              Params: tensor([  5.1652, -16.1582])\n",
      "              Grad: tensor([-0.0345,  0.1951])\n",
      "Epoch: 1604, Loss: 3.0427849292755127 \n",
      "              Params: tensor([  5.1655, -16.1602])\n",
      "              Grad: tensor([-0.0344,  0.1948])\n",
      "Epoch: 1605, Loss: 3.0423948764801025 \n",
      "              Params: tensor([  5.1659, -16.1621])\n",
      "              Grad: tensor([-0.0343,  0.1944])\n",
      "Epoch: 1606, Loss: 3.0420045852661133 \n",
      "              Params: tensor([  5.1662, -16.1641])\n",
      "              Grad: tensor([-0.0343,  0.1941])\n",
      "Epoch: 1607, Loss: 3.0416154861450195 \n",
      "              Params: tensor([  5.1666, -16.1660])\n",
      "              Grad: tensor([-0.0342,  0.1938])\n",
      "Epoch: 1608, Loss: 3.0412299633026123 \n",
      "              Params: tensor([  5.1669, -16.1680])\n",
      "              Grad: tensor([-0.0342,  0.1934])\n",
      "Epoch: 1609, Loss: 3.040844202041626 \n",
      "              Params: tensor([  5.1672, -16.1699])\n",
      "              Grad: tensor([-0.0341,  0.1931])\n",
      "Epoch: 1610, Loss: 3.0404608249664307 \n",
      "              Params: tensor([  5.1676, -16.1718])\n",
      "              Grad: tensor([-0.0341,  0.1928])\n",
      "Epoch: 1611, Loss: 3.040076971054077 \n",
      "              Params: tensor([  5.1679, -16.1737])\n",
      "              Grad: tensor([-0.0340,  0.1925])\n",
      "Epoch: 1612, Loss: 3.0396950244903564 \n",
      "              Params: tensor([  5.1683, -16.1757])\n",
      "              Grad: tensor([-0.0339,  0.1921])\n",
      "Epoch: 1613, Loss: 3.039314031600952 \n",
      "              Params: tensor([  5.1686, -16.1776])\n",
      "              Grad: tensor([-0.0339,  0.1918])\n",
      "Epoch: 1614, Loss: 3.038933753967285 \n",
      "              Params: tensor([  5.1689, -16.1795])\n",
      "              Grad: tensor([-0.0338,  0.1915])\n",
      "Epoch: 1615, Loss: 3.0385565757751465 \n",
      "              Params: tensor([  5.1693, -16.1814])\n",
      "              Grad: tensor([-0.0338,  0.1912])\n",
      "Epoch: 1616, Loss: 3.0381813049316406 \n",
      "              Params: tensor([  5.1696, -16.1833])\n",
      "              Grad: tensor([-0.0337,  0.1908])\n",
      "Epoch: 1617, Loss: 3.0378053188323975 \n",
      "              Params: tensor([  5.1699, -16.1852])\n",
      "              Grad: tensor([-0.0337,  0.1905])\n",
      "Epoch: 1618, Loss: 3.0374319553375244 \n",
      "              Params: tensor([  5.1703, -16.1871])\n",
      "              Grad: tensor([-0.0336,  0.1902])\n",
      "Epoch: 1619, Loss: 3.0370588302612305 \n",
      "              Params: tensor([  5.1706, -16.1890])\n",
      "              Grad: tensor([-0.0335,  0.1899])\n",
      "Epoch: 1620, Loss: 3.036688804626465 \n",
      "              Params: tensor([  5.1710, -16.1909])\n",
      "              Grad: tensor([-0.0335,  0.1895])\n",
      "Epoch: 1621, Loss: 3.036318778991699 \n",
      "              Params: tensor([  5.1713, -16.1928])\n",
      "              Grad: tensor([-0.0334,  0.1892])\n",
      "Epoch: 1622, Loss: 3.0359489917755127 \n",
      "              Params: tensor([  5.1716, -16.1947])\n",
      "              Grad: tensor([-0.0334,  0.1889])\n",
      "Epoch: 1623, Loss: 3.0355825424194336 \n",
      "              Params: tensor([  5.1720, -16.1966])\n",
      "              Grad: tensor([-0.0333,  0.1886])\n",
      "Epoch: 1624, Loss: 3.0352156162261963 \n",
      "              Params: tensor([  5.1723, -16.1985])\n",
      "              Grad: tensor([-0.0333,  0.1883])\n",
      "Epoch: 1625, Loss: 3.0348494052886963 \n",
      "              Params: tensor([  5.1726, -16.2003])\n",
      "              Grad: tensor([-0.0332,  0.1879])\n",
      "Epoch: 1626, Loss: 3.03448486328125 \n",
      "              Params: tensor([  5.1729, -16.2022])\n",
      "              Grad: tensor([-0.0331,  0.1876])\n",
      "Epoch: 1627, Loss: 3.0341227054595947 \n",
      "              Params: tensor([  5.1733, -16.2041])\n",
      "              Grad: tensor([-0.0331,  0.1873])\n",
      "Epoch: 1628, Loss: 3.033762216567993 \n",
      "              Params: tensor([  5.1736, -16.2060])\n",
      "              Grad: tensor([-0.0330,  0.1870])\n",
      "Epoch: 1629, Loss: 3.0334017276763916 \n",
      "              Params: tensor([  5.1739, -16.2078])\n",
      "              Grad: tensor([-0.0330,  0.1867])\n",
      "Epoch: 1630, Loss: 3.033041477203369 \n",
      "              Params: tensor([  5.1743, -16.2097])\n",
      "              Grad: tensor([-0.0329,  0.1863])\n",
      "Epoch: 1631, Loss: 3.0326850414276123 \n",
      "              Params: tensor([  5.1746, -16.2116])\n",
      "              Grad: tensor([-0.0329,  0.1860])\n",
      "Epoch: 1632, Loss: 3.0323286056518555 \n",
      "              Params: tensor([  5.1749, -16.2134])\n",
      "              Grad: tensor([-0.0328,  0.1857])\n",
      "Epoch: 1633, Loss: 3.031973361968994 \n",
      "              Params: tensor([  5.1753, -16.2153])\n",
      "              Grad: tensor([-0.0327,  0.1854])\n",
      "Epoch: 1634, Loss: 3.0316193103790283 \n",
      "              Params: tensor([  5.1756, -16.2171])\n",
      "              Grad: tensor([-0.0327,  0.1851])\n",
      "Epoch: 1635, Loss: 3.0312652587890625 \n",
      "              Params: tensor([  5.1759, -16.2190])\n",
      "              Grad: tensor([-0.0326,  0.1848])\n",
      "Epoch: 1636, Loss: 3.0309133529663086 \n",
      "              Params: tensor([  5.1762, -16.2208])\n",
      "              Grad: tensor([-0.0326,  0.1845])\n",
      "Epoch: 1637, Loss: 3.030564308166504 \n",
      "              Params: tensor([  5.1766, -16.2226])\n",
      "              Grad: tensor([-0.0325,  0.1841])\n",
      "Epoch: 1638, Loss: 3.030214548110962 \n",
      "              Params: tensor([  5.1769, -16.2245])\n",
      "              Grad: tensor([-0.0325,  0.1838])\n",
      "Epoch: 1639, Loss: 3.0298666954040527 \n",
      "              Params: tensor([  5.1772, -16.2263])\n",
      "              Grad: tensor([-0.0324,  0.1835])\n",
      "Epoch: 1640, Loss: 3.0295183658599854 \n",
      "              Params: tensor([  5.1775, -16.2282])\n",
      "              Grad: tensor([-0.0324,  0.1832])\n",
      "Epoch: 1641, Loss: 3.029172658920288 \n",
      "              Params: tensor([  5.1779, -16.2300])\n",
      "              Grad: tensor([-0.0323,  0.1829])\n",
      "Epoch: 1642, Loss: 3.0288283824920654 \n",
      "              Params: tensor([  5.1782, -16.2318])\n",
      "              Grad: tensor([-0.0323,  0.1826])\n",
      "Epoch: 1643, Loss: 3.0284860134124756 \n",
      "              Params: tensor([  5.1785, -16.2336])\n",
      "              Grad: tensor([-0.0322,  0.1823])\n",
      "Epoch: 1644, Loss: 3.028141975402832 \n",
      "              Params: tensor([  5.1788, -16.2355])\n",
      "              Grad: tensor([-0.0321,  0.1820])\n",
      "Epoch: 1645, Loss: 3.027801752090454 \n",
      "              Params: tensor([  5.1791, -16.2373])\n",
      "              Grad: tensor([-0.0321,  0.1817])\n",
      "Epoch: 1646, Loss: 3.0274627208709717 \n",
      "              Params: tensor([  5.1795, -16.2391])\n",
      "              Grad: tensor([-0.0320,  0.1813])\n",
      "Epoch: 1647, Loss: 3.0271222591400146 \n",
      "              Params: tensor([  5.1798, -16.2409])\n",
      "              Grad: tensor([-0.0320,  0.1810])\n",
      "Epoch: 1648, Loss: 3.0267839431762695 \n",
      "              Params: tensor([  5.1801, -16.2427])\n",
      "              Grad: tensor([-0.0319,  0.1807])\n",
      "Epoch: 1649, Loss: 3.026447057723999 \n",
      "              Params: tensor([  5.1804, -16.2445])\n",
      "              Grad: tensor([-0.0319,  0.1804])\n",
      "Epoch: 1650, Loss: 3.026111364364624 \n",
      "              Params: tensor([  5.1807, -16.2463])\n",
      "              Grad: tensor([-0.0318,  0.1801])\n",
      "Epoch: 1651, Loss: 3.0257797241210938 \n",
      "              Params: tensor([  5.1811, -16.2481])\n",
      "              Grad: tensor([-0.0318,  0.1798])\n",
      "Epoch: 1652, Loss: 3.025446891784668 \n",
      "              Params: tensor([  5.1814, -16.2499])\n",
      "              Grad: tensor([-0.0317,  0.1795])\n",
      "Epoch: 1653, Loss: 3.025113821029663 \n",
      "              Params: tensor([  5.1817, -16.2517])\n",
      "              Grad: tensor([-0.0317,  0.1792])\n",
      "Epoch: 1654, Loss: 3.0247817039489746 \n",
      "              Params: tensor([  5.1820, -16.2535])\n",
      "              Grad: tensor([-0.0316,  0.1789])\n",
      "Epoch: 1655, Loss: 3.0244522094726562 \n",
      "              Params: tensor([  5.1823, -16.2553])\n",
      "              Grad: tensor([-0.0316,  0.1786])\n",
      "Epoch: 1656, Loss: 3.02412486076355 \n",
      "              Params: tensor([  5.1826, -16.2570])\n",
      "              Grad: tensor([-0.0315,  0.1783])\n",
      "Epoch: 1657, Loss: 3.023796319961548 \n",
      "              Params: tensor([  5.1829, -16.2588])\n",
      "              Grad: tensor([-0.0315,  0.1780])\n",
      "Epoch: 1658, Loss: 3.0234711170196533 \n",
      "              Params: tensor([  5.1833, -16.2606])\n",
      "              Grad: tensor([-0.0314,  0.1777])\n",
      "Epoch: 1659, Loss: 3.0231454372406006 \n",
      "              Params: tensor([  5.1836, -16.2624])\n",
      "              Grad: tensor([-0.0313,  0.1774])\n",
      "Epoch: 1660, Loss: 3.022820472717285 \n",
      "              Params: tensor([  5.1839, -16.2641])\n",
      "              Grad: tensor([-0.0313,  0.1771])\n",
      "Epoch: 1661, Loss: 3.022498369216919 \n",
      "              Params: tensor([  5.1842, -16.2659])\n",
      "              Grad: tensor([-0.0312,  0.1768])\n",
      "Epoch: 1662, Loss: 3.022176504135132 \n",
      "              Params: tensor([  5.1845, -16.2677])\n",
      "              Grad: tensor([-0.0312,  0.1765])\n",
      "Epoch: 1663, Loss: 3.0218546390533447 \n",
      "              Params: tensor([  5.1848, -16.2694])\n",
      "              Grad: tensor([-0.0311,  0.1762])\n",
      "Epoch: 1664, Loss: 3.021533966064453 \n",
      "              Params: tensor([  5.1851, -16.2712])\n",
      "              Grad: tensor([-0.0311,  0.1759])\n",
      "Epoch: 1665, Loss: 3.021216630935669 \n",
      "              Params: tensor([  5.1854, -16.2730])\n",
      "              Grad: tensor([-0.0310,  0.1756])\n",
      "Epoch: 1666, Loss: 3.0208983421325684 \n",
      "              Params: tensor([  5.1858, -16.2747])\n",
      "              Grad: tensor([-0.0310,  0.1753])\n",
      "Epoch: 1667, Loss: 3.020582437515259 \n",
      "              Params: tensor([  5.1861, -16.2765])\n",
      "              Grad: tensor([-0.0309,  0.1750])\n",
      "Epoch: 1668, Loss: 3.0202653408050537 \n",
      "              Params: tensor([  5.1864, -16.2782])\n",
      "              Grad: tensor([-0.0309,  0.1747])\n",
      "Epoch: 1669, Loss: 3.019951820373535 \n",
      "              Params: tensor([  5.1867, -16.2800])\n",
      "              Grad: tensor([-0.0308,  0.1744])\n",
      "Epoch: 1670, Loss: 3.019639492034912 \n",
      "              Params: tensor([  5.1870, -16.2817])\n",
      "              Grad: tensor([-0.0308,  0.1741])\n",
      "Epoch: 1671, Loss: 3.0193254947662354 \n",
      "              Params: tensor([  5.1873, -16.2834])\n",
      "              Grad: tensor([-0.0307,  0.1738])\n",
      "Epoch: 1672, Loss: 3.0190162658691406 \n",
      "              Params: tensor([  5.1876, -16.2852])\n",
      "              Grad: tensor([-0.0307,  0.1735])\n",
      "Epoch: 1673, Loss: 3.0187063217163086 \n",
      "              Params: tensor([  5.1879, -16.2869])\n",
      "              Grad: tensor([-0.0306,  0.1732])\n",
      "Epoch: 1674, Loss: 3.018394708633423 \n",
      "              Params: tensor([  5.1882, -16.2886])\n",
      "              Grad: tensor([-0.0305,  0.1729])\n",
      "Epoch: 1675, Loss: 3.0180890560150146 \n",
      "              Params: tensor([  5.1885, -16.2904])\n",
      "              Grad: tensor([-0.0305,  0.1726])\n",
      "Epoch: 1676, Loss: 3.017779588699341 \n",
      "              Params: tensor([  5.1888, -16.2921])\n",
      "              Grad: tensor([-0.0304,  0.1723])\n",
      "Epoch: 1677, Loss: 3.017474889755249 \n",
      "              Params: tensor([  5.1891, -16.2938])\n",
      "              Grad: tensor([-0.0304,  0.1720])\n",
      "Epoch: 1678, Loss: 3.017169713973999 \n",
      "              Params: tensor([  5.1894, -16.2955])\n",
      "              Grad: tensor([-0.0303,  0.1717])\n",
      "Epoch: 1679, Loss: 3.016867160797119 \n",
      "              Params: tensor([  5.1897, -16.2972])\n",
      "              Grad: tensor([-0.0303,  0.1715])\n",
      "Epoch: 1680, Loss: 3.016563653945923 \n",
      "              Params: tensor([  5.1900, -16.2989])\n",
      "              Grad: tensor([-0.0302,  0.1712])\n",
      "Epoch: 1681, Loss: 3.0162618160247803 \n",
      "              Params: tensor([  5.1903, -16.3006])\n",
      "              Grad: tensor([-0.0302,  0.1709])\n",
      "Epoch: 1682, Loss: 3.0159590244293213 \n",
      "              Params: tensor([  5.1906, -16.3024])\n",
      "              Grad: tensor([-0.0301,  0.1706])\n",
      "Epoch: 1683, Loss: 3.0156617164611816 \n",
      "              Params: tensor([  5.1909, -16.3041])\n",
      "              Grad: tensor([-0.0301,  0.1703])\n",
      "Epoch: 1684, Loss: 3.0153610706329346 \n",
      "              Params: tensor([  5.1912, -16.3058])\n",
      "              Grad: tensor([-0.0300,  0.1700])\n",
      "Epoch: 1685, Loss: 3.015064001083374 \n",
      "              Params: tensor([  5.1915, -16.3075])\n",
      "              Grad: tensor([-0.0300,  0.1697])\n",
      "Epoch: 1686, Loss: 3.014767646789551 \n",
      "              Params: tensor([  5.1918, -16.3091])\n",
      "              Grad: tensor([-0.0299,  0.1694])\n",
      "Epoch: 1687, Loss: 3.014472246170044 \n",
      "              Params: tensor([  5.1921, -16.3108])\n",
      "              Grad: tensor([-0.0299,  0.1691])\n",
      "Epoch: 1688, Loss: 3.014178991317749 \n",
      "              Params: tensor([  5.1924, -16.3125])\n",
      "              Grad: tensor([-0.0298,  0.1688])\n",
      "Epoch: 1689, Loss: 3.0138838291168213 \n",
      "              Params: tensor([  5.1927, -16.3142])\n",
      "              Grad: tensor([-0.0298,  0.1686])\n",
      "Epoch: 1690, Loss: 3.0135910511016846 \n",
      "              Params: tensor([  5.1930, -16.3159])\n",
      "              Grad: tensor([-0.0297,  0.1683])\n",
      "Epoch: 1691, Loss: 3.0132994651794434 \n",
      "              Params: tensor([  5.1933, -16.3176])\n",
      "              Grad: tensor([-0.0297,  0.1680])\n",
      "Epoch: 1692, Loss: 3.013007879257202 \n",
      "              Params: tensor([  5.1936, -16.3193])\n",
      "              Grad: tensor([-0.0296,  0.1677])\n",
      "Epoch: 1693, Loss: 3.012718915939331 \n",
      "              Params: tensor([  5.1939, -16.3209])\n",
      "              Grad: tensor([-0.0296,  0.1674])\n",
      "Epoch: 1694, Loss: 3.0124306678771973 \n",
      "              Params: tensor([  5.1942, -16.3226])\n",
      "              Grad: tensor([-0.0295,  0.1671])\n",
      "Epoch: 1695, Loss: 3.0121407508850098 \n",
      "              Params: tensor([  5.1945, -16.3243])\n",
      "              Grad: tensor([-0.0295,  0.1668])\n",
      "Epoch: 1696, Loss: 3.011855125427246 \n",
      "              Params: tensor([  5.1948, -16.3259])\n",
      "              Grad: tensor([-0.0294,  0.1666])\n",
      "Epoch: 1697, Loss: 3.0115699768066406 \n",
      "              Params: tensor([  5.1951, -16.3276])\n",
      "              Grad: tensor([-0.0294,  0.1663])\n",
      "Epoch: 1698, Loss: 3.0112838745117188 \n",
      "              Params: tensor([  5.1954, -16.3293])\n",
      "              Grad: tensor([-0.0293,  0.1660])\n",
      "Epoch: 1699, Loss: 3.0110013484954834 \n",
      "              Params: tensor([  5.1957, -16.3309])\n",
      "              Grad: tensor([-0.0293,  0.1657])\n",
      "Epoch: 1700, Loss: 3.01071834564209 \n",
      "              Params: tensor([  5.1960, -16.3326])\n",
      "              Grad: tensor([-0.0292,  0.1654])\n",
      "Epoch: 1701, Loss: 3.0104362964630127 \n",
      "              Params: tensor([  5.1963, -16.3342])\n",
      "              Grad: tensor([-0.0292,  0.1652])\n",
      "Epoch: 1702, Loss: 3.01015567779541 \n",
      "              Params: tensor([  5.1966, -16.3359])\n",
      "              Grad: tensor([-0.0291,  0.1649])\n",
      "Epoch: 1703, Loss: 3.009875535964966 \n",
      "              Params: tensor([  5.1968, -16.3375])\n",
      "              Grad: tensor([-0.0291,  0.1646])\n",
      "Epoch: 1704, Loss: 3.0095953941345215 \n",
      "              Params: tensor([  5.1971, -16.3392])\n",
      "              Grad: tensor([-0.0290,  0.1643])\n",
      "Epoch: 1705, Loss: 3.0093190670013428 \n",
      "              Params: tensor([  5.1974, -16.3408])\n",
      "              Grad: tensor([-0.0290,  0.1640])\n",
      "Epoch: 1706, Loss: 3.009039878845215 \n",
      "              Params: tensor([  5.1977, -16.3424])\n",
      "              Grad: tensor([-0.0289,  0.1638])\n",
      "Epoch: 1707, Loss: 3.008763313293457 \n",
      "              Params: tensor([  5.1980, -16.3441])\n",
      "              Grad: tensor([-0.0289,  0.1635])\n",
      "Epoch: 1708, Loss: 3.0084874629974365 \n",
      "              Params: tensor([  5.1983, -16.3457])\n",
      "              Grad: tensor([-0.0288,  0.1632])\n",
      "Epoch: 1709, Loss: 3.0082147121429443 \n",
      "              Params: tensor([  5.1986, -16.3473])\n",
      "              Grad: tensor([-0.0288,  0.1629])\n",
      "Epoch: 1710, Loss: 3.0079407691955566 \n",
      "              Params: tensor([  5.1989, -16.3490])\n",
      "              Grad: tensor([-0.0287,  0.1626])\n",
      "Epoch: 1711, Loss: 3.0076675415039062 \n",
      "              Params: tensor([  5.1992, -16.3506])\n",
      "              Grad: tensor([-0.0287,  0.1624])\n",
      "Epoch: 1712, Loss: 3.007396697998047 \n",
      "              Params: tensor([  5.1994, -16.3522])\n",
      "              Grad: tensor([-0.0286,  0.1621])\n",
      "Epoch: 1713, Loss: 3.0071260929107666 \n",
      "              Params: tensor([  5.1997, -16.3538])\n",
      "              Grad: tensor([-0.0286,  0.1618])\n",
      "Epoch: 1714, Loss: 3.006856679916382 \n",
      "              Params: tensor([  5.2000, -16.3554])\n",
      "              Grad: tensor([-0.0285,  0.1615])\n",
      "Epoch: 1715, Loss: 3.0065863132476807 \n",
      "              Params: tensor([  5.2003, -16.3570])\n",
      "              Grad: tensor([-0.0285,  0.1613])\n",
      "Epoch: 1716, Loss: 3.0063180923461914 \n",
      "              Params: tensor([  5.2006, -16.3587])\n",
      "              Grad: tensor([-0.0284,  0.1610])\n",
      "Epoch: 1717, Loss: 3.006052255630493 \n",
      "              Params: tensor([  5.2009, -16.3603])\n",
      "              Grad: tensor([-0.0284,  0.1607])\n",
      "Epoch: 1718, Loss: 3.0057852268218994 \n",
      "              Params: tensor([  5.2012, -16.3619])\n",
      "              Grad: tensor([-0.0284,  0.1604])\n",
      "Epoch: 1719, Loss: 3.0055205821990967 \n",
      "              Params: tensor([  5.2014, -16.3635])\n",
      "              Grad: tensor([-0.0283,  0.1602])\n",
      "Epoch: 1720, Loss: 3.005256414413452 \n",
      "              Params: tensor([  5.2017, -16.3651])\n",
      "              Grad: tensor([-0.0283,  0.1599])\n",
      "Epoch: 1721, Loss: 3.004993200302124 \n",
      "              Params: tensor([  5.2020, -16.3667])\n",
      "              Grad: tensor([-0.0282,  0.1596])\n",
      "Epoch: 1722, Loss: 3.0047292709350586 \n",
      "              Params: tensor([  5.2023, -16.3683])\n",
      "              Grad: tensor([-0.0281,  0.1594])\n",
      "Epoch: 1723, Loss: 3.0044667720794678 \n",
      "              Params: tensor([  5.2026, -16.3699])\n",
      "              Grad: tensor([-0.0281,  0.1591])\n",
      "Epoch: 1724, Loss: 3.004206895828247 \n",
      "              Params: tensor([  5.2028, -16.3714])\n",
      "              Grad: tensor([-0.0280,  0.1588])\n",
      "Epoch: 1725, Loss: 3.0039467811584473 \n",
      "              Params: tensor([  5.2031, -16.3730])\n",
      "              Grad: tensor([-0.0280,  0.1586])\n",
      "Epoch: 1726, Loss: 3.003689765930176 \n",
      "              Params: tensor([  5.2034, -16.3746])\n",
      "              Grad: tensor([-0.0280,  0.1583])\n",
      "Epoch: 1727, Loss: 3.0034308433532715 \n",
      "              Params: tensor([  5.2037, -16.3762])\n",
      "              Grad: tensor([-0.0279,  0.1580])\n",
      "Epoch: 1728, Loss: 3.003173589706421 \n",
      "              Params: tensor([  5.2040, -16.3778])\n",
      "              Grad: tensor([-0.0279,  0.1577])\n",
      "Epoch: 1729, Loss: 3.002917528152466 \n",
      "              Params: tensor([  5.2042, -16.3793])\n",
      "              Grad: tensor([-0.0278,  0.1575])\n",
      "Epoch: 1730, Loss: 3.0026609897613525 \n",
      "              Params: tensor([  5.2045, -16.3809])\n",
      "              Grad: tensor([-0.0278,  0.1572])\n",
      "Epoch: 1731, Loss: 3.002406120300293 \n",
      "              Params: tensor([  5.2048, -16.3825])\n",
      "              Grad: tensor([-0.0277,  0.1569])\n",
      "Epoch: 1732, Loss: 3.00215220451355 \n",
      "              Params: tensor([  5.2051, -16.3840])\n",
      "              Grad: tensor([-0.0277,  0.1567])\n",
      "Epoch: 1733, Loss: 3.001901149749756 \n",
      "              Params: tensor([  5.2053, -16.3856])\n",
      "              Grad: tensor([-0.0276,  0.1564])\n",
      "Epoch: 1734, Loss: 3.0016493797302246 \n",
      "              Params: tensor([  5.2056, -16.3872])\n",
      "              Grad: tensor([-0.0276,  0.1561])\n",
      "Epoch: 1735, Loss: 3.0013954639434814 \n",
      "              Params: tensor([  5.2059, -16.3887])\n",
      "              Grad: tensor([-0.0275,  0.1559])\n",
      "Epoch: 1736, Loss: 3.0011446475982666 \n",
      "              Params: tensor([  5.2062, -16.3903])\n",
      "              Grad: tensor([-0.0275,  0.1556])\n",
      "Epoch: 1737, Loss: 3.0008978843688965 \n",
      "              Params: tensor([  5.2064, -16.3918])\n",
      "              Grad: tensor([-0.0274,  0.1553])\n",
      "Epoch: 1738, Loss: 3.000647783279419 \n",
      "              Params: tensor([  5.2067, -16.3934])\n",
      "              Grad: tensor([-0.0274,  0.1551])\n",
      "Epoch: 1739, Loss: 3.0004003047943115 \n",
      "              Params: tensor([  5.2070, -16.3949])\n",
      "              Grad: tensor([-0.0273,  0.1548])\n",
      "Epoch: 1740, Loss: 3.0001542568206787 \n",
      "              Params: tensor([  5.2073, -16.3965])\n",
      "              Grad: tensor([-0.0273,  0.1546])\n",
      "Epoch: 1741, Loss: 2.9999074935913086 \n",
      "              Params: tensor([  5.2075, -16.3980])\n",
      "              Grad: tensor([-0.0273,  0.1543])\n",
      "Epoch: 1742, Loss: 2.999662160873413 \n",
      "              Params: tensor([  5.2078, -16.3996])\n",
      "              Grad: tensor([-0.0272,  0.1540])\n",
      "Epoch: 1743, Loss: 2.999417304992676 \n",
      "              Params: tensor([  5.2081, -16.4011])\n",
      "              Grad: tensor([-0.0272,  0.1538])\n",
      "Epoch: 1744, Loss: 2.999173641204834 \n",
      "              Params: tensor([  5.2084, -16.4026])\n",
      "              Grad: tensor([-0.0271,  0.1535])\n",
      "Epoch: 1745, Loss: 2.998929738998413 \n",
      "              Params: tensor([  5.2086, -16.4042])\n",
      "              Grad: tensor([-0.0271,  0.1533])\n",
      "Epoch: 1746, Loss: 2.998687744140625 \n",
      "              Params: tensor([  5.2089, -16.4057])\n",
      "              Grad: tensor([-0.0270,  0.1530])\n",
      "Epoch: 1747, Loss: 2.998448133468628 \n",
      "              Params: tensor([  5.2092, -16.4072])\n",
      "              Grad: tensor([-0.0270,  0.1527])\n",
      "Epoch: 1748, Loss: 2.9982080459594727 \n",
      "              Params: tensor([  5.2094, -16.4088])\n",
      "              Grad: tensor([-0.0269,  0.1525])\n",
      "Epoch: 1749, Loss: 2.9979681968688965 \n",
      "              Params: tensor([  5.2097, -16.4103])\n",
      "              Grad: tensor([-0.0269,  0.1522])\n",
      "Epoch: 1750, Loss: 2.997730016708374 \n",
      "              Params: tensor([  5.2100, -16.4118])\n",
      "              Grad: tensor([-0.0268,  0.1520])\n",
      "Epoch: 1751, Loss: 2.997490167617798 \n",
      "              Params: tensor([  5.2102, -16.4133])\n",
      "              Grad: tensor([-0.0268,  0.1517])\n",
      "Epoch: 1752, Loss: 2.997253656387329 \n",
      "              Params: tensor([  5.2105, -16.4148])\n",
      "              Grad: tensor([-0.0267,  0.1514])\n",
      "Epoch: 1753, Loss: 2.9970178604125977 \n",
      "              Params: tensor([  5.2108, -16.4163])\n",
      "              Grad: tensor([-0.0267,  0.1512])\n",
      "Epoch: 1754, Loss: 2.996782064437866 \n",
      "              Params: tensor([  5.2110, -16.4179])\n",
      "              Grad: tensor([-0.0266,  0.1509])\n",
      "Epoch: 1755, Loss: 2.9965476989746094 \n",
      "              Params: tensor([  5.2113, -16.4194])\n",
      "              Grad: tensor([-0.0266,  0.1507])\n",
      "Epoch: 1756, Loss: 2.9963133335113525 \n",
      "              Params: tensor([  5.2116, -16.4209])\n",
      "              Grad: tensor([-0.0266,  0.1504])\n",
      "Epoch: 1757, Loss: 2.9960806369781494 \n",
      "              Params: tensor([  5.2118, -16.4224])\n",
      "              Grad: tensor([-0.0265,  0.1502])\n",
      "Epoch: 1758, Loss: 2.995847463607788 \n",
      "              Params: tensor([  5.2121, -16.4239])\n",
      "              Grad: tensor([-0.0265,  0.1499])\n",
      "Epoch: 1759, Loss: 2.9956154823303223 \n",
      "              Params: tensor([  5.2124, -16.4254])\n",
      "              Grad: tensor([-0.0264,  0.1496])\n",
      "Epoch: 1760, Loss: 2.9953866004943848 \n",
      "              Params: tensor([  5.2126, -16.4269])\n",
      "              Grad: tensor([-0.0264,  0.1494])\n",
      "Epoch: 1761, Loss: 2.9951562881469727 \n",
      "              Params: tensor([  5.2129, -16.4283])\n",
      "              Grad: tensor([-0.0263,  0.1491])\n",
      "Epoch: 1762, Loss: 2.9949281215667725 \n",
      "              Params: tensor([  5.2132, -16.4298])\n",
      "              Grad: tensor([-0.0263,  0.1489])\n",
      "Epoch: 1763, Loss: 2.994699239730835 \n",
      "              Params: tensor([  5.2134, -16.4313])\n",
      "              Grad: tensor([-0.0263,  0.1486])\n",
      "Epoch: 1764, Loss: 2.9944710731506348 \n",
      "              Params: tensor([  5.2137, -16.4328])\n",
      "              Grad: tensor([-0.0262,  0.1484])\n",
      "Epoch: 1765, Loss: 2.9942452907562256 \n",
      "              Params: tensor([  5.2139, -16.4343])\n",
      "              Grad: tensor([-0.0262,  0.1481])\n",
      "Epoch: 1766, Loss: 2.9940185546875 \n",
      "              Params: tensor([  5.2142, -16.4358])\n",
      "              Grad: tensor([-0.0261,  0.1479])\n",
      "Epoch: 1767, Loss: 2.9937944412231445 \n",
      "              Params: tensor([  5.2145, -16.4372])\n",
      "              Grad: tensor([-0.0261,  0.1476])\n",
      "Epoch: 1768, Loss: 2.9935693740844727 \n",
      "              Params: tensor([  5.2147, -16.4387])\n",
      "              Grad: tensor([-0.0260,  0.1474])\n",
      "Epoch: 1769, Loss: 2.993344306945801 \n",
      "              Params: tensor([  5.2150, -16.4402])\n",
      "              Grad: tensor([-0.0260,  0.1471])\n",
      "Epoch: 1770, Loss: 2.9931211471557617 \n",
      "              Params: tensor([  5.2152, -16.4417])\n",
      "              Grad: tensor([-0.0260,  0.1469])\n",
      "Epoch: 1771, Loss: 2.9929001331329346 \n",
      "              Params: tensor([  5.2155, -16.4431])\n",
      "              Grad: tensor([-0.0259,  0.1466])\n",
      "Epoch: 1772, Loss: 2.992677927017212 \n",
      "              Params: tensor([  5.2158, -16.4446])\n",
      "              Grad: tensor([-0.0259,  0.1464])\n",
      "Epoch: 1773, Loss: 2.992457389831543 \n",
      "              Params: tensor([  5.2160, -16.4460])\n",
      "              Grad: tensor([-0.0258,  0.1461])\n",
      "Epoch: 1774, Loss: 2.9922373294830322 \n",
      "              Params: tensor([  5.2163, -16.4475])\n",
      "              Grad: tensor([-0.0258,  0.1459])\n",
      "Epoch: 1775, Loss: 2.9920167922973633 \n",
      "              Params: tensor([  5.2165, -16.4490])\n",
      "              Grad: tensor([-0.0257,  0.1456])\n",
      "Epoch: 1776, Loss: 2.991797685623169 \n",
      "              Params: tensor([  5.2168, -16.4504])\n",
      "              Grad: tensor([-0.0257,  0.1454])\n",
      "Epoch: 1777, Loss: 2.9915823936462402 \n",
      "              Params: tensor([  5.2170, -16.4519])\n",
      "              Grad: tensor([-0.0256,  0.1451])\n",
      "Epoch: 1778, Loss: 2.991365671157837 \n",
      "              Params: tensor([  5.2173, -16.4533])\n",
      "              Grad: tensor([-0.0256,  0.1449])\n",
      "Epoch: 1779, Loss: 2.9911460876464844 \n",
      "              Params: tensor([  5.2176, -16.4548])\n",
      "              Grad: tensor([-0.0256,  0.1446])\n",
      "Epoch: 1780, Loss: 2.990931510925293 \n",
      "              Params: tensor([  5.2178, -16.4562])\n",
      "              Grad: tensor([-0.0255,  0.1444])\n",
      "Epoch: 1781, Loss: 2.9907188415527344 \n",
      "              Params: tensor([  5.2181, -16.4576])\n",
      "              Grad: tensor([-0.0255,  0.1442])\n",
      "Epoch: 1782, Loss: 2.9905028343200684 \n",
      "              Params: tensor([  5.2183, -16.4591])\n",
      "              Grad: tensor([-0.0254,  0.1439])\n",
      "Epoch: 1783, Loss: 2.990288496017456 \n",
      "              Params: tensor([  5.2186, -16.4605])\n",
      "              Grad: tensor([-0.0254,  0.1437])\n",
      "Epoch: 1784, Loss: 2.9900782108306885 \n",
      "              Params: tensor([  5.2188, -16.4620])\n",
      "              Grad: tensor([-0.0253,  0.1434])\n",
      "Epoch: 1785, Loss: 2.989866018295288 \n",
      "              Params: tensor([  5.2191, -16.4634])\n",
      "              Grad: tensor([-0.0253,  0.1432])\n",
      "Epoch: 1786, Loss: 2.989654541015625 \n",
      "              Params: tensor([  5.2193, -16.4648])\n",
      "              Grad: tensor([-0.0252,  0.1429])\n",
      "Epoch: 1787, Loss: 2.989443302154541 \n",
      "              Params: tensor([  5.2196, -16.4662])\n",
      "              Grad: tensor([-0.0252,  0.1427])\n",
      "Epoch: 1788, Loss: 2.9892327785491943 \n",
      "              Params: tensor([  5.2198, -16.4677])\n",
      "              Grad: tensor([-0.0252,  0.1424])\n",
      "Epoch: 1789, Loss: 2.9890248775482178 \n",
      "              Params: tensor([  5.2201, -16.4691])\n",
      "              Grad: tensor([-0.0251,  0.1422])\n",
      "Epoch: 1790, Loss: 2.988816976547241 \n",
      "              Params: tensor([  5.2203, -16.4705])\n",
      "              Grad: tensor([-0.0251,  0.1420])\n",
      "Epoch: 1791, Loss: 2.9886085987091064 \n",
      "              Params: tensor([  5.2206, -16.4719])\n",
      "              Grad: tensor([-0.0250,  0.1417])\n",
      "Epoch: 1792, Loss: 2.988401174545288 \n",
      "              Params: tensor([  5.2208, -16.4733])\n",
      "              Grad: tensor([-0.0250,  0.1415])\n",
      "Epoch: 1793, Loss: 2.9881949424743652 \n",
      "              Params: tensor([  5.2211, -16.4748])\n",
      "              Grad: tensor([-0.0249,  0.1412])\n",
      "Epoch: 1794, Loss: 2.9879889488220215 \n",
      "              Params: tensor([  5.2213, -16.4762])\n",
      "              Grad: tensor([-0.0249,  0.1410])\n",
      "Epoch: 1795, Loss: 2.9877851009368896 \n",
      "              Params: tensor([  5.2216, -16.4776])\n",
      "              Grad: tensor([-0.0249,  0.1408])\n",
      "Epoch: 1796, Loss: 2.987581729888916 \n",
      "              Params: tensor([  5.2218, -16.4790])\n",
      "              Grad: tensor([-0.0248,  0.1405])\n",
      "Epoch: 1797, Loss: 2.987377166748047 \n",
      "              Params: tensor([  5.2221, -16.4804])\n",
      "              Grad: tensor([-0.0248,  0.1403])\n",
      "Epoch: 1798, Loss: 2.987173557281494 \n",
      "              Params: tensor([  5.2223, -16.4818])\n",
      "              Grad: tensor([-0.0247,  0.1400])\n",
      "Epoch: 1799, Loss: 2.986973524093628 \n",
      "              Params: tensor([  5.2226, -16.4832])\n",
      "              Grad: tensor([-0.0247,  0.1398])\n",
      "Epoch: 1800, Loss: 2.98677134513855 \n",
      "              Params: tensor([  5.2228, -16.4846])\n",
      "              Grad: tensor([-0.0246,  0.1396])\n",
      "Epoch: 1801, Loss: 2.986570119857788 \n",
      "              Params: tensor([  5.2231, -16.4860])\n",
      "              Grad: tensor([-0.0246,  0.1393])\n",
      "Epoch: 1802, Loss: 2.9863710403442383 \n",
      "              Params: tensor([  5.2233, -16.4874])\n",
      "              Grad: tensor([-0.0246,  0.1391])\n",
      "Epoch: 1803, Loss: 2.986170768737793 \n",
      "              Params: tensor([  5.2236, -16.4888])\n",
      "              Grad: tensor([-0.0245,  0.1389])\n",
      "Epoch: 1804, Loss: 2.985971689224243 \n",
      "              Params: tensor([  5.2238, -16.4901])\n",
      "              Grad: tensor([-0.0245,  0.1386])\n",
      "Epoch: 1805, Loss: 2.985774040222168 \n",
      "              Params: tensor([  5.2241, -16.4915])\n",
      "              Grad: tensor([-0.0245,  0.1384])\n",
      "Epoch: 1806, Loss: 2.9855782985687256 \n",
      "              Params: tensor([  5.2243, -16.4929])\n",
      "              Grad: tensor([-0.0244,  0.1382])\n",
      "Epoch: 1807, Loss: 2.9853806495666504 \n",
      "              Params: tensor([  5.2245, -16.4943])\n",
      "              Grad: tensor([-0.0244,  0.1379])\n",
      "Epoch: 1808, Loss: 2.9851839542388916 \n",
      "              Params: tensor([  5.2248, -16.4957])\n",
      "              Grad: tensor([-0.0243,  0.1377])\n",
      "Epoch: 1809, Loss: 2.9849894046783447 \n",
      "              Params: tensor([  5.2250, -16.4970])\n",
      "              Grad: tensor([-0.0243,  0.1374])\n",
      "Epoch: 1810, Loss: 2.984792947769165 \n",
      "              Params: tensor([  5.2253, -16.4984])\n",
      "              Grad: tensor([-0.0243,  0.1372])\n",
      "Epoch: 1811, Loss: 2.9846010208129883 \n",
      "              Params: tensor([  5.2255, -16.4998])\n",
      "              Grad: tensor([-0.0242,  0.1370])\n",
      "Epoch: 1812, Loss: 2.9844069480895996 \n",
      "              Params: tensor([  5.2258, -16.5011])\n",
      "              Grad: tensor([-0.0242,  0.1368])\n",
      "Epoch: 1813, Loss: 2.984215021133423 \n",
      "              Params: tensor([  5.2260, -16.5025])\n",
      "              Grad: tensor([-0.0241,  0.1365])\n",
      "Epoch: 1814, Loss: 2.9840216636657715 \n",
      "              Params: tensor([  5.2262, -16.5039])\n",
      "              Grad: tensor([-0.0241,  0.1363])\n",
      "Epoch: 1815, Loss: 2.9838309288024902 \n",
      "              Params: tensor([  5.2265, -16.5052])\n",
      "              Grad: tensor([-0.0240,  0.1361])\n",
      "Epoch: 1816, Loss: 2.9836390018463135 \n",
      "              Params: tensor([  5.2267, -16.5066])\n",
      "              Grad: tensor([-0.0240,  0.1358])\n",
      "Epoch: 1817, Loss: 2.9834494590759277 \n",
      "              Params: tensor([  5.2270, -16.5079])\n",
      "              Grad: tensor([-0.0239,  0.1356])\n",
      "Epoch: 1818, Loss: 2.983259439468384 \n",
      "              Params: tensor([  5.2272, -16.5093])\n",
      "              Grad: tensor([-0.0239,  0.1354])\n",
      "Epoch: 1819, Loss: 2.983072519302368 \n",
      "              Params: tensor([  5.2274, -16.5107])\n",
      "              Grad: tensor([-0.0239,  0.1351])\n",
      "Epoch: 1820, Loss: 2.9828836917877197 \n",
      "              Params: tensor([  5.2277, -16.5120])\n",
      "              Grad: tensor([-0.0238,  0.1349])\n",
      "Epoch: 1821, Loss: 2.9826972484588623 \n",
      "              Params: tensor([  5.2279, -16.5133])\n",
      "              Grad: tensor([-0.0238,  0.1347])\n",
      "Epoch: 1822, Loss: 2.9825096130371094 \n",
      "              Params: tensor([  5.2281, -16.5147])\n",
      "              Grad: tensor([-0.0237,  0.1344])\n",
      "Epoch: 1823, Loss: 2.9823217391967773 \n",
      "              Params: tensor([  5.2284, -16.5160])\n",
      "              Grad: tensor([-0.0237,  0.1342])\n",
      "Epoch: 1824, Loss: 2.982137441635132 \n",
      "              Params: tensor([  5.2286, -16.5174])\n",
      "              Grad: tensor([-0.0237,  0.1340])\n",
      "Epoch: 1825, Loss: 2.9819529056549072 \n",
      "              Params: tensor([  5.2289, -16.5187])\n",
      "              Grad: tensor([-0.0236,  0.1338])\n",
      "Epoch: 1826, Loss: 2.9817686080932617 \n",
      "              Params: tensor([  5.2291, -16.5200])\n",
      "              Grad: tensor([-0.0236,  0.1335])\n",
      "Epoch: 1827, Loss: 2.9815855026245117 \n",
      "              Params: tensor([  5.2293, -16.5214])\n",
      "              Grad: tensor([-0.0236,  0.1333])\n",
      "Epoch: 1828, Loss: 2.9814023971557617 \n",
      "              Params: tensor([  5.2296, -16.5227])\n",
      "              Grad: tensor([-0.0235,  0.1331])\n",
      "Epoch: 1829, Loss: 2.9812192916870117 \n",
      "              Params: tensor([  5.2298, -16.5240])\n",
      "              Grad: tensor([-0.0235,  0.1329])\n",
      "Epoch: 1830, Loss: 2.981037139892578 \n",
      "              Params: tensor([  5.2300, -16.5254])\n",
      "              Grad: tensor([-0.0235,  0.1326])\n",
      "Epoch: 1831, Loss: 2.980855703353882 \n",
      "              Params: tensor([  5.2303, -16.5267])\n",
      "              Grad: tensor([-0.0234,  0.1324])\n",
      "Epoch: 1832, Loss: 2.980675458908081 \n",
      "              Params: tensor([  5.2305, -16.5280])\n",
      "              Grad: tensor([-0.0234,  0.1322])\n",
      "Epoch: 1833, Loss: 2.9804954528808594 \n",
      "              Params: tensor([  5.2307, -16.5293])\n",
      "              Grad: tensor([-0.0233,  0.1320])\n",
      "Epoch: 1834, Loss: 2.9803154468536377 \n",
      "              Params: tensor([  5.2310, -16.5306])\n",
      "              Grad: tensor([-0.0233,  0.1317])\n",
      "Epoch: 1835, Loss: 2.9801371097564697 \n",
      "              Params: tensor([  5.2312, -16.5320])\n",
      "              Grad: tensor([-0.0232,  0.1315])\n",
      "Epoch: 1836, Loss: 2.9799582958221436 \n",
      "              Params: tensor([  5.2314, -16.5333])\n",
      "              Grad: tensor([-0.0232,  0.1313])\n",
      "Epoch: 1837, Loss: 2.9797821044921875 \n",
      "              Params: tensor([  5.2317, -16.5346])\n",
      "              Grad: tensor([-0.0232,  0.1311])\n",
      "Epoch: 1838, Loss: 2.9796042442321777 \n",
      "              Params: tensor([  5.2319, -16.5359])\n",
      "              Grad: tensor([-0.0231,  0.1308])\n",
      "Epoch: 1839, Loss: 2.979428291320801 \n",
      "              Params: tensor([  5.2321, -16.5372])\n",
      "              Grad: tensor([-0.0231,  0.1306])\n",
      "Epoch: 1840, Loss: 2.979252576828003 \n",
      "              Params: tensor([  5.2324, -16.5385])\n",
      "              Grad: tensor([-0.0230,  0.1304])\n",
      "Epoch: 1841, Loss: 2.9790780544281006 \n",
      "              Params: tensor([  5.2326, -16.5398])\n",
      "              Grad: tensor([-0.0230,  0.1302])\n",
      "Epoch: 1842, Loss: 2.9789023399353027 \n",
      "              Params: tensor([  5.2328, -16.5411])\n",
      "              Grad: tensor([-0.0229,  0.1300])\n",
      "Epoch: 1843, Loss: 2.9787285327911377 \n",
      "              Params: tensor([  5.2330, -16.5424])\n",
      "              Grad: tensor([-0.0229,  0.1297])\n",
      "Epoch: 1844, Loss: 2.978555917739868 \n",
      "              Params: tensor([  5.2333, -16.5437])\n",
      "              Grad: tensor([-0.0229,  0.1295])\n",
      "Epoch: 1845, Loss: 2.9783823490142822 \n",
      "              Params: tensor([  5.2335, -16.5450])\n",
      "              Grad: tensor([-0.0228,  0.1293])\n",
      "Epoch: 1846, Loss: 2.978210687637329 \n",
      "              Params: tensor([  5.2337, -16.5463])\n",
      "              Grad: tensor([-0.0228,  0.1291])\n",
      "Epoch: 1847, Loss: 2.9780385494232178 \n",
      "              Params: tensor([  5.2340, -16.5476])\n",
      "              Grad: tensor([-0.0228,  0.1288])\n",
      "Epoch: 1848, Loss: 2.9778671264648438 \n",
      "              Params: tensor([  5.2342, -16.5489])\n",
      "              Grad: tensor([-0.0227,  0.1286])\n",
      "Epoch: 1849, Loss: 2.977696180343628 \n",
      "              Params: tensor([  5.2344, -16.5501])\n",
      "              Grad: tensor([-0.0227,  0.1284])\n",
      "Epoch: 1850, Loss: 2.977527379989624 \n",
      "              Params: tensor([  5.2346, -16.5514])\n",
      "              Grad: tensor([-0.0227,  0.1282])\n",
      "Epoch: 1851, Loss: 2.9773573875427246 \n",
      "              Params: tensor([  5.2349, -16.5527])\n",
      "              Grad: tensor([-0.0226,  0.1280])\n",
      "Epoch: 1852, Loss: 2.9771883487701416 \n",
      "              Params: tensor([  5.2351, -16.5540])\n",
      "              Grad: tensor([-0.0226,  0.1278])\n",
      "Epoch: 1853, Loss: 2.977020502090454 \n",
      "              Params: tensor([  5.2353, -16.5553])\n",
      "              Grad: tensor([-0.0225,  0.1275])\n",
      "Epoch: 1854, Loss: 2.976853370666504 \n",
      "              Params: tensor([  5.2355, -16.5565])\n",
      "              Grad: tensor([-0.0225,  0.1273])\n",
      "Epoch: 1855, Loss: 2.976686716079712 \n",
      "              Params: tensor([  5.2358, -16.5578])\n",
      "              Grad: tensor([-0.0225,  0.1271])\n",
      "Epoch: 1856, Loss: 2.9765195846557617 \n",
      "              Params: tensor([  5.2360, -16.5591])\n",
      "              Grad: tensor([-0.0224,  0.1269])\n",
      "Epoch: 1857, Loss: 2.976353645324707 \n",
      "              Params: tensor([  5.2362, -16.5603])\n",
      "              Grad: tensor([-0.0224,  0.1267])\n",
      "Epoch: 1858, Loss: 2.976189374923706 \n",
      "              Params: tensor([  5.2364, -16.5616])\n",
      "              Grad: tensor([-0.0223,  0.1265])\n",
      "Epoch: 1859, Loss: 2.9760231971740723 \n",
      "              Params: tensor([  5.2367, -16.5629])\n",
      "              Grad: tensor([-0.0223,  0.1263])\n",
      "Epoch: 1860, Loss: 2.9758596420288086 \n",
      "              Params: tensor([  5.2369, -16.5641])\n",
      "              Grad: tensor([-0.0223,  0.1260])\n",
      "Epoch: 1861, Loss: 2.975696563720703 \n",
      "              Params: tensor([  5.2371, -16.5654])\n",
      "              Grad: tensor([-0.0222,  0.1258])\n",
      "Epoch: 1862, Loss: 2.9755325317382812 \n",
      "              Params: tensor([  5.2373, -16.5666])\n",
      "              Grad: tensor([-0.0222,  0.1256])\n",
      "Epoch: 1863, Loss: 2.9753692150115967 \n",
      "              Params: tensor([  5.2375, -16.5679])\n",
      "              Grad: tensor([-0.0222,  0.1254])\n",
      "Epoch: 1864, Loss: 2.975208282470703 \n",
      "              Params: tensor([  5.2378, -16.5691])\n",
      "              Grad: tensor([-0.0221,  0.1252])\n",
      "Epoch: 1865, Loss: 2.975045919418335 \n",
      "              Params: tensor([  5.2380, -16.5704])\n",
      "              Grad: tensor([-0.0221,  0.1250])\n",
      "Epoch: 1866, Loss: 2.9748857021331787 \n",
      "              Params: tensor([  5.2382, -16.5716])\n",
      "              Grad: tensor([-0.0220,  0.1248])\n",
      "Epoch: 1867, Loss: 2.974724769592285 \n",
      "              Params: tensor([  5.2384, -16.5729])\n",
      "              Grad: tensor([-0.0220,  0.1245])\n",
      "Epoch: 1868, Loss: 2.974565029144287 \n",
      "              Params: tensor([  5.2386, -16.5741])\n",
      "              Grad: tensor([-0.0220,  0.1243])\n",
      "Epoch: 1869, Loss: 2.9744057655334473 \n",
      "              Params: tensor([  5.2389, -16.5754])\n",
      "              Grad: tensor([-0.0219,  0.1241])\n",
      "Epoch: 1870, Loss: 2.974247694015503 \n",
      "              Params: tensor([  5.2391, -16.5766])\n",
      "              Grad: tensor([-0.0219,  0.1239])\n",
      "Epoch: 1871, Loss: 2.974088191986084 \n",
      "              Params: tensor([  5.2393, -16.5778])\n",
      "              Grad: tensor([-0.0219,  0.1237])\n",
      "Epoch: 1872, Loss: 2.9739303588867188 \n",
      "              Params: tensor([  5.2395, -16.5791])\n",
      "              Grad: tensor([-0.0218,  0.1235])\n",
      "Epoch: 1873, Loss: 2.973775625228882 \n",
      "              Params: tensor([  5.2397, -16.5803])\n",
      "              Grad: tensor([-0.0218,  0.1233])\n",
      "Epoch: 1874, Loss: 2.973618268966675 \n",
      "              Params: tensor([  5.2400, -16.5815])\n",
      "              Grad: tensor([-0.0217,  0.1231])\n",
      "Epoch: 1875, Loss: 2.9734625816345215 \n",
      "              Params: tensor([  5.2402, -16.5828])\n",
      "              Grad: tensor([-0.0217,  0.1229])\n",
      "Epoch: 1876, Loss: 2.973306894302368 \n",
      "              Params: tensor([  5.2404, -16.5840])\n",
      "              Grad: tensor([-0.0217,  0.1227])\n",
      "Epoch: 1877, Loss: 2.9731507301330566 \n",
      "              Params: tensor([  5.2406, -16.5852])\n",
      "              Grad: tensor([-0.0216,  0.1224])\n",
      "Epoch: 1878, Loss: 2.972996473312378 \n",
      "              Params: tensor([  5.2408, -16.5864])\n",
      "              Grad: tensor([-0.0216,  0.1222])\n",
      "Epoch: 1879, Loss: 2.9728434085845947 \n",
      "              Params: tensor([  5.2410, -16.5877])\n",
      "              Grad: tensor([-0.0215,  0.1220])\n",
      "Epoch: 1880, Loss: 2.972689628601074 \n",
      "              Params: tensor([  5.2413, -16.5889])\n",
      "              Grad: tensor([-0.0215,  0.1218])\n",
      "Epoch: 1881, Loss: 2.9725358486175537 \n",
      "              Params: tensor([  5.2415, -16.5901])\n",
      "              Grad: tensor([-0.0215,  0.1216])\n",
      "Epoch: 1882, Loss: 2.9723832607269287 \n",
      "              Params: tensor([  5.2417, -16.5913])\n",
      "              Grad: tensor([-0.0214,  0.1214])\n",
      "Epoch: 1883, Loss: 2.9722321033477783 \n",
      "              Params: tensor([  5.2419, -16.5925])\n",
      "              Grad: tensor([-0.0214,  0.1212])\n",
      "Epoch: 1884, Loss: 2.972081184387207 \n",
      "              Params: tensor([  5.2421, -16.5937])\n",
      "              Grad: tensor([-0.0214,  0.1210])\n",
      "Epoch: 1885, Loss: 2.971931219100952 \n",
      "              Params: tensor([  5.2423, -16.5949])\n",
      "              Grad: tensor([-0.0213,  0.1208])\n",
      "Epoch: 1886, Loss: 2.9717798233032227 \n",
      "              Params: tensor([  5.2425, -16.5961])\n",
      "              Grad: tensor([-0.0213,  0.1206])\n",
      "Epoch: 1887, Loss: 2.9716298580169678 \n",
      "              Params: tensor([  5.2427, -16.5974])\n",
      "              Grad: tensor([-0.0213,  0.1204])\n",
      "Epoch: 1888, Loss: 2.97148060798645 \n",
      "              Params: tensor([  5.2430, -16.5986])\n",
      "              Grad: tensor([-0.0212,  0.1202])\n",
      "Epoch: 1889, Loss: 2.971331834793091 \n",
      "              Params: tensor([  5.2432, -16.5998])\n",
      "              Grad: tensor([-0.0212,  0.1200])\n",
      "Epoch: 1890, Loss: 2.971184492111206 \n",
      "              Params: tensor([  5.2434, -16.6010])\n",
      "              Grad: tensor([-0.0212,  0.1198])\n",
      "Epoch: 1891, Loss: 2.9710352420806885 \n",
      "              Params: tensor([  5.2436, -16.6021])\n",
      "              Grad: tensor([-0.0211,  0.1196])\n",
      "Epoch: 1892, Loss: 2.9708876609802246 \n",
      "              Params: tensor([  5.2438, -16.6033])\n",
      "              Grad: tensor([-0.0211,  0.1194])\n",
      "Epoch: 1893, Loss: 2.970740556716919 \n",
      "              Params: tensor([  5.2440, -16.6045])\n",
      "              Grad: tensor([-0.0211,  0.1192])\n",
      "Epoch: 1894, Loss: 2.9705960750579834 \n",
      "              Params: tensor([  5.2442, -16.6057])\n",
      "              Grad: tensor([-0.0210,  0.1190])\n",
      "Epoch: 1895, Loss: 2.9704489707946777 \n",
      "              Params: tensor([  5.2444, -16.6069])\n",
      "              Grad: tensor([-0.0210,  0.1188])\n",
      "Epoch: 1896, Loss: 2.970303535461426 \n",
      "              Params: tensor([  5.2446, -16.6081])\n",
      "              Grad: tensor([-0.0209,  0.1186])\n",
      "Epoch: 1897, Loss: 2.970158576965332 \n",
      "              Params: tensor([  5.2449, -16.6093])\n",
      "              Grad: tensor([-0.0209,  0.1183])\n",
      "Epoch: 1898, Loss: 2.970015525817871 \n",
      "              Params: tensor([  5.2451, -16.6105])\n",
      "              Grad: tensor([-0.0209,  0.1182])\n",
      "Epoch: 1899, Loss: 2.9698705673217773 \n",
      "              Params: tensor([  5.2453, -16.6116])\n",
      "              Grad: tensor([-0.0208,  0.1180])\n",
      "Epoch: 1900, Loss: 2.9697272777557373 \n",
      "              Params: tensor([  5.2455, -16.6128])\n",
      "              Grad: tensor([-0.0208,  0.1178])\n",
      "Epoch: 1901, Loss: 2.969585657119751 \n",
      "              Params: tensor([  5.2457, -16.6140])\n",
      "              Grad: tensor([-0.0208,  0.1175])\n",
      "Epoch: 1902, Loss: 2.96944260597229 \n",
      "              Params: tensor([  5.2459, -16.6152])\n",
      "              Grad: tensor([-0.0207,  0.1173])\n",
      "Epoch: 1903, Loss: 2.969301700592041 \n",
      "              Params: tensor([  5.2461, -16.6163])\n",
      "              Grad: tensor([-0.0207,  0.1172])\n",
      "Epoch: 1904, Loss: 2.9691596031188965 \n",
      "              Params: tensor([  5.2463, -16.6175])\n",
      "              Grad: tensor([-0.0206,  0.1170])\n",
      "Epoch: 1905, Loss: 2.969017267227173 \n",
      "              Params: tensor([  5.2465, -16.6187])\n",
      "              Grad: tensor([-0.0206,  0.1168])\n",
      "Epoch: 1906, Loss: 2.968878984451294 \n",
      "              Params: tensor([  5.2467, -16.6198])\n",
      "              Grad: tensor([-0.0206,  0.1166])\n",
      "Epoch: 1907, Loss: 2.9687387943267822 \n",
      "              Params: tensor([  5.2469, -16.6210])\n",
      "              Grad: tensor([-0.0205,  0.1164])\n",
      "Epoch: 1908, Loss: 2.9685988426208496 \n",
      "              Params: tensor([  5.2471, -16.6222])\n",
      "              Grad: tensor([-0.0205,  0.1162])\n",
      "Epoch: 1909, Loss: 2.9684598445892334 \n",
      "              Params: tensor([  5.2473, -16.6233])\n",
      "              Grad: tensor([-0.0205,  0.1160])\n",
      "Epoch: 1910, Loss: 2.9683213233947754 \n",
      "              Params: tensor([  5.2475, -16.6245])\n",
      "              Grad: tensor([-0.0204,  0.1158])\n",
      "Epoch: 1911, Loss: 2.9681832790374756 \n",
      "              Params: tensor([  5.2477, -16.6256])\n",
      "              Grad: tensor([-0.0204,  0.1156])\n",
      "Epoch: 1912, Loss: 2.968045711517334 \n",
      "              Params: tensor([  5.2479, -16.6268])\n",
      "              Grad: tensor([-0.0204,  0.1154])\n",
      "Epoch: 1913, Loss: 2.9679079055786133 \n",
      "              Params: tensor([  5.2482, -16.6279])\n",
      "              Grad: tensor([-0.0204,  0.1152])\n",
      "Epoch: 1914, Loss: 2.9677717685699463 \n",
      "              Params: tensor([  5.2484, -16.6291])\n",
      "              Grad: tensor([-0.0203,  0.1150])\n",
      "Epoch: 1915, Loss: 2.9676358699798584 \n",
      "              Params: tensor([  5.2486, -16.6302])\n",
      "              Grad: tensor([-0.0203,  0.1148])\n",
      "Epoch: 1916, Loss: 2.967499017715454 \n",
      "              Params: tensor([  5.2488, -16.6314])\n",
      "              Grad: tensor([-0.0202,  0.1146])\n",
      "Epoch: 1917, Loss: 2.967364549636841 \n",
      "              Params: tensor([  5.2490, -16.6325])\n",
      "              Grad: tensor([-0.0202,  0.1144])\n",
      "Epoch: 1918, Loss: 2.9672296047210693 \n",
      "              Params: tensor([  5.2492, -16.6337])\n",
      "              Grad: tensor([-0.0202,  0.1142])\n",
      "Epoch: 1919, Loss: 2.967095375061035 \n",
      "              Params: tensor([  5.2494, -16.6348])\n",
      "              Grad: tensor([-0.0202,  0.1140])\n",
      "Epoch: 1920, Loss: 2.966960906982422 \n",
      "              Params: tensor([  5.2496, -16.6360])\n",
      "              Grad: tensor([-0.0201,  0.1138])\n",
      "Epoch: 1921, Loss: 2.966827630996704 \n",
      "              Params: tensor([  5.2498, -16.6371])\n",
      "              Grad: tensor([-0.0201,  0.1136])\n",
      "Epoch: 1922, Loss: 2.966693162918091 \n",
      "              Params: tensor([  5.2500, -16.6382])\n",
      "              Grad: tensor([-0.0200,  0.1134])\n",
      "Epoch: 1923, Loss: 2.9665610790252686 \n",
      "              Params: tensor([  5.2502, -16.6394])\n",
      "              Grad: tensor([-0.0200,  0.1132])\n",
      "Epoch: 1924, Loss: 2.9664289951324463 \n",
      "              Params: tensor([  5.2504, -16.6405])\n",
      "              Grad: tensor([-0.0200,  0.1130])\n",
      "Epoch: 1925, Loss: 2.9662973880767822 \n",
      "              Params: tensor([  5.2506, -16.6416])\n",
      "              Grad: tensor([-0.0199,  0.1128])\n",
      "Epoch: 1926, Loss: 2.966167688369751 \n",
      "              Params: tensor([  5.2508, -16.6427])\n",
      "              Grad: tensor([-0.0199,  0.1127])\n",
      "Epoch: 1927, Loss: 2.9660356044769287 \n",
      "              Params: tensor([  5.2510, -16.6439])\n",
      "              Grad: tensor([-0.0199,  0.1125])\n",
      "Epoch: 1928, Loss: 2.965904474258423 \n",
      "              Params: tensor([  5.2512, -16.6450])\n",
      "              Grad: tensor([-0.0198,  0.1123])\n",
      "Epoch: 1929, Loss: 2.9657766819000244 \n",
      "              Params: tensor([  5.2514, -16.6461])\n",
      "              Grad: tensor([-0.0198,  0.1121])\n",
      "Epoch: 1930, Loss: 2.965646505355835 \n",
      "              Params: tensor([  5.2516, -16.6472])\n",
      "              Grad: tensor([-0.0198,  0.1119])\n",
      "Epoch: 1931, Loss: 2.9655160903930664 \n",
      "              Params: tensor([  5.2518, -16.6484])\n",
      "              Grad: tensor([-0.0197,  0.1117])\n",
      "Epoch: 1932, Loss: 2.9653875827789307 \n",
      "              Params: tensor([  5.2520, -16.6495])\n",
      "              Grad: tensor([-0.0197,  0.1115])\n",
      "Epoch: 1933, Loss: 2.9652605056762695 \n",
      "              Params: tensor([  5.2522, -16.6506])\n",
      "              Grad: tensor([-0.0197,  0.1113])\n",
      "Epoch: 1934, Loss: 2.9651312828063965 \n",
      "              Params: tensor([  5.2523, -16.6517])\n",
      "              Grad: tensor([-0.0196,  0.1111])\n",
      "Epoch: 1935, Loss: 2.96500563621521 \n",
      "              Params: tensor([  5.2525, -16.6528])\n",
      "              Grad: tensor([-0.0196,  0.1109])\n",
      "Epoch: 1936, Loss: 2.9648773670196533 \n",
      "              Params: tensor([  5.2527, -16.6539])\n",
      "              Grad: tensor([-0.0196,  0.1108])\n",
      "Epoch: 1937, Loss: 2.9647514820098877 \n",
      "              Params: tensor([  5.2529, -16.6550])\n",
      "              Grad: tensor([-0.0195,  0.1106])\n",
      "Epoch: 1938, Loss: 2.964625358581543 \n",
      "              Params: tensor([  5.2531, -16.6561])\n",
      "              Grad: tensor([-0.0195,  0.1104])\n",
      "Epoch: 1939, Loss: 2.9644997119903564 \n",
      "              Params: tensor([  5.2533, -16.6572])\n",
      "              Grad: tensor([-0.0195,  0.1102])\n",
      "Epoch: 1940, Loss: 2.9643752574920654 \n",
      "              Params: tensor([  5.2535, -16.6583])\n",
      "              Grad: tensor([-0.0195,  0.1100])\n",
      "Epoch: 1941, Loss: 2.964250326156616 \n",
      "              Params: tensor([  5.2537, -16.6594])\n",
      "              Grad: tensor([-0.0194,  0.1098])\n",
      "Epoch: 1942, Loss: 2.964125871658325 \n",
      "              Params: tensor([  5.2539, -16.6605])\n",
      "              Grad: tensor([-0.0194,  0.1096])\n",
      "Epoch: 1943, Loss: 2.964000940322876 \n",
      "              Params: tensor([  5.2541, -16.6616])\n",
      "              Grad: tensor([-0.0194,  0.1094])\n",
      "Epoch: 1944, Loss: 2.963878870010376 \n",
      "              Params: tensor([  5.2543, -16.6627])\n",
      "              Grad: tensor([-0.0193,  0.1093])\n",
      "Epoch: 1945, Loss: 2.9637558460235596 \n",
      "              Params: tensor([  5.2545, -16.6638])\n",
      "              Grad: tensor([-0.0193,  0.1091])\n",
      "Epoch: 1946, Loss: 2.963632345199585 \n",
      "              Params: tensor([  5.2547, -16.6649])\n",
      "              Grad: tensor([-0.0192,  0.1089])\n",
      "Epoch: 1947, Loss: 2.963510751724243 \n",
      "              Params: tensor([  5.2549, -16.6660])\n",
      "              Grad: tensor([-0.0192,  0.1087])\n",
      "Epoch: 1948, Loss: 2.963387966156006 \n",
      "              Params: tensor([  5.2551, -16.6671])\n",
      "              Grad: tensor([-0.0192,  0.1085])\n",
      "Epoch: 1949, Loss: 2.963266134262085 \n",
      "              Params: tensor([  5.2553, -16.6681])\n",
      "              Grad: tensor([-0.0191,  0.1083])\n",
      "Epoch: 1950, Loss: 2.963148593902588 \n",
      "              Params: tensor([  5.2554, -16.6692])\n",
      "              Grad: tensor([-0.0191,  0.1081])\n",
      "Epoch: 1951, Loss: 2.963026285171509 \n",
      "              Params: tensor([  5.2556, -16.6703])\n",
      "              Grad: tensor([-0.0191,  0.1080])\n",
      "Epoch: 1952, Loss: 2.9629065990448 \n",
      "              Params: tensor([  5.2558, -16.6714])\n",
      "              Grad: tensor([-0.0190,  0.1078])\n",
      "Epoch: 1953, Loss: 2.9627878665924072 \n",
      "              Params: tensor([  5.2560, -16.6725])\n",
      "              Grad: tensor([-0.0190,  0.1076])\n",
      "Epoch: 1954, Loss: 2.9626665115356445 \n",
      "              Params: tensor([  5.2562, -16.6735])\n",
      "              Grad: tensor([-0.0190,  0.1074])\n",
      "Epoch: 1955, Loss: 2.9625465869903564 \n",
      "              Params: tensor([  5.2564, -16.6746])\n",
      "              Grad: tensor([-0.0189,  0.1072])\n",
      "Epoch: 1956, Loss: 2.9624290466308594 \n",
      "              Params: tensor([  5.2566, -16.6757])\n",
      "              Grad: tensor([-0.0189,  0.1071])\n",
      "Epoch: 1957, Loss: 2.9623117446899414 \n",
      "              Params: tensor([  5.2568, -16.6767])\n",
      "              Grad: tensor([-0.0189,  0.1069])\n",
      "Epoch: 1958, Loss: 2.9621946811676025 \n",
      "              Params: tensor([  5.2570, -16.6778])\n",
      "              Grad: tensor([-0.0188,  0.1067])\n",
      "Epoch: 1959, Loss: 2.9620778560638428 \n",
      "              Params: tensor([  5.2572, -16.6789])\n",
      "              Grad: tensor([-0.0188,  0.1065])\n",
      "Epoch: 1960, Loss: 2.96195912361145 \n",
      "              Params: tensor([  5.2573, -16.6799])\n",
      "              Grad: tensor([-0.0188,  0.1063])\n",
      "Epoch: 1961, Loss: 2.9618430137634277 \n",
      "              Params: tensor([  5.2575, -16.6810])\n",
      "              Grad: tensor([-0.0187,  0.1062])\n",
      "Epoch: 1962, Loss: 2.961728096008301 \n",
      "              Params: tensor([  5.2577, -16.6821])\n",
      "              Grad: tensor([-0.0187,  0.1060])\n",
      "Epoch: 1963, Loss: 2.961611270904541 \n",
      "              Params: tensor([  5.2579, -16.6831])\n",
      "              Grad: tensor([-0.0187,  0.1058])\n",
      "Epoch: 1964, Loss: 2.961495876312256 \n",
      "              Params: tensor([  5.2581, -16.6842])\n",
      "              Grad: tensor([-0.0187,  0.1056])\n",
      "Epoch: 1965, Loss: 2.9613821506500244 \n",
      "              Params: tensor([  5.2583, -16.6852])\n",
      "              Grad: tensor([-0.0186,  0.1054])\n",
      "Epoch: 1966, Loss: 2.9612672328948975 \n",
      "              Params: tensor([  5.2585, -16.6863])\n",
      "              Grad: tensor([-0.0186,  0.1052])\n",
      "Epoch: 1967, Loss: 2.9611525535583496 \n",
      "              Params: tensor([  5.2586, -16.6873])\n",
      "              Grad: tensor([-0.0186,  0.1051])\n",
      "Epoch: 1968, Loss: 2.96103835105896 \n",
      "              Params: tensor([  5.2588, -16.6884])\n",
      "              Grad: tensor([-0.0185,  0.1049])\n",
      "Epoch: 1969, Loss: 2.9609262943267822 \n",
      "              Params: tensor([  5.2590, -16.6894])\n",
      "              Grad: tensor([-0.0185,  0.1047])\n",
      "Epoch: 1970, Loss: 2.960813283920288 \n",
      "              Params: tensor([  5.2592, -16.6905])\n",
      "              Grad: tensor([-0.0185,  0.1045])\n",
      "Epoch: 1971, Loss: 2.9606995582580566 \n",
      "              Params: tensor([  5.2594, -16.6915])\n",
      "              Grad: tensor([-0.0184,  0.1044])\n",
      "Epoch: 1972, Loss: 2.9605867862701416 \n",
      "              Params: tensor([  5.2596, -16.6926])\n",
      "              Grad: tensor([-0.0184,  0.1042])\n",
      "Epoch: 1973, Loss: 2.960475206375122 \n",
      "              Params: tensor([  5.2598, -16.6936])\n",
      "              Grad: tensor([-0.0184,  0.1040])\n",
      "Epoch: 1974, Loss: 2.960365056991577 \n",
      "              Params: tensor([  5.2599, -16.6946])\n",
      "              Grad: tensor([-0.0183,  0.1038])\n",
      "Epoch: 1975, Loss: 2.960254669189453 \n",
      "              Params: tensor([  5.2601, -16.6957])\n",
      "              Grad: tensor([-0.0183,  0.1037])\n",
      "Epoch: 1976, Loss: 2.9601433277130127 \n",
      "              Params: tensor([  5.2603, -16.6967])\n",
      "              Grad: tensor([-0.0183,  0.1035])\n",
      "Epoch: 1977, Loss: 2.9600331783294678 \n",
      "              Params: tensor([  5.2605, -16.6977])\n",
      "              Grad: tensor([-0.0182,  0.1033])\n",
      "Epoch: 1978, Loss: 2.959923028945923 \n",
      "              Params: tensor([  5.2607, -16.6988])\n",
      "              Grad: tensor([-0.0182,  0.1031])\n",
      "Epoch: 1979, Loss: 2.9598119258880615 \n",
      "              Params: tensor([  5.2608, -16.6998])\n",
      "              Grad: tensor([-0.0182,  0.1029])\n",
      "Epoch: 1980, Loss: 2.959703207015991 \n",
      "              Params: tensor([  5.2610, -16.7008])\n",
      "              Grad: tensor([-0.0182,  0.1028])\n",
      "Epoch: 1981, Loss: 2.9595940113067627 \n",
      "              Params: tensor([  5.2612, -16.7019])\n",
      "              Grad: tensor([-0.0181,  0.1026])\n",
      "Epoch: 1982, Loss: 2.9594857692718506 \n",
      "              Params: tensor([  5.2614, -16.7029])\n",
      "              Grad: tensor([-0.0181,  0.1024])\n",
      "Epoch: 1983, Loss: 2.959378242492676 \n",
      "              Params: tensor([  5.2616, -16.7039])\n",
      "              Grad: tensor([-0.0181,  0.1022])\n",
      "Epoch: 1984, Loss: 2.959270715713501 \n",
      "              Params: tensor([  5.2618, -16.7049])\n",
      "              Grad: tensor([-0.0180,  0.1021])\n",
      "Epoch: 1985, Loss: 2.9591622352600098 \n",
      "              Params: tensor([  5.2619, -16.7059])\n",
      "              Grad: tensor([-0.0180,  0.1019])\n",
      "Epoch: 1986, Loss: 2.959054708480835 \n",
      "              Params: tensor([  5.2621, -16.7070])\n",
      "              Grad: tensor([-0.0180,  0.1017])\n",
      "Epoch: 1987, Loss: 2.9589502811431885 \n",
      "              Params: tensor([  5.2623, -16.7080])\n",
      "              Grad: tensor([-0.0179,  0.1016])\n",
      "Epoch: 1988, Loss: 2.9588422775268555 \n",
      "              Params: tensor([  5.2625, -16.7090])\n",
      "              Grad: tensor([-0.0179,  0.1014])\n",
      "Epoch: 1989, Loss: 2.958737850189209 \n",
      "              Params: tensor([  5.2626, -16.7100])\n",
      "              Grad: tensor([-0.0179,  0.1012])\n",
      "Epoch: 1990, Loss: 2.958631992340088 \n",
      "              Params: tensor([  5.2628, -16.7110])\n",
      "              Grad: tensor([-0.0179,  0.1010])\n",
      "Epoch: 1991, Loss: 2.9585258960723877 \n",
      "              Params: tensor([  5.2630, -16.7120])\n",
      "              Grad: tensor([-0.0178,  0.1009])\n",
      "Epoch: 1992, Loss: 2.9584219455718994 \n",
      "              Params: tensor([  5.2632, -16.7130])\n",
      "              Grad: tensor([-0.0178,  0.1007])\n",
      "Epoch: 1993, Loss: 2.9583168029785156 \n",
      "              Params: tensor([  5.2634, -16.7140])\n",
      "              Grad: tensor([-0.0178,  0.1005])\n",
      "Epoch: 1994, Loss: 2.95821213722229 \n",
      "              Params: tensor([  5.2635, -16.7150])\n",
      "              Grad: tensor([-0.0177,  0.1004])\n",
      "Epoch: 1995, Loss: 2.958109140396118 \n",
      "              Params: tensor([  5.2637, -16.7160])\n",
      "              Grad: tensor([-0.0177,  0.1002])\n",
      "Epoch: 1996, Loss: 2.9580061435699463 \n",
      "              Params: tensor([  5.2639, -16.7170])\n",
      "              Grad: tensor([-0.0176,  0.1000])\n",
      "Epoch: 1997, Loss: 2.9579038619995117 \n",
      "              Params: tensor([  5.2641, -16.7180])\n",
      "              Grad: tensor([-0.0176,  0.0998])\n",
      "Epoch: 1998, Loss: 2.95780086517334 \n",
      "              Params: tensor([  5.2642, -16.7190])\n",
      "              Grad: tensor([-0.0176,  0.0997])\n",
      "Epoch: 1999, Loss: 2.957697868347168 \n",
      "              Params: tensor([  5.2644, -16.7200])\n",
      "              Grad: tensor([-0.0176,  0.0995])\n",
      "Epoch: 2000, Loss: 2.9575960636138916 \n",
      "              Params: tensor([  5.2646, -16.7210])\n",
      "              Grad: tensor([-0.0176,  0.0993])\n",
      "Epoch: 2001, Loss: 2.9574942588806152 \n",
      "              Params: tensor([  5.2648, -16.7220])\n",
      "              Grad: tensor([-0.0175,  0.0992])\n",
      "Epoch: 2002, Loss: 2.957392692565918 \n",
      "              Params: tensor([  5.2649, -16.7230])\n",
      "              Grad: tensor([-0.0175,  0.0990])\n",
      "Epoch: 2003, Loss: 2.957292079925537 \n",
      "              Params: tensor([  5.2651, -16.7240])\n",
      "              Grad: tensor([-0.0174,  0.0988])\n",
      "Epoch: 2004, Loss: 2.957192897796631 \n",
      "              Params: tensor([  5.2653, -16.7250])\n",
      "              Grad: tensor([-0.0174,  0.0987])\n",
      "Epoch: 2005, Loss: 2.9570913314819336 \n",
      "              Params: tensor([  5.2655, -16.7260])\n",
      "              Grad: tensor([-0.0174,  0.0985])\n",
      "Epoch: 2006, Loss: 2.956991672515869 \n",
      "              Params: tensor([  5.2656, -16.7269])\n",
      "              Grad: tensor([-0.0174,  0.0983])\n",
      "Epoch: 2007, Loss: 2.956892490386963 \n",
      "              Params: tensor([  5.2658, -16.7279])\n",
      "              Grad: tensor([-0.0173,  0.0982])\n",
      "Epoch: 2008, Loss: 2.9567923545837402 \n",
      "              Params: tensor([  5.2660, -16.7289])\n",
      "              Grad: tensor([-0.0173,  0.0980])\n",
      "Epoch: 2009, Loss: 2.9566941261291504 \n",
      "              Params: tensor([  5.2662, -16.7299])\n",
      "              Grad: tensor([-0.0173,  0.0978])\n",
      "Epoch: 2010, Loss: 2.9565954208374023 \n",
      "              Params: tensor([  5.2663, -16.7309])\n",
      "              Grad: tensor([-0.0172,  0.0977])\n",
      "Epoch: 2011, Loss: 2.956496238708496 \n",
      "              Params: tensor([  5.2665, -16.7318])\n",
      "              Grad: tensor([-0.0172,  0.0975])\n",
      "Epoch: 2012, Loss: 2.95639705657959 \n",
      "              Params: tensor([  5.2667, -16.7328])\n",
      "              Grad: tensor([-0.0172,  0.0973])\n",
      "Epoch: 2013, Loss: 2.9563004970550537 \n",
      "              Params: tensor([  5.2668, -16.7338])\n",
      "              Grad: tensor([-0.0172,  0.0972])\n",
      "Epoch: 2014, Loss: 2.9562041759490967 \n",
      "              Params: tensor([  5.2670, -16.7348])\n",
      "              Grad: tensor([-0.0171,  0.0970])\n",
      "Epoch: 2015, Loss: 2.9561080932617188 \n",
      "              Params: tensor([  5.2672, -16.7357])\n",
      "              Grad: tensor([-0.0171,  0.0968])\n",
      "Epoch: 2016, Loss: 2.95600962638855 \n",
      "              Params: tensor([  5.2674, -16.7367])\n",
      "              Grad: tensor([-0.0171,  0.0967])\n",
      "Epoch: 2017, Loss: 2.955913782119751 \n",
      "              Params: tensor([  5.2675, -16.7377])\n",
      "              Grad: tensor([-0.0171,  0.0965])\n",
      "Epoch: 2018, Loss: 2.955817461013794 \n",
      "              Params: tensor([  5.2677, -16.7386])\n",
      "              Grad: tensor([-0.0170,  0.0963])\n",
      "Epoch: 2019, Loss: 2.955721855163574 \n",
      "              Params: tensor([  5.2679, -16.7396])\n",
      "              Grad: tensor([-0.0170,  0.0962])\n",
      "Epoch: 2020, Loss: 2.955627202987671 \n",
      "              Params: tensor([  5.2680, -16.7405])\n",
      "              Grad: tensor([-0.0170,  0.0960])\n",
      "Epoch: 2021, Loss: 2.955533027648926 \n",
      "              Params: tensor([  5.2682, -16.7415])\n",
      "              Grad: tensor([-0.0169,  0.0959])\n",
      "Epoch: 2022, Loss: 2.9554357528686523 \n",
      "              Params: tensor([  5.2684, -16.7425])\n",
      "              Grad: tensor([-0.0169,  0.0957])\n",
      "Epoch: 2023, Loss: 2.955343008041382 \n",
      "              Params: tensor([  5.2686, -16.7434])\n",
      "              Grad: tensor([-0.0169,  0.0955])\n",
      "Epoch: 2024, Loss: 2.955249786376953 \n",
      "              Params: tensor([  5.2687, -16.7444])\n",
      "              Grad: tensor([-0.0169,  0.0954])\n",
      "Epoch: 2025, Loss: 2.9551544189453125 \n",
      "              Params: tensor([  5.2689, -16.7453])\n",
      "              Grad: tensor([-0.0168,  0.0952])\n",
      "Epoch: 2026, Loss: 2.9550621509552 \n",
      "              Params: tensor([  5.2691, -16.7463])\n",
      "              Grad: tensor([-0.0168,  0.0950])\n",
      "Epoch: 2027, Loss: 2.9549689292907715 \n",
      "              Params: tensor([  5.2692, -16.7472])\n",
      "              Grad: tensor([-0.0168,  0.0949])\n",
      "Epoch: 2028, Loss: 2.9548749923706055 \n",
      "              Params: tensor([  5.2694, -16.7482])\n",
      "              Grad: tensor([-0.0167,  0.0947])\n",
      "Epoch: 2029, Loss: 2.9547829627990723 \n",
      "              Params: tensor([  5.2696, -16.7491])\n",
      "              Grad: tensor([-0.0167,  0.0946])\n",
      "Epoch: 2030, Loss: 2.9546914100646973 \n",
      "              Params: tensor([  5.2697, -16.7501])\n",
      "              Grad: tensor([-0.0167,  0.0944])\n",
      "Epoch: 2031, Loss: 2.954599618911743 \n",
      "              Params: tensor([  5.2699, -16.7510])\n",
      "              Grad: tensor([-0.0167,  0.0942])\n",
      "Epoch: 2032, Loss: 2.954507350921631 \n",
      "              Params: tensor([  5.2701, -16.7519])\n",
      "              Grad: tensor([-0.0166,  0.0941])\n",
      "Epoch: 2033, Loss: 2.9544172286987305 \n",
      "              Params: tensor([  5.2702, -16.7529])\n",
      "              Grad: tensor([-0.0166,  0.0939])\n",
      "Epoch: 2034, Loss: 2.9543263912200928 \n",
      "              Params: tensor([  5.2704, -16.7538])\n",
      "              Grad: tensor([-0.0165,  0.0938])\n",
      "Epoch: 2035, Loss: 2.954235076904297 \n",
      "              Params: tensor([  5.2706, -16.7547])\n",
      "              Grad: tensor([-0.0165,  0.0936])\n",
      "Epoch: 2036, Loss: 2.9541451930999756 \n",
      "              Params: tensor([  5.2707, -16.7557])\n",
      "              Grad: tensor([-0.0165,  0.0934])\n",
      "Epoch: 2037, Loss: 2.954055070877075 \n",
      "              Params: tensor([  5.2709, -16.7566])\n",
      "              Grad: tensor([-0.0165,  0.0933])\n",
      "Epoch: 2038, Loss: 2.953965902328491 \n",
      "              Params: tensor([  5.2710, -16.7575])\n",
      "              Grad: tensor([-0.0164,  0.0931])\n",
      "Epoch: 2039, Loss: 2.9538755416870117 \n",
      "              Params: tensor([  5.2712, -16.7585])\n",
      "              Grad: tensor([-0.0164,  0.0930])\n",
      "Epoch: 2040, Loss: 2.953787326812744 \n",
      "              Params: tensor([  5.2714, -16.7594])\n",
      "              Grad: tensor([-0.0164,  0.0928])\n",
      "Epoch: 2041, Loss: 2.953697919845581 \n",
      "              Params: tensor([  5.2715, -16.7603])\n",
      "              Grad: tensor([-0.0164,  0.0926])\n",
      "Epoch: 2042, Loss: 2.9536101818084717 \n",
      "              Params: tensor([  5.2717, -16.7613])\n",
      "              Grad: tensor([-0.0163,  0.0925])\n",
      "Epoch: 2043, Loss: 2.9535210132598877 \n",
      "              Params: tensor([  5.2719, -16.7622])\n",
      "              Grad: tensor([-0.0163,  0.0923])\n",
      "Epoch: 2044, Loss: 2.9534342288970947 \n",
      "              Params: tensor([  5.2720, -16.7631])\n",
      "              Grad: tensor([-0.0163,  0.0922])\n",
      "Epoch: 2045, Loss: 2.9533462524414062 \n",
      "              Params: tensor([  5.2722, -16.7640])\n",
      "              Grad: tensor([-0.0163,  0.0920])\n",
      "Epoch: 2046, Loss: 2.953259229660034 \n",
      "              Params: tensor([  5.2724, -16.7649])\n",
      "              Grad: tensor([-0.0162,  0.0919])\n",
      "Epoch: 2047, Loss: 2.953171491622925 \n",
      "              Params: tensor([  5.2725, -16.7659])\n",
      "              Grad: tensor([-0.0162,  0.0917])\n",
      "Epoch: 2048, Loss: 2.953085422515869 \n",
      "              Params: tensor([  5.2727, -16.7668])\n",
      "              Grad: tensor([-0.0162,  0.0915])\n",
      "Epoch: 2049, Loss: 2.9529998302459717 \n",
      "              Params: tensor([  5.2728, -16.7677])\n",
      "              Grad: tensor([-0.0162,  0.0914])\n",
      "Epoch: 2050, Loss: 2.9529130458831787 \n",
      "              Params: tensor([  5.2730, -16.7686])\n",
      "              Grad: tensor([-0.0161,  0.0912])\n",
      "Epoch: 2051, Loss: 2.9528284072875977 \n",
      "              Params: tensor([  5.2732, -16.7695])\n",
      "              Grad: tensor([-0.0161,  0.0911])\n",
      "Epoch: 2052, Loss: 2.952741861343384 \n",
      "              Params: tensor([  5.2733, -16.7704])\n",
      "              Grad: tensor([-0.0161,  0.0909])\n",
      "Epoch: 2053, Loss: 2.9526567459106445 \n",
      "              Params: tensor([  5.2735, -16.7713])\n",
      "              Grad: tensor([-0.0160,  0.0908])\n",
      "Epoch: 2054, Loss: 2.952570915222168 \n",
      "              Params: tensor([  5.2736, -16.7722])\n",
      "              Grad: tensor([-0.0160,  0.0906])\n",
      "Epoch: 2055, Loss: 2.952486991882324 \n",
      "              Params: tensor([  5.2738, -16.7731])\n",
      "              Grad: tensor([-0.0160,  0.0905])\n",
      "Epoch: 2056, Loss: 2.9524033069610596 \n",
      "              Params: tensor([  5.2740, -16.7740])\n",
      "              Grad: tensor([-0.0160,  0.0903])\n",
      "Epoch: 2057, Loss: 2.952317953109741 \n",
      "              Params: tensor([  5.2741, -16.7749])\n",
      "              Grad: tensor([-0.0159,  0.0902])\n",
      "Epoch: 2058, Loss: 2.9522347450256348 \n",
      "              Params: tensor([  5.2743, -16.7758])\n",
      "              Grad: tensor([-0.0159,  0.0900])\n",
      "Epoch: 2059, Loss: 2.9521515369415283 \n",
      "              Params: tensor([  5.2744, -16.7767])\n",
      "              Grad: tensor([-0.0159,  0.0899])\n",
      "Epoch: 2060, Loss: 2.9520680904388428 \n",
      "              Params: tensor([  5.2746, -16.7776])\n",
      "              Grad: tensor([-0.0158,  0.0897])\n",
      "Epoch: 2061, Loss: 2.9519851207733154 \n",
      "              Params: tensor([  5.2748, -16.7785])\n",
      "              Grad: tensor([-0.0158,  0.0895])\n",
      "Epoch: 2062, Loss: 2.951901912689209 \n",
      "              Params: tensor([  5.2749, -16.7794])\n",
      "              Grad: tensor([-0.0158,  0.0894])\n",
      "Epoch: 2063, Loss: 2.9518203735351562 \n",
      "              Params: tensor([  5.2751, -16.7803])\n",
      "              Grad: tensor([-0.0158,  0.0892])\n",
      "Epoch: 2064, Loss: 2.951737880706787 \n",
      "              Params: tensor([  5.2752, -16.7812])\n",
      "              Grad: tensor([-0.0157,  0.0891])\n",
      "Epoch: 2065, Loss: 2.9516561031341553 \n",
      "              Params: tensor([  5.2754, -16.7821])\n",
      "              Grad: tensor([-0.0157,  0.0889])\n",
      "Epoch: 2066, Loss: 2.951575517654419 \n",
      "              Params: tensor([  5.2755, -16.7830])\n",
      "              Grad: tensor([-0.0157,  0.0888])\n",
      "Epoch: 2067, Loss: 2.9514944553375244 \n",
      "              Params: tensor([  5.2757, -16.7839])\n",
      "              Grad: tensor([-0.0157,  0.0886])\n",
      "Epoch: 2068, Loss: 2.951413154602051 \n",
      "              Params: tensor([  5.2759, -16.7848])\n",
      "              Grad: tensor([-0.0157,  0.0885])\n",
      "Epoch: 2069, Loss: 2.9513330459594727 \n",
      "              Params: tensor([  5.2760, -16.7856])\n",
      "              Grad: tensor([-0.0156,  0.0883])\n",
      "Epoch: 2070, Loss: 2.9512522220611572 \n",
      "              Params: tensor([  5.2762, -16.7865])\n",
      "              Grad: tensor([-0.0156,  0.0882])\n",
      "Epoch: 2071, Loss: 2.9511711597442627 \n",
      "              Params: tensor([  5.2763, -16.7874])\n",
      "              Grad: tensor([-0.0155,  0.0880])\n",
      "Epoch: 2072, Loss: 2.9510927200317383 \n",
      "              Params: tensor([  5.2765, -16.7883])\n",
      "              Grad: tensor([-0.0155,  0.0879])\n",
      "Epoch: 2073, Loss: 2.9510116577148438 \n",
      "              Params: tensor([  5.2766, -16.7892])\n",
      "              Grad: tensor([-0.0155,  0.0877])\n",
      "Epoch: 2074, Loss: 2.9509315490722656 \n",
      "              Params: tensor([  5.2768, -16.7900])\n",
      "              Grad: tensor([-0.0155,  0.0876])\n",
      "Epoch: 2075, Loss: 2.950853109359741 \n",
      "              Params: tensor([  5.2769, -16.7909])\n",
      "              Grad: tensor([-0.0154,  0.0874])\n",
      "Epoch: 2076, Loss: 2.9507744312286377 \n",
      "              Params: tensor([  5.2771, -16.7918])\n",
      "              Grad: tensor([-0.0154,  0.0873])\n",
      "Epoch: 2077, Loss: 2.950697422027588 \n",
      "              Params: tensor([  5.2772, -16.7927])\n",
      "              Grad: tensor([-0.0154,  0.0871])\n",
      "Epoch: 2078, Loss: 2.950618028640747 \n",
      "              Params: tensor([  5.2774, -16.7935])\n",
      "              Grad: tensor([-0.0154,  0.0870])\n",
      "Epoch: 2079, Loss: 2.95054030418396 \n",
      "              Params: tensor([  5.2776, -16.7944])\n",
      "              Grad: tensor([-0.0154,  0.0868])\n",
      "Epoch: 2080, Loss: 2.950463056564331 \n",
      "              Params: tensor([  5.2777, -16.7953])\n",
      "              Grad: tensor([-0.0153,  0.0867])\n",
      "Epoch: 2081, Loss: 2.950385093688965 \n",
      "              Params: tensor([  5.2779, -16.7961])\n",
      "              Grad: tensor([-0.0153,  0.0866])\n",
      "Epoch: 2082, Loss: 2.950308322906494 \n",
      "              Params: tensor([  5.2780, -16.7970])\n",
      "              Grad: tensor([-0.0153,  0.0864])\n",
      "Epoch: 2083, Loss: 2.9502313137054443 \n",
      "              Params: tensor([  5.2782, -16.7979])\n",
      "              Grad: tensor([-0.0152,  0.0863])\n",
      "Epoch: 2084, Loss: 2.9501543045043945 \n",
      "              Params: tensor([  5.2783, -16.7987])\n",
      "              Grad: tensor([-0.0152,  0.0861])\n",
      "Epoch: 2085, Loss: 2.9500784873962402 \n",
      "              Params: tensor([  5.2785, -16.7996])\n",
      "              Grad: tensor([-0.0152,  0.0860])\n",
      "Epoch: 2086, Loss: 2.950003147125244 \n",
      "              Params: tensor([  5.2786, -16.8004])\n",
      "              Grad: tensor([-0.0152,  0.0858])\n",
      "Epoch: 2087, Loss: 2.949925422668457 \n",
      "              Params: tensor([  5.2788, -16.8013])\n",
      "              Grad: tensor([-0.0152,  0.0857])\n",
      "Epoch: 2088, Loss: 2.949849843978882 \n",
      "              Params: tensor([  5.2789, -16.8021])\n",
      "              Grad: tensor([-0.0151,  0.0855])\n",
      "Epoch: 2089, Loss: 2.9497756958007812 \n",
      "              Params: tensor([  5.2791, -16.8030])\n",
      "              Grad: tensor([-0.0151,  0.0854])\n",
      "Epoch: 2090, Loss: 2.9496991634368896 \n",
      "              Params: tensor([  5.2792, -16.8039])\n",
      "              Grad: tensor([-0.0151,  0.0852])\n",
      "Epoch: 2091, Loss: 2.9496259689331055 \n",
      "              Params: tensor([  5.2794, -16.8047])\n",
      "              Grad: tensor([-0.0150,  0.0851])\n",
      "Epoch: 2092, Loss: 2.9495503902435303 \n",
      "              Params: tensor([  5.2795, -16.8056])\n",
      "              Grad: tensor([-0.0150,  0.0850])\n",
      "Epoch: 2093, Loss: 2.9494760036468506 \n",
      "              Params: tensor([  5.2797, -16.8064])\n",
      "              Grad: tensor([-0.0150,  0.0848])\n",
      "Epoch: 2094, Loss: 2.9494011402130127 \n",
      "              Params: tensor([  5.2798, -16.8072])\n",
      "              Grad: tensor([-0.0149,  0.0847])\n",
      "Epoch: 2095, Loss: 2.9493277072906494 \n",
      "              Params: tensor([  5.2800, -16.8081])\n",
      "              Grad: tensor([-0.0150,  0.0845])\n",
      "Epoch: 2096, Loss: 2.949253797531128 \n",
      "              Params: tensor([  5.2801, -16.8089])\n",
      "              Grad: tensor([-0.0149,  0.0844])\n",
      "Epoch: 2097, Loss: 2.94918155670166 \n",
      "              Params: tensor([  5.2803, -16.8098])\n",
      "              Grad: tensor([-0.0149,  0.0842])\n",
      "Epoch: 2098, Loss: 2.949108123779297 \n",
      "              Params: tensor([  5.2804, -16.8106])\n",
      "              Grad: tensor([-0.0149,  0.0841])\n",
      "Epoch: 2099, Loss: 2.949035406112671 \n",
      "              Params: tensor([  5.2806, -16.8115])\n",
      "              Grad: tensor([-0.0148,  0.0839])\n",
      "Epoch: 2100, Loss: 2.9489617347717285 \n",
      "              Params: tensor([  5.2807, -16.8123])\n",
      "              Grad: tensor([-0.0148,  0.0838])\n",
      "Epoch: 2101, Loss: 2.948889970779419 \n",
      "              Params: tensor([  5.2809, -16.8131])\n",
      "              Grad: tensor([-0.0148,  0.0837])\n",
      "Epoch: 2102, Loss: 2.9488184452056885 \n",
      "              Params: tensor([  5.2810, -16.8140])\n",
      "              Grad: tensor([-0.0148,  0.0835])\n",
      "Epoch: 2103, Loss: 2.9487454891204834 \n",
      "              Params: tensor([  5.2812, -16.8148])\n",
      "              Grad: tensor([-0.0148,  0.0834])\n",
      "Epoch: 2104, Loss: 2.9486746788024902 \n",
      "              Params: tensor([  5.2813, -16.8156])\n",
      "              Grad: tensor([-0.0147,  0.0832])\n",
      "Epoch: 2105, Loss: 2.9486021995544434 \n",
      "              Params: tensor([  5.2815, -16.8165])\n",
      "              Grad: tensor([-0.0147,  0.0831])\n",
      "Epoch: 2106, Loss: 2.9485321044921875 \n",
      "              Params: tensor([  5.2816, -16.8173])\n",
      "              Grad: tensor([-0.0146,  0.0830])\n",
      "Epoch: 2107, Loss: 2.94846248626709 \n",
      "              Params: tensor([  5.2817, -16.8181])\n",
      "              Grad: tensor([-0.0146,  0.0828])\n",
      "Epoch: 2108, Loss: 2.9483907222747803 \n",
      "              Params: tensor([  5.2819, -16.8189])\n",
      "              Grad: tensor([-0.0146,  0.0827])\n",
      "Epoch: 2109, Loss: 2.9483213424682617 \n",
      "              Params: tensor([  5.2820, -16.8198])\n",
      "              Grad: tensor([-0.0146,  0.0825])\n",
      "Epoch: 2110, Loss: 2.9482500553131104 \n",
      "              Params: tensor([  5.2822, -16.8206])\n",
      "              Grad: tensor([-0.0145,  0.0824])\n",
      "Epoch: 2111, Loss: 2.948180913925171 \n",
      "              Params: tensor([  5.2823, -16.8214])\n",
      "              Grad: tensor([-0.0145,  0.0823])\n",
      "Epoch: 2112, Loss: 2.9481089115142822 \n",
      "              Params: tensor([  5.2825, -16.8222])\n",
      "              Grad: tensor([-0.0145,  0.0821])\n",
      "Epoch: 2113, Loss: 2.948040723800659 \n",
      "              Params: tensor([  5.2826, -16.8231])\n",
      "              Grad: tensor([-0.0145,  0.0820])\n",
      "Epoch: 2114, Loss: 2.9479713439941406 \n",
      "              Params: tensor([  5.2828, -16.8239])\n",
      "              Grad: tensor([-0.0144,  0.0818])\n",
      "Epoch: 2115, Loss: 2.947901725769043 \n",
      "              Params: tensor([  5.2829, -16.8247])\n",
      "              Grad: tensor([-0.0144,  0.0817])\n",
      "Epoch: 2116, Loss: 2.9478330612182617 \n",
      "              Params: tensor([  5.2831, -16.8255])\n",
      "              Grad: tensor([-0.0144,  0.0816])\n",
      "Epoch: 2117, Loss: 2.9477646350860596 \n",
      "              Params: tensor([  5.2832, -16.8263])\n",
      "              Grad: tensor([-0.0144,  0.0814])\n",
      "Epoch: 2118, Loss: 2.9476959705352783 \n",
      "              Params: tensor([  5.2833, -16.8271])\n",
      "              Grad: tensor([-0.0144,  0.0813])\n",
      "Epoch: 2119, Loss: 2.9476282596588135 \n",
      "              Params: tensor([  5.2835, -16.8280])\n",
      "              Grad: tensor([-0.0143,  0.0811])\n",
      "Epoch: 2120, Loss: 2.9475603103637695 \n",
      "              Params: tensor([  5.2836, -16.8288])\n",
      "              Grad: tensor([-0.0143,  0.0810])\n",
      "Epoch: 2121, Loss: 2.9474942684173584 \n",
      "              Params: tensor([  5.2838, -16.8296])\n",
      "              Grad: tensor([-0.0143,  0.0809])\n",
      "Epoch: 2122, Loss: 2.9474258422851562 \n",
      "              Params: tensor([  5.2839, -16.8304])\n",
      "              Grad: tensor([-0.0143,  0.0807])\n",
      "Epoch: 2123, Loss: 2.947357416152954 \n",
      "              Params: tensor([  5.2841, -16.8312])\n",
      "              Grad: tensor([-0.0142,  0.0806])\n",
      "Epoch: 2124, Loss: 2.947293281555176 \n",
      "              Params: tensor([  5.2842, -16.8320])\n",
      "              Grad: tensor([-0.0142,  0.0805])\n",
      "Epoch: 2125, Loss: 2.9472250938415527 \n",
      "              Params: tensor([  5.2843, -16.8328])\n",
      "              Grad: tensor([-0.0142,  0.0803])\n",
      "Epoch: 2126, Loss: 2.947158098220825 \n",
      "              Params: tensor([  5.2845, -16.8336])\n",
      "              Grad: tensor([-0.0142,  0.0802])\n",
      "Epoch: 2127, Loss: 2.947091579437256 \n",
      "              Params: tensor([  5.2846, -16.8344])\n",
      "              Grad: tensor([-0.0141,  0.0800])\n",
      "Epoch: 2128, Loss: 2.947026252746582 \n",
      "              Params: tensor([  5.2848, -16.8352])\n",
      "              Grad: tensor([-0.0141,  0.0799])\n",
      "Epoch: 2129, Loss: 2.94696044921875 \n",
      "              Params: tensor([  5.2849, -16.8360])\n",
      "              Grad: tensor([-0.0141,  0.0798])\n",
      "Epoch: 2130, Loss: 2.946894645690918 \n",
      "              Params: tensor([  5.2850, -16.8368])\n",
      "              Grad: tensor([-0.0141,  0.0796])\n",
      "Epoch: 2131, Loss: 2.9468297958374023 \n",
      "              Params: tensor([  5.2852, -16.8376])\n",
      "              Grad: tensor([-0.0141,  0.0795])\n",
      "Epoch: 2132, Loss: 2.9467644691467285 \n",
      "              Params: tensor([  5.2853, -16.8384])\n",
      "              Grad: tensor([-0.0140,  0.0794])\n",
      "Epoch: 2133, Loss: 2.946699619293213 \n",
      "              Params: tensor([  5.2855, -16.8392])\n",
      "              Grad: tensor([-0.0140,  0.0792])\n",
      "Epoch: 2134, Loss: 2.9466352462768555 \n",
      "              Params: tensor([  5.2856, -16.8400])\n",
      "              Grad: tensor([-0.0140,  0.0791])\n",
      "Epoch: 2135, Loss: 2.946570634841919 \n",
      "              Params: tensor([  5.2857, -16.8407])\n",
      "              Grad: tensor([-0.0139,  0.0790])\n",
      "Epoch: 2136, Loss: 2.9465065002441406 \n",
      "              Params: tensor([  5.2859, -16.8415])\n",
      "              Grad: tensor([-0.0139,  0.0788])\n",
      "Epoch: 2137, Loss: 2.946441888809204 \n",
      "              Params: tensor([  5.2860, -16.8423])\n",
      "              Grad: tensor([-0.0139,  0.0787])\n",
      "Epoch: 2138, Loss: 2.946377754211426 \n",
      "              Params: tensor([  5.2862, -16.8431])\n",
      "              Grad: tensor([-0.0139,  0.0786])\n",
      "Epoch: 2139, Loss: 2.9463140964508057 \n",
      "              Params: tensor([  5.2863, -16.8439])\n",
      "              Grad: tensor([-0.0138,  0.0784])\n",
      "Epoch: 2140, Loss: 2.946251153945923 \n",
      "              Params: tensor([  5.2864, -16.8447])\n",
      "              Grad: tensor([-0.0138,  0.0783])\n",
      "Epoch: 2141, Loss: 2.9461889266967773 \n",
      "              Params: tensor([  5.2866, -16.8455])\n",
      "              Grad: tensor([-0.0138,  0.0782])\n",
      "Epoch: 2142, Loss: 2.9461257457733154 \n",
      "              Params: tensor([  5.2867, -16.8462])\n",
      "              Grad: tensor([-0.0138,  0.0780])\n",
      "Epoch: 2143, Loss: 2.9460630416870117 \n",
      "              Params: tensor([  5.2869, -16.8470])\n",
      "              Grad: tensor([-0.0138,  0.0779])\n",
      "Epoch: 2144, Loss: 2.946000814437866 \n",
      "              Params: tensor([  5.2870, -16.8478])\n",
      "              Grad: tensor([-0.0137,  0.0778])\n",
      "Epoch: 2145, Loss: 2.945937395095825 \n",
      "              Params: tensor([  5.2871, -16.8486])\n",
      "              Grad: tensor([-0.0137,  0.0776])\n",
      "Epoch: 2146, Loss: 2.945876121520996 \n",
      "              Params: tensor([  5.2873, -16.8493])\n",
      "              Grad: tensor([-0.0137,  0.0775])\n",
      "Epoch: 2147, Loss: 2.945814609527588 \n",
      "              Params: tensor([  5.2874, -16.8501])\n",
      "              Grad: tensor([-0.0137,  0.0774])\n",
      "Epoch: 2148, Loss: 2.9457528591156006 \n",
      "              Params: tensor([  5.2875, -16.8509])\n",
      "              Grad: tensor([-0.0136,  0.0772])\n",
      "Epoch: 2149, Loss: 2.945690155029297 \n",
      "              Params: tensor([  5.2877, -16.8517])\n",
      "              Grad: tensor([-0.0136,  0.0771])\n",
      "Epoch: 2150, Loss: 2.9456300735473633 \n",
      "              Params: tensor([  5.2878, -16.8524])\n",
      "              Grad: tensor([-0.0136,  0.0770])\n",
      "Epoch: 2151, Loss: 2.9455673694610596 \n",
      "              Params: tensor([  5.2879, -16.8532])\n",
      "              Grad: tensor([-0.0136,  0.0768])\n",
      "Epoch: 2152, Loss: 2.9455084800720215 \n",
      "              Params: tensor([  5.2881, -16.8540])\n",
      "              Grad: tensor([-0.0135,  0.0767])\n",
      "Epoch: 2153, Loss: 2.9454474449157715 \n",
      "              Params: tensor([  5.2882, -16.8547])\n",
      "              Grad: tensor([-0.0135,  0.0766])\n",
      "Epoch: 2154, Loss: 2.945385217666626 \n",
      "              Params: tensor([  5.2884, -16.8555])\n",
      "              Grad: tensor([-0.0135,  0.0765])\n",
      "Epoch: 2155, Loss: 2.9453253746032715 \n",
      "              Params: tensor([  5.2885, -16.8563])\n",
      "              Grad: tensor([-0.0135,  0.0763])\n",
      "Epoch: 2156, Loss: 2.9452669620513916 \n",
      "              Params: tensor([  5.2886, -16.8570])\n",
      "              Grad: tensor([-0.0135,  0.0762])\n",
      "Epoch: 2157, Loss: 2.9452064037323 \n",
      "              Params: tensor([  5.2888, -16.8578])\n",
      "              Grad: tensor([-0.0134,  0.0761])\n",
      "Epoch: 2158, Loss: 2.945146083831787 \n",
      "              Params: tensor([  5.2889, -16.8585])\n",
      "              Grad: tensor([-0.0134,  0.0759])\n",
      "Epoch: 2159, Loss: 2.9450876712799072 \n",
      "              Params: tensor([  5.2890, -16.8593])\n",
      "              Grad: tensor([-0.0134,  0.0758])\n",
      "Epoch: 2160, Loss: 2.9450278282165527 \n",
      "              Params: tensor([  5.2892, -16.8601])\n",
      "              Grad: tensor([-0.0134,  0.0757])\n",
      "Epoch: 2161, Loss: 2.944969415664673 \n",
      "              Params: tensor([  5.2893, -16.8608])\n",
      "              Grad: tensor([-0.0133,  0.0755])\n",
      "Epoch: 2162, Loss: 2.944911003112793 \n",
      "              Params: tensor([  5.2894, -16.8616])\n",
      "              Grad: tensor([-0.0133,  0.0754])\n",
      "Epoch: 2163, Loss: 2.944852352142334 \n",
      "              Params: tensor([  5.2896, -16.8623])\n",
      "              Grad: tensor([-0.0133,  0.0753])\n",
      "Epoch: 2164, Loss: 2.9447922706604004 \n",
      "              Params: tensor([  5.2897, -16.8631])\n",
      "              Grad: tensor([-0.0133,  0.0752])\n",
      "Epoch: 2165, Loss: 2.9447362422943115 \n",
      "              Params: tensor([  5.2898, -16.8638])\n",
      "              Grad: tensor([-0.0133,  0.0750])\n",
      "Epoch: 2166, Loss: 2.9446775913238525 \n",
      "              Params: tensor([  5.2900, -16.8646])\n",
      "              Grad: tensor([-0.0132,  0.0749])\n",
      "Epoch: 2167, Loss: 2.9446191787719727 \n",
      "              Params: tensor([  5.2901, -16.8653])\n",
      "              Grad: tensor([-0.0132,  0.0748])\n",
      "Epoch: 2168, Loss: 2.944561719894409 \n",
      "              Params: tensor([  5.2902, -16.8661])\n",
      "              Grad: tensor([-0.0132,  0.0747])\n",
      "Epoch: 2169, Loss: 2.9445037841796875 \n",
      "              Params: tensor([  5.2903, -16.8668])\n",
      "              Grad: tensor([-0.0132,  0.0745])\n",
      "Epoch: 2170, Loss: 2.9444468021392822 \n",
      "              Params: tensor([  5.2905, -16.8676])\n",
      "              Grad: tensor([-0.0132,  0.0744])\n",
      "Epoch: 2171, Loss: 2.9443907737731934 \n",
      "              Params: tensor([  5.2906, -16.8683])\n",
      "              Grad: tensor([-0.0131,  0.0743])\n",
      "Epoch: 2172, Loss: 2.9443323612213135 \n",
      "              Params: tensor([  5.2907, -16.8690])\n",
      "              Grad: tensor([-0.0131,  0.0742])\n",
      "Epoch: 2173, Loss: 2.9442763328552246 \n",
      "              Params: tensor([  5.2909, -16.8698])\n",
      "              Grad: tensor([-0.0131,  0.0740])\n",
      "Epoch: 2174, Loss: 2.9442200660705566 \n",
      "              Params: tensor([  5.2910, -16.8705])\n",
      "              Grad: tensor([-0.0131,  0.0739])\n",
      "Epoch: 2175, Loss: 2.9441640377044678 \n",
      "              Params: tensor([  5.2911, -16.8713])\n",
      "              Grad: tensor([-0.0130,  0.0738])\n",
      "Epoch: 2176, Loss: 2.9441077709198 \n",
      "              Params: tensor([  5.2913, -16.8720])\n",
      "              Grad: tensor([-0.0130,  0.0736])\n",
      "Epoch: 2177, Loss: 2.9440526962280273 \n",
      "              Params: tensor([  5.2914, -16.8727])\n",
      "              Grad: tensor([-0.0130,  0.0735])\n",
      "Epoch: 2178, Loss: 2.9439961910247803 \n",
      "              Params: tensor([  5.2915, -16.8735])\n",
      "              Grad: tensor([-0.0130,  0.0734])\n",
      "Epoch: 2179, Loss: 2.9439408779144287 \n",
      "              Params: tensor([  5.2917, -16.8742])\n",
      "              Grad: tensor([-0.0129,  0.0733])\n",
      "Epoch: 2180, Loss: 2.9438865184783936 \n",
      "              Params: tensor([  5.2918, -16.8749])\n",
      "              Grad: tensor([-0.0129,  0.0731])\n",
      "Epoch: 2181, Loss: 2.943830728530884 \n",
      "              Params: tensor([  5.2919, -16.8757])\n",
      "              Grad: tensor([-0.0129,  0.0730])\n",
      "Epoch: 2182, Loss: 2.9437758922576904 \n",
      "              Params: tensor([  5.2920, -16.8764])\n",
      "              Grad: tensor([-0.0129,  0.0729])\n",
      "Epoch: 2183, Loss: 2.943721055984497 \n",
      "              Params: tensor([  5.2922, -16.8771])\n",
      "              Grad: tensor([-0.0129,  0.0728])\n",
      "Epoch: 2184, Loss: 2.9436662197113037 \n",
      "              Params: tensor([  5.2923, -16.8778])\n",
      "              Grad: tensor([-0.0128,  0.0727])\n",
      "Epoch: 2185, Loss: 2.943612575531006 \n",
      "              Params: tensor([  5.2924, -16.8786])\n",
      "              Grad: tensor([-0.0128,  0.0725])\n",
      "Epoch: 2186, Loss: 2.9435575008392334 \n",
      "              Params: tensor([  5.2926, -16.8793])\n",
      "              Grad: tensor([-0.0128,  0.0724])\n",
      "Epoch: 2187, Loss: 2.9435033798217773 \n",
      "              Params: tensor([  5.2927, -16.8800])\n",
      "              Grad: tensor([-0.0128,  0.0723])\n",
      "Epoch: 2188, Loss: 2.943450689315796 \n",
      "              Params: tensor([  5.2928, -16.8807])\n",
      "              Grad: tensor([-0.0127,  0.0722])\n",
      "Epoch: 2189, Loss: 2.9433951377868652 \n",
      "              Params: tensor([  5.2929, -16.8815])\n",
      "              Grad: tensor([-0.0127,  0.0720])\n",
      "Epoch: 2190, Loss: 2.943342685699463 \n",
      "              Params: tensor([  5.2931, -16.8822])\n",
      "              Grad: tensor([-0.0127,  0.0719])\n",
      "Epoch: 2191, Loss: 2.9432904720306396 \n",
      "              Params: tensor([  5.2932, -16.8829])\n",
      "              Grad: tensor([-0.0127,  0.0718])\n",
      "Epoch: 2192, Loss: 2.943234920501709 \n",
      "              Params: tensor([  5.2933, -16.8836])\n",
      "              Grad: tensor([-0.0127,  0.0717])\n",
      "Epoch: 2193, Loss: 2.943183183670044 \n",
      "              Params: tensor([  5.2934, -16.8843])\n",
      "              Grad: tensor([-0.0126,  0.0715])\n",
      "Epoch: 2194, Loss: 2.9431302547454834 \n",
      "              Params: tensor([  5.2936, -16.8850])\n",
      "              Grad: tensor([-0.0126,  0.0714])\n",
      "Epoch: 2195, Loss: 2.9430794715881348 \n",
      "              Params: tensor([  5.2937, -16.8857])\n",
      "              Grad: tensor([-0.0126,  0.0713])\n",
      "Epoch: 2196, Loss: 2.943026542663574 \n",
      "              Params: tensor([  5.2938, -16.8865])\n",
      "              Grad: tensor([-0.0126,  0.0712])\n",
      "Epoch: 2197, Loss: 2.9429733753204346 \n",
      "              Params: tensor([  5.2939, -16.8872])\n",
      "              Grad: tensor([-0.0126,  0.0711])\n",
      "Epoch: 2198, Loss: 2.9429221153259277 \n",
      "              Params: tensor([  5.2941, -16.8879])\n",
      "              Grad: tensor([-0.0125,  0.0709])\n",
      "Epoch: 2199, Loss: 2.9428696632385254 \n",
      "              Params: tensor([  5.2942, -16.8886])\n",
      "              Grad: tensor([-0.0125,  0.0708])\n",
      "Epoch: 2200, Loss: 2.9428184032440186 \n",
      "              Params: tensor([  5.2943, -16.8893])\n",
      "              Grad: tensor([-0.0125,  0.0707])\n",
      "Epoch: 2201, Loss: 2.942765712738037 \n",
      "              Params: tensor([  5.2944, -16.8900])\n",
      "              Grad: tensor([-0.0125,  0.0706])\n",
      "Epoch: 2202, Loss: 2.9427144527435303 \n",
      "              Params: tensor([  5.2946, -16.8907])\n",
      "              Grad: tensor([-0.0124,  0.0705])\n",
      "Epoch: 2203, Loss: 2.942664861679077 \n",
      "              Params: tensor([  5.2947, -16.8914])\n",
      "              Grad: tensor([-0.0124,  0.0703])\n",
      "Epoch: 2204, Loss: 2.942612409591675 \n",
      "              Params: tensor([  5.2948, -16.8921])\n",
      "              Grad: tensor([-0.0124,  0.0702])\n",
      "Epoch: 2205, Loss: 2.942563533782959 \n",
      "              Params: tensor([  5.2949, -16.8928])\n",
      "              Grad: tensor([-0.0124,  0.0701])\n",
      "Epoch: 2206, Loss: 2.9425103664398193 \n",
      "              Params: tensor([  5.2951, -16.8935])\n",
      "              Grad: tensor([-0.0124,  0.0700])\n",
      "Epoch: 2207, Loss: 2.9424614906311035 \n",
      "              Params: tensor([  5.2952, -16.8942])\n",
      "              Grad: tensor([-0.0123,  0.0699])\n",
      "Epoch: 2208, Loss: 2.942411184310913 \n",
      "              Params: tensor([  5.2953, -16.8949])\n",
      "              Grad: tensor([-0.0123,  0.0697])\n",
      "Epoch: 2209, Loss: 2.9423606395721436 \n",
      "              Params: tensor([  5.2954, -16.8956])\n",
      "              Grad: tensor([-0.0123,  0.0696])\n",
      "Epoch: 2210, Loss: 2.942310333251953 \n",
      "              Params: tensor([  5.2956, -16.8963])\n",
      "              Grad: tensor([-0.0123,  0.0695])\n",
      "Epoch: 2211, Loss: 2.942260503768921 \n",
      "              Params: tensor([  5.2957, -16.8970])\n",
      "              Grad: tensor([-0.0122,  0.0694])\n",
      "Epoch: 2212, Loss: 2.942211389541626 \n",
      "              Params: tensor([  5.2958, -16.8977])\n",
      "              Grad: tensor([-0.0122,  0.0693])\n",
      "Epoch: 2213, Loss: 2.9421615600585938 \n",
      "              Params: tensor([  5.2959, -16.8984])\n",
      "              Grad: tensor([-0.0122,  0.0692])\n",
      "Epoch: 2214, Loss: 2.9421119689941406 \n",
      "              Params: tensor([  5.2960, -16.8991])\n",
      "              Grad: tensor([-0.0122,  0.0690])\n",
      "Epoch: 2215, Loss: 2.9420621395111084 \n",
      "              Params: tensor([  5.2962, -16.8998])\n",
      "              Grad: tensor([-0.0122,  0.0689])\n",
      "Epoch: 2216, Loss: 2.942014217376709 \n",
      "              Params: tensor([  5.2963, -16.9004])\n",
      "              Grad: tensor([-0.0122,  0.0688])\n",
      "Epoch: 2217, Loss: 2.941964626312256 \n",
      "              Params: tensor([  5.2964, -16.9011])\n",
      "              Grad: tensor([-0.0121,  0.0687])\n",
      "Epoch: 2218, Loss: 2.941917657852173 \n",
      "              Params: tensor([  5.2965, -16.9018])\n",
      "              Grad: tensor([-0.0121,  0.0686])\n",
      "Epoch: 2219, Loss: 2.9418678283691406 \n",
      "              Params: tensor([  5.2967, -16.9025])\n",
      "              Grad: tensor([-0.0121,  0.0685])\n",
      "Epoch: 2220, Loss: 2.9418210983276367 \n",
      "              Params: tensor([  5.2968, -16.9032])\n",
      "              Grad: tensor([-0.0121,  0.0683])\n",
      "Epoch: 2221, Loss: 2.9417734146118164 \n",
      "              Params: tensor([  5.2969, -16.9039])\n",
      "              Grad: tensor([-0.0120,  0.0682])\n",
      "Epoch: 2222, Loss: 2.9417243003845215 \n",
      "              Params: tensor([  5.2970, -16.9046])\n",
      "              Grad: tensor([-0.0120,  0.0681])\n",
      "Epoch: 2223, Loss: 2.9416770935058594 \n",
      "              Params: tensor([  5.2971, -16.9052])\n",
      "              Grad: tensor([-0.0120,  0.0680])\n",
      "Epoch: 2224, Loss: 2.941628932952881 \n",
      "              Params: tensor([  5.2973, -16.9059])\n",
      "              Grad: tensor([-0.0120,  0.0679])\n",
      "Epoch: 2225, Loss: 2.9415817260742188 \n",
      "              Params: tensor([  5.2974, -16.9066])\n",
      "              Grad: tensor([-0.0120,  0.0678])\n",
      "Epoch: 2226, Loss: 2.9415342807769775 \n",
      "              Params: tensor([  5.2975, -16.9073])\n",
      "              Grad: tensor([-0.0119,  0.0676])\n",
      "Epoch: 2227, Loss: 2.9414877891540527 \n",
      "              Params: tensor([  5.2976, -16.9079])\n",
      "              Grad: tensor([-0.0119,  0.0675])\n",
      "Epoch: 2228, Loss: 2.9414398670196533 \n",
      "              Params: tensor([  5.2977, -16.9086])\n",
      "              Grad: tensor([-0.0119,  0.0674])\n",
      "Epoch: 2229, Loss: 2.941392660140991 \n",
      "              Params: tensor([  5.2979, -16.9093])\n",
      "              Grad: tensor([-0.0119,  0.0673])\n",
      "Epoch: 2230, Loss: 2.9413461685180664 \n",
      "              Params: tensor([  5.2980, -16.9100])\n",
      "              Grad: tensor([-0.0119,  0.0672])\n",
      "Epoch: 2231, Loss: 2.941298723220825 \n",
      "              Params: tensor([  5.2981, -16.9106])\n",
      "              Grad: tensor([-0.0118,  0.0671])\n",
      "Epoch: 2232, Loss: 2.9412527084350586 \n",
      "              Params: tensor([  5.2982, -16.9113])\n",
      "              Grad: tensor([-0.0118,  0.0670])\n",
      "Epoch: 2233, Loss: 2.941206455230713 \n",
      "              Params: tensor([  5.2983, -16.9120])\n",
      "              Grad: tensor([-0.0118,  0.0668])\n",
      "Epoch: 2234, Loss: 2.9411628246307373 \n",
      "              Params: tensor([  5.2984, -16.9126])\n",
      "              Grad: tensor([-0.0118,  0.0667])\n",
      "Epoch: 2235, Loss: 2.941115617752075 \n",
      "              Params: tensor([  5.2986, -16.9133])\n",
      "              Grad: tensor([-0.0118,  0.0666])\n",
      "Epoch: 2236, Loss: 2.9410698413848877 \n",
      "              Params: tensor([  5.2987, -16.9140])\n",
      "              Grad: tensor([-0.0117,  0.0665])\n",
      "Epoch: 2237, Loss: 2.9410247802734375 \n",
      "              Params: tensor([  5.2988, -16.9146])\n",
      "              Grad: tensor([-0.0117,  0.0664])\n",
      "Epoch: 2238, Loss: 2.940978765487671 \n",
      "              Params: tensor([  5.2989, -16.9153])\n",
      "              Grad: tensor([-0.0117,  0.0663])\n",
      "Epoch: 2239, Loss: 2.9409332275390625 \n",
      "              Params: tensor([  5.2990, -16.9160])\n",
      "              Grad: tensor([-0.0117,  0.0662])\n",
      "Epoch: 2240, Loss: 2.940889596939087 \n",
      "              Params: tensor([  5.2991, -16.9166])\n",
      "              Grad: tensor([-0.0117,  0.0661])\n",
      "Epoch: 2241, Loss: 2.9408440589904785 \n",
      "              Params: tensor([  5.2993, -16.9173])\n",
      "              Grad: tensor([-0.0117,  0.0659])\n",
      "Epoch: 2242, Loss: 2.940798282623291 \n",
      "              Params: tensor([  5.2994, -16.9179])\n",
      "              Grad: tensor([-0.0116,  0.0658])\n",
      "Epoch: 2243, Loss: 2.940753221511841 \n",
      "              Params: tensor([  5.2995, -16.9186])\n",
      "              Grad: tensor([-0.0116,  0.0657])\n",
      "Epoch: 2244, Loss: 2.9407105445861816 \n",
      "              Params: tensor([  5.2996, -16.9192])\n",
      "              Grad: tensor([-0.0116,  0.0656])\n",
      "Epoch: 2245, Loss: 2.9406661987304688 \n",
      "              Params: tensor([  5.2997, -16.9199])\n",
      "              Grad: tensor([-0.0116,  0.0655])\n",
      "Epoch: 2246, Loss: 2.9406211376190186 \n",
      "              Params: tensor([  5.2998, -16.9206])\n",
      "              Grad: tensor([-0.0115,  0.0654])\n",
      "Epoch: 2247, Loss: 2.9405760765075684 \n",
      "              Params: tensor([  5.3000, -16.9212])\n",
      "              Grad: tensor([-0.0115,  0.0653])\n",
      "Epoch: 2248, Loss: 2.940532922744751 \n",
      "              Params: tensor([  5.3001, -16.9219])\n",
      "              Grad: tensor([-0.0115,  0.0652])\n",
      "Epoch: 2249, Loss: 2.9404892921447754 \n",
      "              Params: tensor([  5.3002, -16.9225])\n",
      "              Grad: tensor([-0.0115,  0.0650])\n",
      "Epoch: 2250, Loss: 2.9404456615448 \n",
      "              Params: tensor([  5.3003, -16.9232])\n",
      "              Grad: tensor([-0.0115,  0.0649])\n",
      "Epoch: 2251, Loss: 2.9404027462005615 \n",
      "              Params: tensor([  5.3004, -16.9238])\n",
      "              Grad: tensor([-0.0114,  0.0648])\n",
      "Epoch: 2252, Loss: 2.9403581619262695 \n",
      "              Params: tensor([  5.3005, -16.9245])\n",
      "              Grad: tensor([-0.0114,  0.0647])\n",
      "Epoch: 2253, Loss: 2.9403159618377686 \n",
      "              Params: tensor([  5.3006, -16.9251])\n",
      "              Grad: tensor([-0.0114,  0.0646])\n",
      "Epoch: 2254, Loss: 2.940274238586426 \n",
      "              Params: tensor([  5.3008, -16.9257])\n",
      "              Grad: tensor([-0.0114,  0.0645])\n",
      "Epoch: 2255, Loss: 2.9402289390563965 \n",
      "              Params: tensor([  5.3009, -16.9264])\n",
      "              Grad: tensor([-0.0114,  0.0644])\n",
      "Epoch: 2256, Loss: 2.940187692642212 \n",
      "              Params: tensor([  5.3010, -16.9270])\n",
      "              Grad: tensor([-0.0114,  0.0643])\n",
      "Epoch: 2257, Loss: 2.940143585205078 \n",
      "              Params: tensor([  5.3011, -16.9277])\n",
      "              Grad: tensor([-0.0114,  0.0642])\n",
      "Epoch: 2258, Loss: 2.9401016235351562 \n",
      "              Params: tensor([  5.3012, -16.9283])\n",
      "              Grad: tensor([-0.0113,  0.0641])\n",
      "Epoch: 2259, Loss: 2.9400599002838135 \n",
      "              Params: tensor([  5.3013, -16.9290])\n",
      "              Grad: tensor([-0.0113,  0.0640])\n",
      "Epoch: 2260, Loss: 2.94001841545105 \n",
      "              Params: tensor([  5.3014, -16.9296])\n",
      "              Grad: tensor([-0.0113,  0.0638])\n",
      "Epoch: 2261, Loss: 2.9399771690368652 \n",
      "              Params: tensor([  5.3016, -16.9302])\n",
      "              Grad: tensor([-0.0113,  0.0637])\n",
      "Epoch: 2262, Loss: 2.9399335384368896 \n",
      "              Params: tensor([  5.3017, -16.9309])\n",
      "              Grad: tensor([-0.0112,  0.0636])\n",
      "Epoch: 2263, Loss: 2.9398910999298096 \n",
      "              Params: tensor([  5.3018, -16.9315])\n",
      "              Grad: tensor([-0.0112,  0.0635])\n",
      "Epoch: 2264, Loss: 2.9398512840270996 \n",
      "              Params: tensor([  5.3019, -16.9321])\n",
      "              Grad: tensor([-0.0112,  0.0634])\n",
      "Epoch: 2265, Loss: 2.9398093223571777 \n",
      "              Params: tensor([  5.3020, -16.9328])\n",
      "              Grad: tensor([-0.0112,  0.0633])\n",
      "Epoch: 2266, Loss: 2.9397695064544678 \n",
      "              Params: tensor([  5.3021, -16.9334])\n",
      "              Grad: tensor([-0.0112,  0.0632])\n",
      "Epoch: 2267, Loss: 2.9397268295288086 \n",
      "              Params: tensor([  5.3022, -16.9340])\n",
      "              Grad: tensor([-0.0111,  0.0631])\n",
      "Epoch: 2268, Loss: 2.939685583114624 \n",
      "              Params: tensor([  5.3023, -16.9347])\n",
      "              Grad: tensor([-0.0111,  0.0630])\n",
      "Epoch: 2269, Loss: 2.939645528793335 \n",
      "              Params: tensor([  5.3024, -16.9353])\n",
      "              Grad: tensor([-0.0111,  0.0629])\n",
      "Epoch: 2270, Loss: 2.9396049976348877 \n",
      "              Params: tensor([  5.3026, -16.9359])\n",
      "              Grad: tensor([-0.0111,  0.0628])\n",
      "Epoch: 2271, Loss: 2.939565896987915 \n",
      "              Params: tensor([  5.3027, -16.9365])\n",
      "              Grad: tensor([-0.0111,  0.0627])\n",
      "Epoch: 2272, Loss: 2.9395220279693604 \n",
      "              Params: tensor([  5.3028, -16.9372])\n",
      "              Grad: tensor([-0.0111,  0.0626])\n",
      "Epoch: 2273, Loss: 2.9394829273223877 \n",
      "              Params: tensor([  5.3029, -16.9378])\n",
      "              Grad: tensor([-0.0110,  0.0624])\n",
      "Epoch: 2274, Loss: 2.9394431114196777 \n",
      "              Params: tensor([  5.3030, -16.9384])\n",
      "              Grad: tensor([-0.0110,  0.0623])\n",
      "Epoch: 2275, Loss: 2.9394028186798096 \n",
      "              Params: tensor([  5.3031, -16.9390])\n",
      "              Grad: tensor([-0.0110,  0.0622])\n",
      "Epoch: 2276, Loss: 2.939361333847046 \n",
      "              Params: tensor([  5.3032, -16.9397])\n",
      "              Grad: tensor([-0.0110,  0.0621])\n",
      "Epoch: 2277, Loss: 2.9393234252929688 \n",
      "              Params: tensor([  5.3033, -16.9403])\n",
      "              Grad: tensor([-0.0110,  0.0620])\n",
      "Epoch: 2278, Loss: 2.939282178878784 \n",
      "              Params: tensor([  5.3034, -16.9409])\n",
      "              Grad: tensor([-0.0109,  0.0619])\n",
      "Epoch: 2279, Loss: 2.9392430782318115 \n",
      "              Params: tensor([  5.3035, -16.9415])\n",
      "              Grad: tensor([-0.0109,  0.0618])\n",
      "Epoch: 2280, Loss: 2.9392054080963135 \n",
      "              Params: tensor([  5.3037, -16.9421])\n",
      "              Grad: tensor([-0.0109,  0.0617])\n",
      "Epoch: 2281, Loss: 2.939164638519287 \n",
      "              Params: tensor([  5.3038, -16.9428])\n",
      "              Grad: tensor([-0.0109,  0.0616])\n",
      "Epoch: 2282, Loss: 2.93912672996521 \n",
      "              Params: tensor([  5.3039, -16.9434])\n",
      "              Grad: tensor([-0.0109,  0.0615])\n",
      "Epoch: 2283, Loss: 2.9390869140625 \n",
      "              Params: tensor([  5.3040, -16.9440])\n",
      "              Grad: tensor([-0.0108,  0.0614])\n",
      "Epoch: 2284, Loss: 2.939049005508423 \n",
      "              Params: tensor([  5.3041, -16.9446])\n",
      "              Grad: tensor([-0.0108,  0.0613])\n",
      "Epoch: 2285, Loss: 2.939011335372925 \n",
      "              Params: tensor([  5.3042, -16.9452])\n",
      "              Grad: tensor([-0.0108,  0.0612])\n",
      "Epoch: 2286, Loss: 2.9389710426330566 \n",
      "              Params: tensor([  5.3043, -16.9458])\n",
      "              Grad: tensor([-0.0108,  0.0611])\n",
      "Epoch: 2287, Loss: 2.9389328956604004 \n",
      "              Params: tensor([  5.3044, -16.9464])\n",
      "              Grad: tensor([-0.0108,  0.0610])\n",
      "Epoch: 2288, Loss: 2.9388930797576904 \n",
      "              Params: tensor([  5.3045, -16.9470])\n",
      "              Grad: tensor([-0.0108,  0.0609])\n",
      "Epoch: 2289, Loss: 2.938857078552246 \n",
      "              Params: tensor([  5.3046, -16.9476])\n",
      "              Grad: tensor([-0.0107,  0.0608])\n",
      "Epoch: 2290, Loss: 2.9388198852539062 \n",
      "              Params: tensor([  5.3047, -16.9482])\n",
      "              Grad: tensor([-0.0107,  0.0607])\n",
      "Epoch: 2291, Loss: 2.938779354095459 \n",
      "              Params: tensor([  5.3048, -16.9489])\n",
      "              Grad: tensor([-0.0107,  0.0606])\n",
      "Epoch: 2292, Loss: 2.9387433528900146 \n",
      "              Params: tensor([  5.3049, -16.9495])\n",
      "              Grad: tensor([-0.0107,  0.0605])\n",
      "Epoch: 2293, Loss: 2.9387052059173584 \n",
      "              Params: tensor([  5.3051, -16.9501])\n",
      "              Grad: tensor([-0.0107,  0.0604])\n",
      "Epoch: 2294, Loss: 2.9386672973632812 \n",
      "              Params: tensor([  5.3052, -16.9507])\n",
      "              Grad: tensor([-0.0106,  0.0603])\n",
      "Epoch: 2295, Loss: 2.938628911972046 \n",
      "              Params: tensor([  5.3053, -16.9513])\n",
      "              Grad: tensor([-0.0106,  0.0602])\n",
      "Epoch: 2296, Loss: 2.9385931491851807 \n",
      "              Params: tensor([  5.3054, -16.9519])\n",
      "              Grad: tensor([-0.0106,  0.0601])\n",
      "Epoch: 2297, Loss: 2.9385550022125244 \n",
      "              Params: tensor([  5.3055, -16.9525])\n",
      "              Grad: tensor([-0.0106,  0.0600])\n",
      "Epoch: 2298, Loss: 2.938519239425659 \n",
      "              Params: tensor([  5.3056, -16.9531])\n",
      "              Grad: tensor([-0.0106,  0.0598])\n",
      "Epoch: 2299, Loss: 2.9384806156158447 \n",
      "              Params: tensor([  5.3057, -16.9537])\n",
      "              Grad: tensor([-0.0106,  0.0597])\n",
      "Epoch: 2300, Loss: 2.938443899154663 \n",
      "              Params: tensor([  5.3058, -16.9543])\n",
      "              Grad: tensor([-0.0105,  0.0596])\n",
      "Epoch: 2301, Loss: 2.938408136367798 \n",
      "              Params: tensor([  5.3059, -16.9549])\n",
      "              Grad: tensor([-0.0105,  0.0595])\n",
      "Epoch: 2302, Loss: 2.938371419906616 \n",
      "              Params: tensor([  5.3060, -16.9554])\n",
      "              Grad: tensor([-0.0105,  0.0594])\n",
      "Epoch: 2303, Loss: 2.9383347034454346 \n",
      "              Params: tensor([  5.3061, -16.9560])\n",
      "              Grad: tensor([-0.0105,  0.0593])\n",
      "Epoch: 2304, Loss: 2.9382987022399902 \n",
      "              Params: tensor([  5.3062, -16.9566])\n",
      "              Grad: tensor([-0.0105,  0.0592])\n",
      "Epoch: 2305, Loss: 2.938262701034546 \n",
      "              Params: tensor([  5.3063, -16.9572])\n",
      "              Grad: tensor([-0.0105,  0.0591])\n",
      "Epoch: 2306, Loss: 2.9382269382476807 \n",
      "              Params: tensor([  5.3064, -16.9578])\n",
      "              Grad: tensor([-0.0104,  0.0590])\n",
      "Epoch: 2307, Loss: 2.938190460205078 \n",
      "              Params: tensor([  5.3065, -16.9584])\n",
      "              Grad: tensor([-0.0104,  0.0589])\n",
      "Epoch: 2308, Loss: 2.938155174255371 \n",
      "              Params: tensor([  5.3066, -16.9590])\n",
      "              Grad: tensor([-0.0104,  0.0588])\n",
      "Epoch: 2309, Loss: 2.9381182193756104 \n",
      "              Params: tensor([  5.3067, -16.9596])\n",
      "              Grad: tensor([-0.0104,  0.0587])\n",
      "Epoch: 2310, Loss: 2.938084363937378 \n",
      "              Params: tensor([  5.3068, -16.9602])\n",
      "              Grad: tensor([-0.0104,  0.0586])\n",
      "Epoch: 2311, Loss: 2.9380486011505127 \n",
      "              Params: tensor([  5.3069, -16.9608])\n",
      "              Grad: tensor([-0.0103,  0.0585])\n",
      "Epoch: 2312, Loss: 2.9380135536193848 \n",
      "              Params: tensor([  5.3070, -16.9613])\n",
      "              Grad: tensor([-0.0103,  0.0584])\n",
      "Epoch: 2313, Loss: 2.937976837158203 \n",
      "              Params: tensor([  5.3072, -16.9619])\n",
      "              Grad: tensor([-0.0103,  0.0583])\n",
      "Epoch: 2314, Loss: 2.937943458557129 \n",
      "              Params: tensor([  5.3073, -16.9625])\n",
      "              Grad: tensor([-0.0103,  0.0582])\n",
      "Epoch: 2315, Loss: 2.937908411026001 \n",
      "              Params: tensor([  5.3074, -16.9631])\n",
      "              Grad: tensor([-0.0103,  0.0581])\n",
      "Epoch: 2316, Loss: 2.9378724098205566 \n",
      "              Params: tensor([  5.3075, -16.9637])\n",
      "              Grad: tensor([-0.0103,  0.0580])\n",
      "Epoch: 2317, Loss: 2.937838554382324 \n",
      "              Params: tensor([  5.3076, -16.9642])\n",
      "              Grad: tensor([-0.0102,  0.0580])\n",
      "Epoch: 2318, Loss: 2.9378042221069336 \n",
      "              Params: tensor([  5.3077, -16.9648])\n",
      "              Grad: tensor([-0.0102,  0.0578])\n",
      "Epoch: 2319, Loss: 2.9377686977386475 \n",
      "              Params: tensor([  5.3078, -16.9654])\n",
      "              Grad: tensor([-0.0102,  0.0578])\n",
      "Epoch: 2320, Loss: 2.937734365463257 \n",
      "              Params: tensor([  5.3079, -16.9660])\n",
      "              Grad: tensor([-0.0102,  0.0577])\n",
      "Epoch: 2321, Loss: 2.937699794769287 \n",
      "              Params: tensor([  5.3080, -16.9666])\n",
      "              Grad: tensor([-0.0102,  0.0576])\n",
      "Epoch: 2322, Loss: 2.9376649856567383 \n",
      "              Params: tensor([  5.3081, -16.9671])\n",
      "              Grad: tensor([-0.0102,  0.0575])\n",
      "Epoch: 2323, Loss: 2.9376320838928223 \n",
      "              Params: tensor([  5.3082, -16.9677])\n",
      "              Grad: tensor([-0.0101,  0.0574])\n",
      "Epoch: 2324, Loss: 2.9375975131988525 \n",
      "              Params: tensor([  5.3083, -16.9683])\n",
      "              Grad: tensor([-0.0101,  0.0573])\n",
      "Epoch: 2325, Loss: 2.9375646114349365 \n",
      "              Params: tensor([  5.3084, -16.9688])\n",
      "              Grad: tensor([-0.0101,  0.0572])\n",
      "Epoch: 2326, Loss: 2.937530755996704 \n",
      "              Params: tensor([  5.3085, -16.9694])\n",
      "              Grad: tensor([-0.0101,  0.0571])\n",
      "Epoch: 2327, Loss: 2.9374985694885254 \n",
      "              Params: tensor([  5.3086, -16.9700])\n",
      "              Grad: tensor([-0.0101,  0.0570])\n",
      "Epoch: 2328, Loss: 2.937464714050293 \n",
      "              Params: tensor([  5.3087, -16.9706])\n",
      "              Grad: tensor([-0.0101,  0.0569])\n",
      "Epoch: 2329, Loss: 2.937429666519165 \n",
      "              Params: tensor([  5.3088, -16.9711])\n",
      "              Grad: tensor([-0.0100,  0.0568])\n",
      "Epoch: 2330, Loss: 2.9373979568481445 \n",
      "              Params: tensor([  5.3089, -16.9717])\n",
      "              Grad: tensor([-0.0100,  0.0567])\n",
      "Epoch: 2331, Loss: 2.937364339828491 \n",
      "              Params: tensor([  5.3090, -16.9723])\n",
      "              Grad: tensor([-0.0100,  0.0566])\n",
      "Epoch: 2332, Loss: 2.9373323917388916 \n",
      "              Params: tensor([  5.3091, -16.9728])\n",
      "              Grad: tensor([-0.0100,  0.0565])\n",
      "Epoch: 2333, Loss: 2.937298536300659 \n",
      "              Params: tensor([  5.3092, -16.9734])\n",
      "              Grad: tensor([-0.0100,  0.0564])\n",
      "Epoch: 2334, Loss: 2.937265157699585 \n",
      "              Params: tensor([  5.3093, -16.9739])\n",
      "              Grad: tensor([-0.0100,  0.0563])\n",
      "Epoch: 2335, Loss: 2.937232255935669 \n",
      "              Params: tensor([  5.3094, -16.9745])\n",
      "              Grad: tensor([-0.0099,  0.0562])\n",
      "Epoch: 2336, Loss: 2.9372007846832275 \n",
      "              Params: tensor([  5.3095, -16.9751])\n",
      "              Grad: tensor([-0.0099,  0.0561])\n",
      "Epoch: 2337, Loss: 2.9371674060821533 \n",
      "              Params: tensor([  5.3096, -16.9756])\n",
      "              Grad: tensor([-0.0099,  0.0560])\n",
      "Epoch: 2338, Loss: 2.937134027481079 \n",
      "              Params: tensor([  5.3097, -16.9762])\n",
      "              Grad: tensor([-0.0099,  0.0559])\n",
      "Epoch: 2339, Loss: 2.9371042251586914 \n",
      "              Params: tensor([  5.3098, -16.9767])\n",
      "              Grad: tensor([-0.0099,  0.0558])\n",
      "Epoch: 2340, Loss: 2.9370713233947754 \n",
      "              Params: tensor([  5.3099, -16.9773])\n",
      "              Grad: tensor([-0.0098,  0.0557])\n",
      "Epoch: 2341, Loss: 2.937039375305176 \n",
      "              Params: tensor([  5.3100, -16.9779])\n",
      "              Grad: tensor([-0.0098,  0.0556])\n",
      "Epoch: 2342, Loss: 2.9370079040527344 \n",
      "              Params: tensor([  5.3101, -16.9784])\n",
      "              Grad: tensor([-0.0098,  0.0555])\n",
      "Epoch: 2343, Loss: 2.9369757175445557 \n",
      "              Params: tensor([  5.3102, -16.9790])\n",
      "              Grad: tensor([-0.0098,  0.0554])\n",
      "Epoch: 2344, Loss: 2.9369447231292725 \n",
      "              Params: tensor([  5.3103, -16.9795])\n",
      "              Grad: tensor([-0.0098,  0.0553])\n",
      "Epoch: 2345, Loss: 2.9369115829467773 \n",
      "              Params: tensor([  5.3104, -16.9801])\n",
      "              Grad: tensor([-0.0098,  0.0553])\n",
      "Epoch: 2346, Loss: 2.936882734298706 \n",
      "              Params: tensor([  5.3105, -16.9806])\n",
      "              Grad: tensor([-0.0097,  0.0552])\n",
      "Epoch: 2347, Loss: 2.9368505477905273 \n",
      "              Params: tensor([  5.3106, -16.9812])\n",
      "              Grad: tensor([-0.0097,  0.0551])\n",
      "Epoch: 2348, Loss: 2.936819314956665 \n",
      "              Params: tensor([  5.3107, -16.9817])\n",
      "              Grad: tensor([-0.0097,  0.0550])\n",
      "Epoch: 2349, Loss: 2.936788320541382 \n",
      "              Params: tensor([  5.3107, -16.9823])\n",
      "              Grad: tensor([-0.0097,  0.0549])\n",
      "Epoch: 2350, Loss: 2.9367570877075195 \n",
      "              Params: tensor([  5.3108, -16.9828])\n",
      "              Grad: tensor([-0.0097,  0.0548])\n",
      "Epoch: 2351, Loss: 2.936724901199341 \n",
      "              Params: tensor([  5.3109, -16.9834])\n",
      "              Grad: tensor([-0.0097,  0.0547])\n",
      "Epoch: 2352, Loss: 2.9366941452026367 \n",
      "              Params: tensor([  5.3110, -16.9839])\n",
      "              Grad: tensor([-0.0096,  0.0546])\n",
      "Epoch: 2353, Loss: 2.936664581298828 \n",
      "              Params: tensor([  5.3111, -16.9845])\n",
      "              Grad: tensor([-0.0096,  0.0545])\n",
      "Epoch: 2354, Loss: 2.936633348464966 \n",
      "              Params: tensor([  5.3112, -16.9850])\n",
      "              Grad: tensor([-0.0096,  0.0544])\n",
      "Epoch: 2355, Loss: 2.9366021156311035 \n",
      "              Params: tensor([  5.3113, -16.9856])\n",
      "              Grad: tensor([-0.0096,  0.0543])\n",
      "Epoch: 2356, Loss: 2.9365720748901367 \n",
      "              Params: tensor([  5.3114, -16.9861])\n",
      "              Grad: tensor([-0.0096,  0.0542])\n",
      "Epoch: 2357, Loss: 2.9365415573120117 \n",
      "              Params: tensor([  5.3115, -16.9866])\n",
      "              Grad: tensor([-0.0095,  0.0541])\n",
      "Epoch: 2358, Loss: 2.936511278152466 \n",
      "              Params: tensor([  5.3116, -16.9872])\n",
      "              Grad: tensor([-0.0096,  0.0540])\n",
      "Epoch: 2359, Loss: 2.936481475830078 \n",
      "              Params: tensor([  5.3117, -16.9877])\n",
      "              Grad: tensor([-0.0095,  0.0540])\n",
      "Epoch: 2360, Loss: 2.936450719833374 \n",
      "              Params: tensor([  5.3118, -16.9883])\n",
      "              Grad: tensor([-0.0095,  0.0539])\n",
      "Epoch: 2361, Loss: 2.9364211559295654 \n",
      "              Params: tensor([  5.3119, -16.9888])\n",
      "              Grad: tensor([-0.0095,  0.0538])\n",
      "Epoch: 2362, Loss: 2.936392307281494 \n",
      "              Params: tensor([  5.3120, -16.9893])\n",
      "              Grad: tensor([-0.0095,  0.0537])\n",
      "Epoch: 2363, Loss: 2.93636155128479 \n",
      "              Params: tensor([  5.3121, -16.9899])\n",
      "              Grad: tensor([-0.0094,  0.0536])\n",
      "Epoch: 2364, Loss: 2.9363324642181396 \n",
      "              Params: tensor([  5.3122, -16.9904])\n",
      "              Grad: tensor([-0.0094,  0.0535])\n",
      "Epoch: 2365, Loss: 2.9363043308258057 \n",
      "              Params: tensor([  5.3123, -16.9909])\n",
      "              Grad: tensor([-0.0094,  0.0534])\n",
      "Epoch: 2366, Loss: 2.9362738132476807 \n",
      "              Params: tensor([  5.3124, -16.9915])\n",
      "              Grad: tensor([-0.0094,  0.0533])\n",
      "Epoch: 2367, Loss: 2.9362435340881348 \n",
      "              Params: tensor([  5.3125, -16.9920])\n",
      "              Grad: tensor([-0.0094,  0.0532])\n",
      "Epoch: 2368, Loss: 2.936215877532959 \n",
      "              Params: tensor([  5.3126, -16.9925])\n",
      "              Grad: tensor([-0.0094,  0.0531])\n",
      "Epoch: 2369, Loss: 2.936187505722046 \n",
      "              Params: tensor([  5.3127, -16.9931])\n",
      "              Grad: tensor([-0.0094,  0.0530])\n",
      "Epoch: 2370, Loss: 2.9361562728881836 \n",
      "              Params: tensor([  5.3127, -16.9936])\n",
      "              Grad: tensor([-0.0094,  0.0530])\n",
      "Epoch: 2371, Loss: 2.9361283779144287 \n",
      "              Params: tensor([  5.3128, -16.9941])\n",
      "              Grad: tensor([-0.0093,  0.0529])\n",
      "Epoch: 2372, Loss: 2.9360997676849365 \n",
      "              Params: tensor([  5.3129, -16.9946])\n",
      "              Grad: tensor([-0.0093,  0.0528])\n",
      "Epoch: 2373, Loss: 2.9360716342926025 \n",
      "              Params: tensor([  5.3130, -16.9952])\n",
      "              Grad: tensor([-0.0093,  0.0527])\n",
      "Epoch: 2374, Loss: 2.936042070388794 \n",
      "              Params: tensor([  5.3131, -16.9957])\n",
      "              Grad: tensor([-0.0093,  0.0526])\n",
      "Epoch: 2375, Loss: 2.936014413833618 \n",
      "              Params: tensor([  5.3132, -16.9962])\n",
      "              Grad: tensor([-0.0093,  0.0525])\n",
      "Epoch: 2376, Loss: 2.935985803604126 \n",
      "              Params: tensor([  5.3133, -16.9967])\n",
      "              Grad: tensor([-0.0093,  0.0524])\n",
      "Epoch: 2377, Loss: 2.935957193374634 \n",
      "              Params: tensor([  5.3134, -16.9973])\n",
      "              Grad: tensor([-0.0093,  0.0523])\n",
      "Epoch: 2378, Loss: 2.9359281063079834 \n",
      "              Params: tensor([  5.3135, -16.9978])\n",
      "              Grad: tensor([-0.0092,  0.0522])\n",
      "Epoch: 2379, Loss: 2.9359006881713867 \n",
      "              Params: tensor([  5.3136, -16.9983])\n",
      "              Grad: tensor([-0.0092,  0.0522])\n",
      "Epoch: 2380, Loss: 2.935872793197632 \n",
      "              Params: tensor([  5.3137, -16.9988])\n",
      "              Grad: tensor([-0.0092,  0.0521])\n",
      "Epoch: 2381, Loss: 2.935845136642456 \n",
      "              Params: tensor([  5.3138, -16.9994])\n",
      "              Grad: tensor([-0.0092,  0.0520])\n",
      "Epoch: 2382, Loss: 2.935817003250122 \n",
      "              Params: tensor([  5.3139, -16.9999])\n",
      "              Grad: tensor([-0.0092,  0.0519])\n",
      "Epoch: 2383, Loss: 2.935788869857788 \n",
      "              Params: tensor([  5.3139, -17.0004])\n",
      "              Grad: tensor([-0.0092,  0.0518])\n",
      "Epoch: 2384, Loss: 2.9357619285583496 \n",
      "              Params: tensor([  5.3140, -17.0009])\n",
      "              Grad: tensor([-0.0092,  0.0517])\n",
      "Epoch: 2385, Loss: 2.9357335567474365 \n",
      "              Params: tensor([  5.3141, -17.0014])\n",
      "              Grad: tensor([-0.0091,  0.0516])\n",
      "Epoch: 2386, Loss: 2.9357070922851562 \n",
      "              Params: tensor([  5.3142, -17.0019])\n",
      "              Grad: tensor([-0.0091,  0.0515])\n",
      "Epoch: 2387, Loss: 2.935678720474243 \n",
      "              Params: tensor([  5.3143, -17.0025])\n",
      "              Grad: tensor([-0.0091,  0.0514])\n",
      "Epoch: 2388, Loss: 2.935650110244751 \n",
      "              Params: tensor([  5.3144, -17.0030])\n",
      "              Grad: tensor([-0.0091,  0.0514])\n",
      "Epoch: 2389, Loss: 2.9356260299682617 \n",
      "              Params: tensor([  5.3145, -17.0035])\n",
      "              Grad: tensor([-0.0090,  0.0513])\n",
      "Epoch: 2390, Loss: 2.935596466064453 \n",
      "              Params: tensor([  5.3146, -17.0040])\n",
      "              Grad: tensor([-0.0090,  0.0512])\n",
      "Epoch: 2391, Loss: 2.9355711936950684 \n",
      "              Params: tensor([  5.3147, -17.0045])\n",
      "              Grad: tensor([-0.0090,  0.0511])\n",
      "Epoch: 2392, Loss: 2.935544490814209 \n",
      "              Params: tensor([  5.3148, -17.0050])\n",
      "              Grad: tensor([-0.0090,  0.0510])\n",
      "Epoch: 2393, Loss: 2.935516357421875 \n",
      "              Params: tensor([  5.3149, -17.0055])\n",
      "              Grad: tensor([-0.0090,  0.0509])\n",
      "Epoch: 2394, Loss: 2.9354889392852783 \n",
      "              Params: tensor([  5.3149, -17.0060])\n",
      "              Grad: tensor([-0.0090,  0.0508])\n",
      "Epoch: 2395, Loss: 2.93546462059021 \n",
      "              Params: tensor([  5.3150, -17.0065])\n",
      "              Grad: tensor([-0.0090,  0.0507])\n",
      "Epoch: 2396, Loss: 2.935436487197876 \n",
      "              Params: tensor([  5.3151, -17.0070])\n",
      "              Grad: tensor([-0.0090,  0.0507])\n",
      "Epoch: 2397, Loss: 2.935411214828491 \n",
      "              Params: tensor([  5.3152, -17.0076])\n",
      "              Grad: tensor([-0.0089,  0.0506])\n",
      "Epoch: 2398, Loss: 2.935385227203369 \n",
      "              Params: tensor([  5.3153, -17.0081])\n",
      "              Grad: tensor([-0.0089,  0.0505])\n",
      "Epoch: 2399, Loss: 2.935356378555298 \n",
      "              Params: tensor([  5.3154, -17.0086])\n",
      "              Grad: tensor([-0.0089,  0.0504])\n",
      "Epoch: 2400, Loss: 2.9353315830230713 \n",
      "              Params: tensor([  5.3155, -17.0091])\n",
      "              Grad: tensor([-0.0089,  0.0503])\n",
      "Epoch: 2401, Loss: 2.9353041648864746 \n",
      "              Params: tensor([  5.3156, -17.0096])\n",
      "              Grad: tensor([-0.0089,  0.0502])\n",
      "Epoch: 2402, Loss: 2.9352805614471436 \n",
      "              Params: tensor([  5.3157, -17.0101])\n",
      "              Grad: tensor([-0.0088,  0.0502])\n",
      "Epoch: 2403, Loss: 2.9352524280548096 \n",
      "              Params: tensor([  5.3157, -17.0106])\n",
      "              Grad: tensor([-0.0088,  0.0501])\n",
      "Epoch: 2404, Loss: 2.935228109359741 \n",
      "              Params: tensor([  5.3158, -17.0111])\n",
      "              Grad: tensor([-0.0088,  0.0500])\n",
      "Epoch: 2405, Loss: 2.9352033138275146 \n",
      "              Params: tensor([  5.3159, -17.0116])\n",
      "              Grad: tensor([-0.0088,  0.0499])\n",
      "Epoch: 2406, Loss: 2.9351766109466553 \n",
      "              Params: tensor([  5.3160, -17.0121])\n",
      "              Grad: tensor([-0.0088,  0.0498])\n",
      "Epoch: 2407, Loss: 2.9351515769958496 \n",
      "              Params: tensor([  5.3161, -17.0126])\n",
      "              Grad: tensor([-0.0088,  0.0497])\n",
      "Epoch: 2408, Loss: 2.9351258277893066 \n",
      "              Params: tensor([  5.3162, -17.0131])\n",
      "              Grad: tensor([-0.0088,  0.0496])\n",
      "Epoch: 2409, Loss: 2.9350996017456055 \n",
      "              Params: tensor([  5.3163, -17.0136])\n",
      "              Grad: tensor([-0.0088,  0.0496])\n",
      "Epoch: 2410, Loss: 2.9350745677948 \n",
      "              Params: tensor([  5.3164, -17.0140])\n",
      "              Grad: tensor([-0.0087,  0.0495])\n",
      "Epoch: 2411, Loss: 2.9350485801696777 \n",
      "              Params: tensor([  5.3164, -17.0145])\n",
      "              Grad: tensor([-0.0087,  0.0494])\n",
      "Epoch: 2412, Loss: 2.9350240230560303 \n",
      "              Params: tensor([  5.3165, -17.0150])\n",
      "              Grad: tensor([-0.0087,  0.0493])\n",
      "Epoch: 2413, Loss: 2.9350011348724365 \n",
      "              Params: tensor([  5.3166, -17.0155])\n",
      "              Grad: tensor([-0.0087,  0.0492])\n",
      "Epoch: 2414, Loss: 2.9349730014801025 \n",
      "              Params: tensor([  5.3167, -17.0160])\n",
      "              Grad: tensor([-0.0087,  0.0491])\n",
      "Epoch: 2415, Loss: 2.934948682785034 \n",
      "              Params: tensor([  5.3168, -17.0165])\n",
      "              Grad: tensor([-0.0087,  0.0491])\n",
      "Epoch: 2416, Loss: 2.9349253177642822 \n",
      "              Params: tensor([  5.3169, -17.0170])\n",
      "              Grad: tensor([-0.0086,  0.0490])\n",
      "Epoch: 2417, Loss: 2.934899091720581 \n",
      "              Params: tensor([  5.3170, -17.0175])\n",
      "              Grad: tensor([-0.0086,  0.0489])\n",
      "Epoch: 2418, Loss: 2.934875726699829 \n",
      "              Params: tensor([  5.3171, -17.0180])\n",
      "              Grad: tensor([-0.0086,  0.0488])\n",
      "Epoch: 2419, Loss: 2.9348526000976562 \n",
      "              Params: tensor([  5.3171, -17.0185])\n",
      "              Grad: tensor([-0.0086,  0.0487])\n",
      "Epoch: 2420, Loss: 2.934826135635376 \n",
      "              Params: tensor([  5.3172, -17.0189])\n",
      "              Grad: tensor([-0.0086,  0.0486])\n",
      "Epoch: 2421, Loss: 2.9348020553588867 \n",
      "              Params: tensor([  5.3173, -17.0194])\n",
      "              Grad: tensor([-0.0086,  0.0486])\n",
      "Epoch: 2422, Loss: 2.93477725982666 \n",
      "              Params: tensor([  5.3174, -17.0199])\n",
      "              Grad: tensor([-0.0086,  0.0485])\n",
      "Epoch: 2423, Loss: 2.934753179550171 \n",
      "              Params: tensor([  5.3175, -17.0204])\n",
      "              Grad: tensor([-0.0086,  0.0484])\n",
      "Epoch: 2424, Loss: 2.93472957611084 \n",
      "              Params: tensor([  5.3176, -17.0209])\n",
      "              Grad: tensor([-0.0086,  0.0483])\n",
      "Epoch: 2425, Loss: 2.9347054958343506 \n",
      "              Params: tensor([  5.3177, -17.0214])\n",
      "              Grad: tensor([-0.0085,  0.0482])\n",
      "Epoch: 2426, Loss: 2.934680938720703 \n",
      "              Params: tensor([  5.3177, -17.0219])\n",
      "              Grad: tensor([-0.0085,  0.0481])\n",
      "Epoch: 2427, Loss: 2.9346578121185303 \n",
      "              Params: tensor([  5.3178, -17.0223])\n",
      "              Grad: tensor([-0.0085,  0.0481])\n",
      "Epoch: 2428, Loss: 2.9346354007720947 \n",
      "              Params: tensor([  5.3179, -17.0228])\n",
      "              Grad: tensor([-0.0085,  0.0480])\n",
      "Epoch: 2429, Loss: 2.9346091747283936 \n",
      "              Params: tensor([  5.3180, -17.0233])\n",
      "              Grad: tensor([-0.0085,  0.0479])\n",
      "Epoch: 2430, Loss: 2.934584856033325 \n",
      "              Params: tensor([  5.3181, -17.0238])\n",
      "              Grad: tensor([-0.0084,  0.0478])\n",
      "Epoch: 2431, Loss: 2.934563398361206 \n",
      "              Params: tensor([  5.3182, -17.0242])\n",
      "              Grad: tensor([-0.0084,  0.0477])\n",
      "Epoch: 2432, Loss: 2.9345412254333496 \n",
      "              Params: tensor([  5.3182, -17.0247])\n",
      "              Grad: tensor([-0.0084,  0.0477])\n",
      "Epoch: 2433, Loss: 2.934516191482544 \n",
      "              Params: tensor([  5.3183, -17.0252])\n",
      "              Grad: tensor([-0.0084,  0.0476])\n",
      "Epoch: 2434, Loss: 2.934492588043213 \n",
      "              Params: tensor([  5.3184, -17.0257])\n",
      "              Grad: tensor([-0.0084,  0.0475])\n",
      "Epoch: 2435, Loss: 2.9344687461853027 \n",
      "              Params: tensor([  5.3185, -17.0261])\n",
      "              Grad: tensor([-0.0084,  0.0474])\n",
      "Epoch: 2436, Loss: 2.934446096420288 \n",
      "              Params: tensor([  5.3186, -17.0266])\n",
      "              Grad: tensor([-0.0084,  0.0473])\n",
      "Epoch: 2437, Loss: 2.9344232082366943 \n",
      "              Params: tensor([  5.3187, -17.0271])\n",
      "              Grad: tensor([-0.0083,  0.0473])\n",
      "Epoch: 2438, Loss: 2.9344000816345215 \n",
      "              Params: tensor([  5.3187, -17.0276])\n",
      "              Grad: tensor([-0.0083,  0.0472])\n",
      "Epoch: 2439, Loss: 2.9343767166137695 \n",
      "              Params: tensor([  5.3188, -17.0280])\n",
      "              Grad: tensor([-0.0083,  0.0471])\n",
      "Epoch: 2440, Loss: 2.934354543685913 \n",
      "              Params: tensor([  5.3189, -17.0285])\n",
      "              Grad: tensor([-0.0083,  0.0470])\n",
      "Epoch: 2441, Loss: 2.9343314170837402 \n",
      "              Params: tensor([  5.3190, -17.0290])\n",
      "              Grad: tensor([-0.0083,  0.0469])\n",
      "Epoch: 2442, Loss: 2.9343085289001465 \n",
      "              Params: tensor([  5.3191, -17.0294])\n",
      "              Grad: tensor([-0.0083,  0.0469])\n",
      "Epoch: 2443, Loss: 2.9342870712280273 \n",
      "              Params: tensor([  5.3192, -17.0299])\n",
      "              Grad: tensor([-0.0083,  0.0468])\n",
      "Epoch: 2444, Loss: 2.9342641830444336 \n",
      "              Params: tensor([  5.3192, -17.0304])\n",
      "              Grad: tensor([-0.0083,  0.0467])\n",
      "Epoch: 2445, Loss: 2.9342422485351562 \n",
      "              Params: tensor([  5.3193, -17.0308])\n",
      "              Grad: tensor([-0.0083,  0.0466])\n",
      "Epoch: 2446, Loss: 2.9342191219329834 \n",
      "              Params: tensor([  5.3194, -17.0313])\n",
      "              Grad: tensor([-0.0082,  0.0465])\n",
      "Epoch: 2447, Loss: 2.9341979026794434 \n",
      "              Params: tensor([  5.3195, -17.0318])\n",
      "              Grad: tensor([-0.0082,  0.0465])\n",
      "Epoch: 2448, Loss: 2.9341750144958496 \n",
      "              Params: tensor([  5.3196, -17.0322])\n",
      "              Grad: tensor([-0.0082,  0.0464])\n",
      "Epoch: 2449, Loss: 2.9341514110565186 \n",
      "              Params: tensor([  5.3197, -17.0327])\n",
      "              Grad: tensor([-0.0082,  0.0463])\n",
      "Epoch: 2450, Loss: 2.934129476547241 \n",
      "              Params: tensor([  5.3197, -17.0332])\n",
      "              Grad: tensor([-0.0082,  0.0462])\n",
      "Epoch: 2451, Loss: 2.934108018875122 \n",
      "              Params: tensor([  5.3198, -17.0336])\n",
      "              Grad: tensor([-0.0082,  0.0461])\n",
      "Epoch: 2452, Loss: 2.934084415435791 \n",
      "              Params: tensor([  5.3199, -17.0341])\n",
      "              Grad: tensor([-0.0081,  0.0461])\n",
      "Epoch: 2453, Loss: 2.9340646266937256 \n",
      "              Params: tensor([  5.3200, -17.0345])\n",
      "              Grad: tensor([-0.0081,  0.0460])\n",
      "Epoch: 2454, Loss: 2.9340431690216064 \n",
      "              Params: tensor([  5.3201, -17.0350])\n",
      "              Grad: tensor([-0.0081,  0.0459])\n",
      "Epoch: 2455, Loss: 2.9340200424194336 \n",
      "              Params: tensor([  5.3201, -17.0355])\n",
      "              Grad: tensor([-0.0081,  0.0458])\n",
      "Epoch: 2456, Loss: 2.93399977684021 \n",
      "              Params: tensor([  5.3202, -17.0359])\n",
      "              Grad: tensor([-0.0081,  0.0457])\n",
      "Epoch: 2457, Loss: 2.933978319168091 \n",
      "              Params: tensor([  5.3203, -17.0364])\n",
      "              Grad: tensor([-0.0081,  0.0457])\n",
      "Epoch: 2458, Loss: 2.9339563846588135 \n",
      "              Params: tensor([  5.3204, -17.0368])\n",
      "              Grad: tensor([-0.0080,  0.0456])\n",
      "Epoch: 2459, Loss: 2.9339349269866943 \n",
      "              Params: tensor([  5.3205, -17.0373])\n",
      "              Grad: tensor([-0.0080,  0.0455])\n",
      "Epoch: 2460, Loss: 2.9339139461517334 \n",
      "              Params: tensor([  5.3205, -17.0377])\n",
      "              Grad: tensor([-0.0080,  0.0454])\n",
      "Epoch: 2461, Loss: 2.9338927268981934 \n",
      "              Params: tensor([  5.3206, -17.0382])\n",
      "              Grad: tensor([-0.0080,  0.0454])\n",
      "Epoch: 2462, Loss: 2.933870553970337 \n",
      "              Params: tensor([  5.3207, -17.0386])\n",
      "              Grad: tensor([-0.0080,  0.0453])\n",
      "Epoch: 2463, Loss: 2.9338486194610596 \n",
      "              Params: tensor([  5.3208, -17.0391])\n",
      "              Grad: tensor([-0.0080,  0.0452])\n",
      "Epoch: 2464, Loss: 2.9338278770446777 \n",
      "              Params: tensor([  5.3209, -17.0396])\n",
      "              Grad: tensor([-0.0080,  0.0451])\n",
      "Epoch: 2465, Loss: 2.9338066577911377 \n",
      "              Params: tensor([  5.3209, -17.0400])\n",
      "              Grad: tensor([-0.0080,  0.0451])\n",
      "Epoch: 2466, Loss: 2.9337873458862305 \n",
      "              Params: tensor([  5.3210, -17.0405])\n",
      "              Grad: tensor([-0.0079,  0.0450])\n",
      "Epoch: 2467, Loss: 2.9337668418884277 \n",
      "              Params: tensor([  5.3211, -17.0409])\n",
      "              Grad: tensor([-0.0079,  0.0449])\n",
      "Epoch: 2468, Loss: 2.9337456226348877 \n",
      "              Params: tensor([  5.3212, -17.0413])\n",
      "              Grad: tensor([-0.0079,  0.0448])\n",
      "Epoch: 2469, Loss: 2.933722734451294 \n",
      "              Params: tensor([  5.3213, -17.0418])\n",
      "              Grad: tensor([-0.0079,  0.0448])\n",
      "Epoch: 2470, Loss: 2.933703660964966 \n",
      "              Params: tensor([  5.3213, -17.0422])\n",
      "              Grad: tensor([-0.0079,  0.0447])\n",
      "Epoch: 2471, Loss: 2.933682441711426 \n",
      "              Params: tensor([  5.3214, -17.0427])\n",
      "              Grad: tensor([-0.0079,  0.0446])\n",
      "Epoch: 2472, Loss: 2.9336624145507812 \n",
      "              Params: tensor([  5.3215, -17.0431])\n",
      "              Grad: tensor([-0.0079,  0.0445])\n",
      "Epoch: 2473, Loss: 2.933642625808716 \n",
      "              Params: tensor([  5.3216, -17.0436])\n",
      "              Grad: tensor([-0.0079,  0.0444])\n",
      "Epoch: 2474, Loss: 2.933622121810913 \n",
      "              Params: tensor([  5.3217, -17.0440])\n",
      "              Grad: tensor([-0.0078,  0.0444])\n",
      "Epoch: 2475, Loss: 2.9336020946502686 \n",
      "              Params: tensor([  5.3217, -17.0445])\n",
      "              Grad: tensor([-0.0078,  0.0443])\n",
      "Epoch: 2476, Loss: 2.9335830211639404 \n",
      "              Params: tensor([  5.3218, -17.0449])\n",
      "              Grad: tensor([-0.0078,  0.0442])\n",
      "Epoch: 2477, Loss: 2.933560848236084 \n",
      "              Params: tensor([  5.3219, -17.0453])\n",
      "              Grad: tensor([-0.0078,  0.0441])\n",
      "Epoch: 2478, Loss: 2.9335405826568604 \n",
      "              Params: tensor([  5.3220, -17.0458])\n",
      "              Grad: tensor([-0.0078,  0.0441])\n",
      "Epoch: 2479, Loss: 2.933521270751953 \n",
      "              Params: tensor([  5.3220, -17.0462])\n",
      "              Grad: tensor([-0.0078,  0.0440])\n",
      "Epoch: 2480, Loss: 2.9335012435913086 \n",
      "              Params: tensor([  5.3221, -17.0467])\n",
      "              Grad: tensor([-0.0078,  0.0439])\n",
      "Epoch: 2481, Loss: 2.9334802627563477 \n",
      "              Params: tensor([  5.3222, -17.0471])\n",
      "              Grad: tensor([-0.0077,  0.0438])\n",
      "Epoch: 2482, Loss: 2.9334633350372314 \n",
      "              Params: tensor([  5.3223, -17.0475])\n",
      "              Grad: tensor([-0.0077,  0.0438])\n",
      "Epoch: 2483, Loss: 2.9334418773651123 \n",
      "              Params: tensor([  5.3224, -17.0480])\n",
      "              Grad: tensor([-0.0077,  0.0437])\n",
      "Epoch: 2484, Loss: 2.933422327041626 \n",
      "              Params: tensor([  5.3224, -17.0484])\n",
      "              Grad: tensor([-0.0077,  0.0436])\n",
      "Epoch: 2485, Loss: 2.933403253555298 \n",
      "              Params: tensor([  5.3225, -17.0489])\n",
      "              Grad: tensor([-0.0077,  0.0436])\n",
      "Epoch: 2486, Loss: 2.9333817958831787 \n",
      "              Params: tensor([  5.3226, -17.0493])\n",
      "              Grad: tensor([-0.0077,  0.0435])\n",
      "Epoch: 2487, Loss: 2.9333646297454834 \n",
      "              Params: tensor([  5.3227, -17.0497])\n",
      "              Grad: tensor([-0.0077,  0.0434])\n",
      "Epoch: 2488, Loss: 2.933344841003418 \n",
      "              Params: tensor([  5.3227, -17.0502])\n",
      "              Grad: tensor([-0.0077,  0.0433])\n",
      "Epoch: 2489, Loss: 2.9333252906799316 \n",
      "              Params: tensor([  5.3228, -17.0506])\n",
      "              Grad: tensor([-0.0076,  0.0433])\n",
      "Epoch: 2490, Loss: 2.9333059787750244 \n",
      "              Params: tensor([  5.3229, -17.0510])\n",
      "              Grad: tensor([-0.0076,  0.0432])\n",
      "Epoch: 2491, Loss: 2.9332869052886963 \n",
      "              Params: tensor([  5.3230, -17.0515])\n",
      "              Grad: tensor([-0.0076,  0.0431])\n",
      "Epoch: 2492, Loss: 2.9332656860351562 \n",
      "              Params: tensor([  5.3230, -17.0519])\n",
      "              Grad: tensor([-0.0076,  0.0430])\n",
      "Epoch: 2493, Loss: 2.93324875831604 \n",
      "              Params: tensor([  5.3231, -17.0523])\n",
      "              Grad: tensor([-0.0076,  0.0430])\n",
      "Epoch: 2494, Loss: 2.9332289695739746 \n",
      "              Params: tensor([  5.3232, -17.0527])\n",
      "              Grad: tensor([-0.0076,  0.0429])\n",
      "Epoch: 2495, Loss: 2.9332094192504883 \n",
      "              Params: tensor([  5.3233, -17.0532])\n",
      "              Grad: tensor([-0.0076,  0.0428])\n",
      "Epoch: 2496, Loss: 2.93319034576416 \n",
      "              Params: tensor([  5.3233, -17.0536])\n",
      "              Grad: tensor([-0.0075,  0.0427])\n",
      "Epoch: 2497, Loss: 2.9331719875335693 \n",
      "              Params: tensor([  5.3234, -17.0540])\n",
      "              Grad: tensor([-0.0075,  0.0427])\n",
      "Epoch: 2498, Loss: 2.9331536293029785 \n",
      "              Params: tensor([  5.3235, -17.0544])\n",
      "              Grad: tensor([-0.0075,  0.0426])\n",
      "Epoch: 2499, Loss: 2.933133840560913 \n",
      "              Params: tensor([  5.3236, -17.0549])\n",
      "              Grad: tensor([-0.0075,  0.0425])\n",
      "Epoch: 2500, Loss: 2.9331159591674805 \n",
      "              Params: tensor([  5.3236, -17.0553])\n",
      "              Grad: tensor([-0.0075,  0.0425])\n",
      "Epoch: 2501, Loss: 2.9330968856811523 \n",
      "              Params: tensor([  5.3237, -17.0557])\n",
      "              Grad: tensor([-0.0075,  0.0424])\n",
      "Epoch: 2502, Loss: 2.9330787658691406 \n",
      "              Params: tensor([  5.3238, -17.0561])\n",
      "              Grad: tensor([-0.0075,  0.0423])\n",
      "Epoch: 2503, Loss: 2.9330599308013916 \n",
      "              Params: tensor([  5.3239, -17.0566])\n",
      "              Grad: tensor([-0.0075,  0.0422])\n",
      "Epoch: 2504, Loss: 2.9330427646636963 \n",
      "              Params: tensor([  5.3239, -17.0570])\n",
      "              Grad: tensor([-0.0074,  0.0422])\n",
      "Epoch: 2505, Loss: 2.933025360107422 \n",
      "              Params: tensor([  5.3240, -17.0574])\n",
      "              Grad: tensor([-0.0074,  0.0421])\n",
      "Epoch: 2506, Loss: 2.93300724029541 \n",
      "              Params: tensor([  5.3241, -17.0578])\n",
      "              Grad: tensor([-0.0074,  0.0420])\n",
      "Epoch: 2507, Loss: 2.932988166809082 \n",
      "              Params: tensor([  5.3242, -17.0582])\n",
      "              Grad: tensor([-0.0074,  0.0420])\n",
      "Epoch: 2508, Loss: 2.9329702854156494 \n",
      "              Params: tensor([  5.3242, -17.0587])\n",
      "              Grad: tensor([-0.0074,  0.0419])\n",
      "Epoch: 2509, Loss: 2.932952880859375 \n",
      "              Params: tensor([  5.3243, -17.0591])\n",
      "              Grad: tensor([-0.0074,  0.0418])\n",
      "Epoch: 2510, Loss: 2.932932138442993 \n",
      "              Params: tensor([  5.3244, -17.0595])\n",
      "              Grad: tensor([-0.0074,  0.0417])\n",
      "Epoch: 2511, Loss: 2.932915449142456 \n",
      "              Params: tensor([  5.3245, -17.0599])\n",
      "              Grad: tensor([-0.0073,  0.0417])\n",
      "Epoch: 2512, Loss: 2.9328980445861816 \n",
      "              Params: tensor([  5.3245, -17.0603])\n",
      "              Grad: tensor([-0.0073,  0.0416])\n",
      "Epoch: 2513, Loss: 2.932880163192749 \n",
      "              Params: tensor([  5.3246, -17.0608])\n",
      "              Grad: tensor([-0.0073,  0.0415])\n",
      "Epoch: 2514, Loss: 2.932861566543579 \n",
      "              Params: tensor([  5.3247, -17.0612])\n",
      "              Grad: tensor([-0.0073,  0.0415])\n",
      "Epoch: 2515, Loss: 2.9328458309173584 \n",
      "              Params: tensor([  5.3248, -17.0616])\n",
      "              Grad: tensor([-0.0073,  0.0414])\n",
      "Epoch: 2516, Loss: 2.932826280593872 \n",
      "              Params: tensor([  5.3248, -17.0620])\n",
      "              Grad: tensor([-0.0073,  0.0413])\n",
      "Epoch: 2517, Loss: 2.932810068130493 \n",
      "              Params: tensor([  5.3249, -17.0624])\n",
      "              Grad: tensor([-0.0073,  0.0412])\n",
      "Epoch: 2518, Loss: 2.9327902793884277 \n",
      "              Params: tensor([  5.3250, -17.0628])\n",
      "              Grad: tensor([-0.0073,  0.0412])\n",
      "Epoch: 2519, Loss: 2.9327738285064697 \n",
      "              Params: tensor([  5.3250, -17.0632])\n",
      "              Grad: tensor([-0.0073,  0.0411])\n",
      "Epoch: 2520, Loss: 2.932758092880249 \n",
      "              Params: tensor([  5.3251, -17.0636])\n",
      "              Grad: tensor([-0.0073,  0.0410])\n",
      "Epoch: 2521, Loss: 2.9327392578125 \n",
      "              Params: tensor([  5.3252, -17.0640])\n",
      "              Grad: tensor([-0.0073,  0.0410])\n",
      "Epoch: 2522, Loss: 2.932722568511963 \n",
      "              Params: tensor([  5.3253, -17.0645])\n",
      "              Grad: tensor([-0.0072,  0.0409])\n",
      "Epoch: 2523, Loss: 2.9327056407928467 \n",
      "              Params: tensor([  5.3253, -17.0649])\n",
      "              Grad: tensor([-0.0072,  0.0408])\n",
      "Epoch: 2524, Loss: 2.9326887130737305 \n",
      "              Params: tensor([  5.3254, -17.0653])\n",
      "              Grad: tensor([-0.0072,  0.0408])\n",
      "Epoch: 2525, Loss: 2.932671308517456 \n",
      "              Params: tensor([  5.3255, -17.0657])\n",
      "              Grad: tensor([-0.0072,  0.0407])\n",
      "Epoch: 2526, Loss: 2.9326536655426025 \n",
      "              Params: tensor([  5.3256, -17.0661])\n",
      "              Grad: tensor([-0.0072,  0.0406])\n",
      "Epoch: 2527, Loss: 2.9326372146606445 \n",
      "              Params: tensor([  5.3256, -17.0665])\n",
      "              Grad: tensor([-0.0072,  0.0405])\n",
      "Epoch: 2528, Loss: 2.9326186180114746 \n",
      "              Params: tensor([  5.3257, -17.0669])\n",
      "              Grad: tensor([-0.0072,  0.0405])\n",
      "Epoch: 2529, Loss: 2.932602882385254 \n",
      "              Params: tensor([  5.3258, -17.0673])\n",
      "              Grad: tensor([-0.0071,  0.0404])\n",
      "Epoch: 2530, Loss: 2.9325852394104004 \n",
      "              Params: tensor([  5.3258, -17.0677])\n",
      "              Grad: tensor([-0.0071,  0.0403])\n",
      "Epoch: 2531, Loss: 2.9325685501098633 \n",
      "              Params: tensor([  5.3259, -17.0681])\n",
      "              Grad: tensor([-0.0071,  0.0403])\n",
      "Epoch: 2532, Loss: 2.932553291320801 \n",
      "              Params: tensor([  5.3260, -17.0685])\n",
      "              Grad: tensor([-0.0071,  0.0402])\n",
      "Epoch: 2533, Loss: 2.93253493309021 \n",
      "              Params: tensor([  5.3261, -17.0689])\n",
      "              Grad: tensor([-0.0071,  0.0401])\n",
      "Epoch: 2534, Loss: 2.9325196743011475 \n",
      "              Params: tensor([  5.3261, -17.0693])\n",
      "              Grad: tensor([-0.0071,  0.0401])\n",
      "Epoch: 2535, Loss: 2.932502031326294 \n",
      "              Params: tensor([  5.3262, -17.0697])\n",
      "              Grad: tensor([-0.0071,  0.0400])\n",
      "Epoch: 2536, Loss: 2.9324867725372314 \n",
      "              Params: tensor([  5.3263, -17.0701])\n",
      "              Grad: tensor([-0.0071,  0.0399])\n",
      "Epoch: 2537, Loss: 2.9324686527252197 \n",
      "              Params: tensor([  5.3263, -17.0705])\n",
      "              Grad: tensor([-0.0070,  0.0399])\n",
      "Epoch: 2538, Loss: 2.93245530128479 \n",
      "              Params: tensor([  5.3264, -17.0709])\n",
      "              Grad: tensor([-0.0070,  0.0398])\n",
      "Epoch: 2539, Loss: 2.9324381351470947 \n",
      "              Params: tensor([  5.3265, -17.0713])\n",
      "              Grad: tensor([-0.0070,  0.0397])\n",
      "Epoch: 2540, Loss: 2.9324212074279785 \n",
      "              Params: tensor([  5.3265, -17.0717])\n",
      "              Grad: tensor([-0.0070,  0.0397])\n",
      "Epoch: 2541, Loss: 2.932403802871704 \n",
      "              Params: tensor([  5.3266, -17.0721])\n",
      "              Grad: tensor([-0.0070,  0.0396])\n",
      "Epoch: 2542, Loss: 2.932386875152588 \n",
      "              Params: tensor([  5.3267, -17.0725])\n",
      "              Grad: tensor([-0.0070,  0.0395])\n",
      "Epoch: 2543, Loss: 2.932370662689209 \n",
      "              Params: tensor([  5.3268, -17.0729])\n",
      "              Grad: tensor([-0.0070,  0.0395])\n",
      "Epoch: 2544, Loss: 2.9323575496673584 \n",
      "              Params: tensor([  5.3268, -17.0733])\n",
      "              Grad: tensor([-0.0070,  0.0394])\n",
      "Epoch: 2545, Loss: 2.932340383529663 \n",
      "              Params: tensor([  5.3269, -17.0737])\n",
      "              Grad: tensor([-0.0069,  0.0393])\n",
      "Epoch: 2546, Loss: 2.9323244094848633 \n",
      "              Params: tensor([  5.3270, -17.0741])\n",
      "              Grad: tensor([-0.0069,  0.0393])\n",
      "Epoch: 2547, Loss: 2.932309627532959 \n",
      "              Params: tensor([  5.3270, -17.0745])\n",
      "              Grad: tensor([-0.0069,  0.0392])\n",
      "Epoch: 2548, Loss: 2.9322926998138428 \n",
      "              Params: tensor([  5.3271, -17.0749])\n",
      "              Grad: tensor([-0.0069,  0.0391])\n",
      "Epoch: 2549, Loss: 2.9322774410247803 \n",
      "              Params: tensor([  5.3272, -17.0752])\n",
      "              Grad: tensor([-0.0069,  0.0391])\n",
      "Epoch: 2550, Loss: 2.9322614669799805 \n",
      "              Params: tensor([  5.3272, -17.0756])\n",
      "              Grad: tensor([-0.0069,  0.0390])\n",
      "Epoch: 2551, Loss: 2.9322457313537598 \n",
      "              Params: tensor([  5.3273, -17.0760])\n",
      "              Grad: tensor([-0.0069,  0.0389])\n",
      "Epoch: 2552, Loss: 2.9322290420532227 \n",
      "              Params: tensor([  5.3274, -17.0764])\n",
      "              Grad: tensor([-0.0069,  0.0389])\n",
      "Epoch: 2553, Loss: 2.9322149753570557 \n",
      "              Params: tensor([  5.3274, -17.0768])\n",
      "              Grad: tensor([-0.0069,  0.0388])\n",
      "Epoch: 2554, Loss: 2.9321978092193604 \n",
      "              Params: tensor([  5.3275, -17.0772])\n",
      "              Grad: tensor([-0.0068,  0.0387])\n",
      "Epoch: 2555, Loss: 2.932183265686035 \n",
      "              Params: tensor([  5.3276, -17.0776])\n",
      "              Grad: tensor([-0.0068,  0.0387])\n",
      "Epoch: 2556, Loss: 2.9321672916412354 \n",
      "              Params: tensor([  5.3276, -17.0780])\n",
      "              Grad: tensor([-0.0068,  0.0386])\n",
      "Epoch: 2557, Loss: 2.9321532249450684 \n",
      "              Params: tensor([  5.3277, -17.0783])\n",
      "              Grad: tensor([-0.0068,  0.0385])\n",
      "Epoch: 2558, Loss: 2.9321374893188477 \n",
      "              Params: tensor([  5.3278, -17.0787])\n",
      "              Grad: tensor([-0.0068,  0.0385])\n",
      "Epoch: 2559, Loss: 2.932121515274048 \n",
      "              Params: tensor([  5.3279, -17.0791])\n",
      "              Grad: tensor([-0.0068,  0.0384])\n",
      "Epoch: 2560, Loss: 2.932107448577881 \n",
      "              Params: tensor([  5.3279, -17.0795])\n",
      "              Grad: tensor([-0.0068,  0.0383])\n",
      "Epoch: 2561, Loss: 2.9320924282073975 \n",
      "              Params: tensor([  5.3280, -17.0799])\n",
      "              Grad: tensor([-0.0068,  0.0383])\n",
      "Epoch: 2562, Loss: 2.9320764541625977 \n",
      "              Params: tensor([  5.3281, -17.0803])\n",
      "              Grad: tensor([-0.0067,  0.0382])\n",
      "Epoch: 2563, Loss: 2.932061195373535 \n",
      "              Params: tensor([  5.3281, -17.0806])\n",
      "              Grad: tensor([-0.0067,  0.0381])\n",
      "Epoch: 2564, Loss: 2.9320473670959473 \n",
      "              Params: tensor([  5.3282, -17.0810])\n",
      "              Grad: tensor([-0.0067,  0.0381])\n",
      "Epoch: 2565, Loss: 2.9320311546325684 \n",
      "              Params: tensor([  5.3283, -17.0814])\n",
      "              Grad: tensor([-0.0067,  0.0380])\n",
      "Epoch: 2566, Loss: 2.9320173263549805 \n",
      "              Params: tensor([  5.3283, -17.0818])\n",
      "              Grad: tensor([-0.0067,  0.0379])\n",
      "Epoch: 2567, Loss: 2.932002305984497 \n",
      "              Params: tensor([  5.3284, -17.0822])\n",
      "              Grad: tensor([-0.0067,  0.0379])\n",
      "Epoch: 2568, Loss: 2.931986093521118 \n",
      "              Params: tensor([  5.3285, -17.0825])\n",
      "              Grad: tensor([-0.0067,  0.0378])\n",
      "Epoch: 2569, Loss: 2.9319722652435303 \n",
      "              Params: tensor([  5.3285, -17.0829])\n",
      "              Grad: tensor([-0.0067,  0.0378])\n",
      "Epoch: 2570, Loss: 2.931957483291626 \n",
      "              Params: tensor([  5.3286, -17.0833])\n",
      "              Grad: tensor([-0.0067,  0.0377])\n",
      "Epoch: 2571, Loss: 2.931941270828247 \n",
      "              Params: tensor([  5.3287, -17.0837])\n",
      "              Grad: tensor([-0.0067,  0.0376])\n",
      "Epoch: 2572, Loss: 2.931929111480713 \n",
      "              Params: tensor([  5.3287, -17.0840])\n",
      "              Grad: tensor([-0.0066,  0.0376])\n",
      "Epoch: 2573, Loss: 2.9319143295288086 \n",
      "              Params: tensor([  5.3288, -17.0844])\n",
      "              Grad: tensor([-0.0066,  0.0375])\n",
      "Epoch: 2574, Loss: 2.9318997859954834 \n",
      "              Params: tensor([  5.3289, -17.0848])\n",
      "              Grad: tensor([-0.0066,  0.0374])\n",
      "Epoch: 2575, Loss: 2.931885004043579 \n",
      "              Params: tensor([  5.3289, -17.0852])\n",
      "              Grad: tensor([-0.0066,  0.0374])\n",
      "Epoch: 2576, Loss: 2.931870222091675 \n",
      "              Params: tensor([  5.3290, -17.0855])\n",
      "              Grad: tensor([-0.0066,  0.0373])\n",
      "Epoch: 2577, Loss: 2.9318552017211914 \n",
      "              Params: tensor([  5.3291, -17.0859])\n",
      "              Grad: tensor([-0.0066,  0.0372])\n",
      "Epoch: 2578, Loss: 2.931842088699341 \n",
      "              Params: tensor([  5.3291, -17.0863])\n",
      "              Grad: tensor([-0.0066,  0.0372])\n",
      "Epoch: 2579, Loss: 2.9318277835845947 \n",
      "              Params: tensor([  5.3292, -17.0867])\n",
      "              Grad: tensor([-0.0066,  0.0371])\n",
      "Epoch: 2580, Loss: 2.9318132400512695 \n",
      "              Params: tensor([  5.3293, -17.0870])\n",
      "              Grad: tensor([-0.0065,  0.0371])\n",
      "Epoch: 2581, Loss: 2.9317986965179443 \n",
      "              Params: tensor([  5.3293, -17.0874])\n",
      "              Grad: tensor([-0.0065,  0.0370])\n",
      "Epoch: 2582, Loss: 2.931785821914673 \n",
      "              Params: tensor([  5.3294, -17.0878])\n",
      "              Grad: tensor([-0.0065,  0.0369])\n",
      "Epoch: 2583, Loss: 2.9317710399627686 \n",
      "              Params: tensor([  5.3294, -17.0881])\n",
      "              Grad: tensor([-0.0065,  0.0369])\n",
      "Epoch: 2584, Loss: 2.9317588806152344 \n",
      "              Params: tensor([  5.3295, -17.0885])\n",
      "              Grad: tensor([-0.0065,  0.0368])\n",
      "Epoch: 2585, Loss: 2.9317421913146973 \n",
      "              Params: tensor([  5.3296, -17.0889])\n",
      "              Grad: tensor([-0.0065,  0.0367])\n",
      "Epoch: 2586, Loss: 2.931729316711426 \n",
      "              Params: tensor([  5.3296, -17.0892])\n",
      "              Grad: tensor([-0.0065,  0.0367])\n",
      "Epoch: 2587, Loss: 2.9317169189453125 \n",
      "              Params: tensor([  5.3297, -17.0896])\n",
      "              Grad: tensor([-0.0065,  0.0366])\n",
      "Epoch: 2588, Loss: 2.9317009449005127 \n",
      "              Params: tensor([  5.3298, -17.0900])\n",
      "              Grad: tensor([-0.0065,  0.0366])\n",
      "Epoch: 2589, Loss: 2.9316866397857666 \n",
      "              Params: tensor([  5.3298, -17.0903])\n",
      "              Grad: tensor([-0.0065,  0.0365])\n",
      "Epoch: 2590, Loss: 2.931673526763916 \n",
      "              Params: tensor([  5.3299, -17.0907])\n",
      "              Grad: tensor([-0.0064,  0.0364])\n",
      "Epoch: 2591, Loss: 2.9316599369049072 \n",
      "              Params: tensor([  5.3300, -17.0911])\n",
      "              Grad: tensor([-0.0064,  0.0364])\n",
      "Epoch: 2592, Loss: 2.931647539138794 \n",
      "              Params: tensor([  5.3300, -17.0914])\n",
      "              Grad: tensor([-0.0064,  0.0363])\n",
      "Epoch: 2593, Loss: 2.9316320419311523 \n",
      "              Params: tensor([  5.3301, -17.0918])\n",
      "              Grad: tensor([-0.0064,  0.0362])\n",
      "Epoch: 2594, Loss: 2.9316186904907227 \n",
      "              Params: tensor([  5.3302, -17.0921])\n",
      "              Grad: tensor([-0.0064,  0.0362])\n",
      "Epoch: 2595, Loss: 2.9316062927246094 \n",
      "              Params: tensor([  5.3302, -17.0925])\n",
      "              Grad: tensor([-0.0064,  0.0361])\n",
      "Epoch: 2596, Loss: 2.931593418121338 \n",
      "              Params: tensor([  5.3303, -17.0929])\n",
      "              Grad: tensor([-0.0064,  0.0361])\n",
      "Epoch: 2597, Loss: 2.931579828262329 \n",
      "              Params: tensor([  5.3303, -17.0932])\n",
      "              Grad: tensor([-0.0064,  0.0360])\n",
      "Epoch: 2598, Loss: 2.9315664768218994 \n",
      "              Params: tensor([  5.3304, -17.0936])\n",
      "              Grad: tensor([-0.0064,  0.0359])\n",
      "Epoch: 2599, Loss: 2.931553840637207 \n",
      "              Params: tensor([  5.3305, -17.0939])\n",
      "              Grad: tensor([-0.0064,  0.0359])\n",
      "Epoch: 2600, Loss: 2.9315383434295654 \n",
      "              Params: tensor([  5.3305, -17.0943])\n",
      "              Grad: tensor([-0.0063,  0.0358])\n",
      "Epoch: 2601, Loss: 2.9315261840820312 \n",
      "              Params: tensor([  5.3306, -17.0947])\n",
      "              Grad: tensor([-0.0063,  0.0358])\n",
      "Epoch: 2602, Loss: 2.9315123558044434 \n",
      "              Params: tensor([  5.3307, -17.0950])\n",
      "              Grad: tensor([-0.0063,  0.0357])\n",
      "Epoch: 2603, Loss: 2.931499481201172 \n",
      "              Params: tensor([  5.3307, -17.0954])\n",
      "              Grad: tensor([-0.0063,  0.0356])\n",
      "Epoch: 2604, Loss: 2.931488275527954 \n",
      "              Params: tensor([  5.3308, -17.0957])\n",
      "              Grad: tensor([-0.0063,  0.0356])\n",
      "Epoch: 2605, Loss: 2.931473731994629 \n",
      "              Params: tensor([  5.3309, -17.0961])\n",
      "              Grad: tensor([-0.0063,  0.0355])\n",
      "Epoch: 2606, Loss: 2.931462049484253 \n",
      "              Params: tensor([  5.3309, -17.0964])\n",
      "              Grad: tensor([-0.0062,  0.0355])\n",
      "Epoch: 2607, Loss: 2.931448459625244 \n",
      "              Params: tensor([  5.3310, -17.0968])\n",
      "              Grad: tensor([-0.0062,  0.0354])\n",
      "Epoch: 2608, Loss: 2.9314355850219727 \n",
      "              Params: tensor([  5.3310, -17.0971])\n",
      "              Grad: tensor([-0.0062,  0.0353])\n",
      "Epoch: 2609, Loss: 2.9314229488372803 \n",
      "              Params: tensor([  5.3311, -17.0975])\n",
      "              Grad: tensor([-0.0062,  0.0353])\n",
      "Epoch: 2610, Loss: 2.931410789489746 \n",
      "              Params: tensor([  5.3312, -17.0979])\n",
      "              Grad: tensor([-0.0062,  0.0352])\n",
      "Epoch: 2611, Loss: 2.9313974380493164 \n",
      "              Params: tensor([  5.3312, -17.0982])\n",
      "              Grad: tensor([-0.0062,  0.0352])\n",
      "Epoch: 2612, Loss: 2.9313840866088867 \n",
      "              Params: tensor([  5.3313, -17.0986])\n",
      "              Grad: tensor([-0.0062,  0.0351])\n",
      "Epoch: 2613, Loss: 2.931370735168457 \n",
      "              Params: tensor([  5.3313, -17.0989])\n",
      "              Grad: tensor([-0.0062,  0.0350])\n",
      "Epoch: 2614, Loss: 2.9313583374023438 \n",
      "              Params: tensor([  5.3314, -17.0993])\n",
      "              Grad: tensor([-0.0062,  0.0350])\n",
      "Epoch: 2615, Loss: 2.9313459396362305 \n",
      "              Params: tensor([  5.3315, -17.0996])\n",
      "              Grad: tensor([-0.0062,  0.0349])\n",
      "Epoch: 2616, Loss: 2.9313347339630127 \n",
      "              Params: tensor([  5.3315, -17.1000])\n",
      "              Grad: tensor([-0.0062,  0.0349])\n",
      "Epoch: 2617, Loss: 2.931321620941162 \n",
      "              Params: tensor([  5.3316, -17.1003])\n",
      "              Grad: tensor([-0.0062,  0.0348])\n",
      "Epoch: 2618, Loss: 2.9313080310821533 \n",
      "              Params: tensor([  5.3317, -17.1006])\n",
      "              Grad: tensor([-0.0061,  0.0347])\n",
      "Epoch: 2619, Loss: 2.931295871734619 \n",
      "              Params: tensor([  5.3317, -17.1010])\n",
      "              Grad: tensor([-0.0061,  0.0347])\n",
      "Epoch: 2620, Loss: 2.9312820434570312 \n",
      "              Params: tensor([  5.3318, -17.1013])\n",
      "              Grad: tensor([-0.0061,  0.0346])\n",
      "Epoch: 2621, Loss: 2.931271553039551 \n",
      "              Params: tensor([  5.3318, -17.1017])\n",
      "              Grad: tensor([-0.0061,  0.0346])\n",
      "Epoch: 2622, Loss: 2.9312584400177 \n",
      "              Params: tensor([  5.3319, -17.1020])\n",
      "              Grad: tensor([-0.0061,  0.0345])\n",
      "Epoch: 2623, Loss: 2.9312446117401123 \n",
      "              Params: tensor([  5.3320, -17.1024])\n",
      "              Grad: tensor([-0.0061,  0.0344])\n",
      "Epoch: 2624, Loss: 2.931234121322632 \n",
      "              Params: tensor([  5.3320, -17.1027])\n",
      "              Grad: tensor([-0.0061,  0.0344])\n",
      "Epoch: 2625, Loss: 2.931222438812256 \n",
      "              Params: tensor([  5.3321, -17.1031])\n",
      "              Grad: tensor([-0.0061,  0.0343])\n",
      "Epoch: 2626, Loss: 2.931211233139038 \n",
      "              Params: tensor([  5.3321, -17.1034])\n",
      "              Grad: tensor([-0.0060,  0.0343])\n",
      "Epoch: 2627, Loss: 2.9311959743499756 \n",
      "              Params: tensor([  5.3322, -17.1038])\n",
      "              Grad: tensor([-0.0060,  0.0342])\n",
      "Epoch: 2628, Loss: 2.931185007095337 \n",
      "              Params: tensor([  5.3323, -17.1041])\n",
      "              Grad: tensor([-0.0060,  0.0342])\n",
      "Epoch: 2629, Loss: 2.931173086166382 \n",
      "              Params: tensor([  5.3323, -17.1044])\n",
      "              Grad: tensor([-0.0060,  0.0341])\n",
      "Epoch: 2630, Loss: 2.931162118911743 \n",
      "              Params: tensor([  5.3324, -17.1048])\n",
      "              Grad: tensor([-0.0060,  0.0340])\n",
      "Epoch: 2631, Loss: 2.9311487674713135 \n",
      "              Params: tensor([  5.3324, -17.1051])\n",
      "              Grad: tensor([-0.0060,  0.0340])\n",
      "Epoch: 2632, Loss: 2.931138038635254 \n",
      "              Params: tensor([  5.3325, -17.1055])\n",
      "              Grad: tensor([-0.0060,  0.0339])\n",
      "Epoch: 2633, Loss: 2.931126356124878 \n",
      "              Params: tensor([  5.3326, -17.1058])\n",
      "              Grad: tensor([-0.0060,  0.0339])\n",
      "Epoch: 2634, Loss: 2.9311141967773438 \n",
      "              Params: tensor([  5.3326, -17.1061])\n",
      "              Grad: tensor([-0.0060,  0.0338])\n",
      "Epoch: 2635, Loss: 2.9311013221740723 \n",
      "              Params: tensor([  5.3327, -17.1065])\n",
      "              Grad: tensor([-0.0060,  0.0337])\n",
      "Epoch: 2636, Loss: 2.9310896396636963 \n",
      "              Params: tensor([  5.3327, -17.1068])\n",
      "              Grad: tensor([-0.0059,  0.0337])\n",
      "Epoch: 2637, Loss: 2.9310789108276367 \n",
      "              Params: tensor([  5.3328, -17.1071])\n",
      "              Grad: tensor([-0.0059,  0.0336])\n",
      "Epoch: 2638, Loss: 2.9310669898986816 \n",
      "              Params: tensor([  5.3329, -17.1075])\n",
      "              Grad: tensor([-0.0059,  0.0336])\n",
      "Epoch: 2639, Loss: 2.93105411529541 \n",
      "              Params: tensor([  5.3329, -17.1078])\n",
      "              Grad: tensor([-0.0059,  0.0335])\n",
      "Epoch: 2640, Loss: 2.931044101715088 \n",
      "              Params: tensor([  5.3330, -17.1081])\n",
      "              Grad: tensor([-0.0059,  0.0335])\n",
      "Epoch: 2641, Loss: 2.9310340881347656 \n",
      "              Params: tensor([  5.3330, -17.1085])\n",
      "              Grad: tensor([-0.0059,  0.0334])\n",
      "Epoch: 2642, Loss: 2.931021213531494 \n",
      "              Params: tensor([  5.3331, -17.1088])\n",
      "              Grad: tensor([-0.0059,  0.0333])\n",
      "Epoch: 2643, Loss: 2.9310102462768555 \n",
      "              Params: tensor([  5.3332, -17.1091])\n",
      "              Grad: tensor([-0.0059,  0.0333])\n",
      "Epoch: 2644, Loss: 2.9309990406036377 \n",
      "              Params: tensor([  5.3332, -17.1095])\n",
      "              Grad: tensor([-0.0059,  0.0332])\n",
      "Epoch: 2645, Loss: 2.9309868812561035 \n",
      "              Params: tensor([  5.3333, -17.1098])\n",
      "              Grad: tensor([-0.0059,  0.0332])\n",
      "Epoch: 2646, Loss: 2.930976152420044 \n",
      "              Params: tensor([  5.3333, -17.1101])\n",
      "              Grad: tensor([-0.0059,  0.0331])\n",
      "Epoch: 2647, Loss: 2.930964469909668 \n",
      "              Params: tensor([  5.3334, -17.1105])\n",
      "              Grad: tensor([-0.0059,  0.0331])\n",
      "Epoch: 2648, Loss: 2.93095326423645 \n",
      "              Params: tensor([  5.3335, -17.1108])\n",
      "              Grad: tensor([-0.0058,  0.0330])\n",
      "Epoch: 2649, Loss: 2.930940866470337 \n",
      "              Params: tensor([  5.3335, -17.1111])\n",
      "              Grad: tensor([-0.0058,  0.0330])\n",
      "Epoch: 2650, Loss: 2.930931806564331 \n",
      "              Params: tensor([  5.3336, -17.1115])\n",
      "              Grad: tensor([-0.0058,  0.0329])\n",
      "Epoch: 2651, Loss: 2.9309206008911133 \n",
      "              Params: tensor([  5.3336, -17.1118])\n",
      "              Grad: tensor([-0.0058,  0.0328])\n",
      "Epoch: 2652, Loss: 2.930907964706421 \n",
      "              Params: tensor([  5.3337, -17.1121])\n",
      "              Grad: tensor([-0.0058,  0.0328])\n",
      "Epoch: 2653, Loss: 2.930899143218994 \n",
      "              Params: tensor([  5.3337, -17.1124])\n",
      "              Grad: tensor([-0.0058,  0.0327])\n",
      "Epoch: 2654, Loss: 2.930885076522827 \n",
      "              Params: tensor([  5.3338, -17.1128])\n",
      "              Grad: tensor([-0.0058,  0.0327])\n",
      "Epoch: 2655, Loss: 2.9308760166168213 \n",
      "              Params: tensor([  5.3339, -17.1131])\n",
      "              Grad: tensor([-0.0058,  0.0326])\n",
      "Epoch: 2656, Loss: 2.930863380432129 \n",
      "              Params: tensor([  5.3339, -17.1134])\n",
      "              Grad: tensor([-0.0057,  0.0326])\n",
      "Epoch: 2657, Loss: 2.930853843688965 \n",
      "              Params: tensor([  5.3340, -17.1137])\n",
      "              Grad: tensor([-0.0057,  0.0325])\n",
      "Epoch: 2658, Loss: 2.9308412075042725 \n",
      "              Params: tensor([  5.3340, -17.1141])\n",
      "              Grad: tensor([-0.0057,  0.0325])\n",
      "Epoch: 2659, Loss: 2.930832624435425 \n",
      "              Params: tensor([  5.3341, -17.1144])\n",
      "              Grad: tensor([-0.0057,  0.0324])\n",
      "Epoch: 2660, Loss: 2.930821418762207 \n",
      "              Params: tensor([  5.3341, -17.1147])\n",
      "              Grad: tensor([-0.0057,  0.0323])\n",
      "Epoch: 2661, Loss: 2.9308106899261475 \n",
      "              Params: tensor([  5.3342, -17.1150])\n",
      "              Grad: tensor([-0.0057,  0.0323])\n",
      "Epoch: 2662, Loss: 2.9308013916015625 \n",
      "              Params: tensor([  5.3343, -17.1154])\n",
      "              Grad: tensor([-0.0057,  0.0322])\n",
      "Epoch: 2663, Loss: 2.930788278579712 \n",
      "              Params: tensor([  5.3343, -17.1157])\n",
      "              Grad: tensor([-0.0057,  0.0322])\n",
      "Epoch: 2664, Loss: 2.9307777881622314 \n",
      "              Params: tensor([  5.3344, -17.1160])\n",
      "              Grad: tensor([-0.0057,  0.0321])\n",
      "Epoch: 2665, Loss: 2.9307668209075928 \n",
      "              Params: tensor([  5.3344, -17.1163])\n",
      "              Grad: tensor([-0.0057,  0.0321])\n",
      "Epoch: 2666, Loss: 2.9307572841644287 \n",
      "              Params: tensor([  5.3345, -17.1166])\n",
      "              Grad: tensor([-0.0057,  0.0320])\n",
      "Epoch: 2667, Loss: 2.930745840072632 \n",
      "              Params: tensor([  5.3345, -17.1170])\n",
      "              Grad: tensor([-0.0056,  0.0320])\n",
      "Epoch: 2668, Loss: 2.9307355880737305 \n",
      "              Params: tensor([  5.3346, -17.1173])\n",
      "              Grad: tensor([-0.0056,  0.0319])\n",
      "Epoch: 2669, Loss: 2.9307243824005127 \n",
      "              Params: tensor([  5.3347, -17.1176])\n",
      "              Grad: tensor([-0.0056,  0.0319])\n",
      "Epoch: 2670, Loss: 2.9307150840759277 \n",
      "              Params: tensor([  5.3347, -17.1179])\n",
      "              Grad: tensor([-0.0056,  0.0318])\n",
      "Epoch: 2671, Loss: 2.930703639984131 \n",
      "              Params: tensor([  5.3348, -17.1182])\n",
      "              Grad: tensor([-0.0056,  0.0317])\n",
      "Epoch: 2672, Loss: 2.9306938648223877 \n",
      "              Params: tensor([  5.3348, -17.1186])\n",
      "              Grad: tensor([-0.0056,  0.0317])\n",
      "Epoch: 2673, Loss: 2.93068528175354 \n",
      "              Params: tensor([  5.3349, -17.1189])\n",
      "              Grad: tensor([-0.0056,  0.0316])\n",
      "Epoch: 2674, Loss: 2.930673837661743 \n",
      "              Params: tensor([  5.3349, -17.1192])\n",
      "              Grad: tensor([-0.0056,  0.0316])\n",
      "Epoch: 2675, Loss: 2.9306631088256836 \n",
      "              Params: tensor([  5.3350, -17.1195])\n",
      "              Grad: tensor([-0.0056,  0.0315])\n",
      "Epoch: 2676, Loss: 2.930654287338257 \n",
      "              Params: tensor([  5.3350, -17.1198])\n",
      "              Grad: tensor([-0.0056,  0.0315])\n",
      "Epoch: 2677, Loss: 2.9306440353393555 \n",
      "              Params: tensor([  5.3351, -17.1201])\n",
      "              Grad: tensor([-0.0055,  0.0314])\n",
      "Epoch: 2678, Loss: 2.930631160736084 \n",
      "              Params: tensor([  5.3352, -17.1204])\n",
      "              Grad: tensor([-0.0055,  0.0314])\n",
      "Epoch: 2679, Loss: 2.930621385574341 \n",
      "              Params: tensor([  5.3352, -17.1208])\n",
      "              Grad: tensor([-0.0055,  0.0313])\n",
      "Epoch: 2680, Loss: 2.9306130409240723 \n",
      "              Params: tensor([  5.3353, -17.1211])\n",
      "              Grad: tensor([-0.0055,  0.0313])\n",
      "Epoch: 2681, Loss: 2.930602788925171 \n",
      "              Params: tensor([  5.3353, -17.1214])\n",
      "              Grad: tensor([-0.0055,  0.0312])\n",
      "Epoch: 2682, Loss: 2.9305925369262695 \n",
      "              Params: tensor([  5.3354, -17.1217])\n",
      "              Grad: tensor([-0.0055,  0.0312])\n",
      "Epoch: 2683, Loss: 2.930582284927368 \n",
      "              Params: tensor([  5.3354, -17.1220])\n",
      "              Grad: tensor([-0.0055,  0.0311])\n",
      "Epoch: 2684, Loss: 2.9305710792541504 \n",
      "              Params: tensor([  5.3355, -17.1223])\n",
      "              Grad: tensor([-0.0055,  0.0310])\n",
      "Epoch: 2685, Loss: 2.9305620193481445 \n",
      "              Params: tensor([  5.3355, -17.1226])\n",
      "              Grad: tensor([-0.0055,  0.0310])\n",
      "Epoch: 2686, Loss: 2.9305520057678223 \n",
      "              Params: tensor([  5.3356, -17.1229])\n",
      "              Grad: tensor([-0.0055,  0.0309])\n",
      "Epoch: 2687, Loss: 2.9305427074432373 \n",
      "              Params: tensor([  5.3356, -17.1232])\n",
      "              Grad: tensor([-0.0055,  0.0309])\n",
      "Epoch: 2688, Loss: 2.9305336475372314 \n",
      "              Params: tensor([  5.3357, -17.1236])\n",
      "              Grad: tensor([-0.0055,  0.0308])\n",
      "Epoch: 2689, Loss: 2.930523157119751 \n",
      "              Params: tensor([  5.3358, -17.1239])\n",
      "              Grad: tensor([-0.0054,  0.0308])\n",
      "Epoch: 2690, Loss: 2.930513620376587 \n",
      "              Params: tensor([  5.3358, -17.1242])\n",
      "              Grad: tensor([-0.0054,  0.0307])\n",
      "Epoch: 2691, Loss: 2.930502414703369 \n",
      "              Params: tensor([  5.3359, -17.1245])\n",
      "              Grad: tensor([-0.0054,  0.0307])\n",
      "Epoch: 2692, Loss: 2.9304933547973633 \n",
      "              Params: tensor([  5.3359, -17.1248])\n",
      "              Grad: tensor([-0.0054,  0.0306])\n",
      "Epoch: 2693, Loss: 2.9304823875427246 \n",
      "              Params: tensor([  5.3360, -17.1251])\n",
      "              Grad: tensor([-0.0054,  0.0306])\n",
      "Epoch: 2694, Loss: 2.930474281311035 \n",
      "              Params: tensor([  5.3360, -17.1254])\n",
      "              Grad: tensor([-0.0054,  0.0305])\n",
      "Epoch: 2695, Loss: 2.930464267730713 \n",
      "              Params: tensor([  5.3361, -17.1257])\n",
      "              Grad: tensor([-0.0054,  0.0305])\n",
      "Epoch: 2696, Loss: 2.9304540157318115 \n",
      "              Params: tensor([  5.3361, -17.1260])\n",
      "              Grad: tensor([-0.0054,  0.0304])\n",
      "Epoch: 2697, Loss: 2.9304451942443848 \n",
      "              Params: tensor([  5.3362, -17.1263])\n",
      "              Grad: tensor([-0.0054,  0.0304])\n",
      "Epoch: 2698, Loss: 2.930436134338379 \n",
      "              Params: tensor([  5.3362, -17.1266])\n",
      "              Grad: tensor([-0.0054,  0.0303])\n",
      "Epoch: 2699, Loss: 2.9304261207580566 \n",
      "              Params: tensor([  5.3363, -17.1269])\n",
      "              Grad: tensor([-0.0054,  0.0303])\n",
      "Epoch: 2700, Loss: 2.9304163455963135 \n",
      "              Params: tensor([  5.3364, -17.1272])\n",
      "              Grad: tensor([-0.0054,  0.0302])\n",
      "Epoch: 2701, Loss: 2.930407762527466 \n",
      "              Params: tensor([  5.3364, -17.1275])\n",
      "              Grad: tensor([-0.0053,  0.0302])\n",
      "Epoch: 2702, Loss: 2.9303979873657227 \n",
      "              Params: tensor([  5.3365, -17.1278])\n",
      "              Grad: tensor([-0.0053,  0.0301])\n",
      "Epoch: 2703, Loss: 2.9303877353668213 \n",
      "              Params: tensor([  5.3365, -17.1281])\n",
      "              Grad: tensor([-0.0053,  0.0301])\n",
      "Epoch: 2704, Loss: 2.93038010597229 \n",
      "              Params: tensor([  5.3366, -17.1284])\n",
      "              Grad: tensor([-0.0053,  0.0300])\n",
      "Epoch: 2705, Loss: 2.930370330810547 \n",
      "              Params: tensor([  5.3366, -17.1287])\n",
      "              Grad: tensor([-0.0053,  0.0300])\n",
      "Epoch: 2706, Loss: 2.9303598403930664 \n",
      "              Params: tensor([  5.3367, -17.1290])\n",
      "              Grad: tensor([-0.0053,  0.0299])\n",
      "Epoch: 2707, Loss: 2.9303526878356934 \n",
      "              Params: tensor([  5.3367, -17.1293])\n",
      "              Grad: tensor([-0.0053,  0.0299])\n",
      "Epoch: 2708, Loss: 2.930342197418213 \n",
      "              Params: tensor([  5.3368, -17.1296])\n",
      "              Grad: tensor([-0.0053,  0.0298])\n",
      "Epoch: 2709, Loss: 2.9303345680236816 \n",
      "              Params: tensor([  5.3368, -17.1299])\n",
      "              Grad: tensor([-0.0053,  0.0298])\n",
      "Epoch: 2710, Loss: 2.9303247928619385 \n",
      "              Params: tensor([  5.3369, -17.1302])\n",
      "              Grad: tensor([-0.0053,  0.0297])\n",
      "Epoch: 2711, Loss: 2.930314540863037 \n",
      "              Params: tensor([  5.3369, -17.1305])\n",
      "              Grad: tensor([-0.0053,  0.0297])\n",
      "Epoch: 2712, Loss: 2.9303061962127686 \n",
      "              Params: tensor([  5.3370, -17.1308])\n",
      "              Grad: tensor([-0.0052,  0.0296])\n",
      "Epoch: 2713, Loss: 2.930297613143921 \n",
      "              Params: tensor([  5.3370, -17.1311])\n",
      "              Grad: tensor([-0.0052,  0.0296])\n",
      "Epoch: 2714, Loss: 2.930288076400757 \n",
      "              Params: tensor([  5.3371, -17.1314])\n",
      "              Grad: tensor([-0.0052,  0.0295])\n",
      "Epoch: 2715, Loss: 2.930278778076172 \n",
      "              Params: tensor([  5.3371, -17.1317])\n",
      "              Grad: tensor([-0.0052,  0.0295])\n",
      "Epoch: 2716, Loss: 2.930270195007324 \n",
      "              Params: tensor([  5.3372, -17.1320])\n",
      "              Grad: tensor([-0.0052,  0.0294])\n",
      "Epoch: 2717, Loss: 2.9302618503570557 \n",
      "              Params: tensor([  5.3372, -17.1323])\n",
      "              Grad: tensor([-0.0052,  0.0294])\n",
      "Epoch: 2718, Loss: 2.930253505706787 \n",
      "              Params: tensor([  5.3373, -17.1326])\n",
      "              Grad: tensor([-0.0052,  0.0293])\n",
      "Epoch: 2719, Loss: 2.930243730545044 \n",
      "              Params: tensor([  5.3373, -17.1329])\n",
      "              Grad: tensor([-0.0052,  0.0293])\n",
      "Epoch: 2720, Loss: 2.9302353858947754 \n",
      "              Params: tensor([  5.3374, -17.1332])\n",
      "              Grad: tensor([-0.0052,  0.0292])\n",
      "Epoch: 2721, Loss: 2.9302256107330322 \n",
      "              Params: tensor([  5.3375, -17.1334])\n",
      "              Grad: tensor([-0.0052,  0.0292])\n",
      "Epoch: 2722, Loss: 2.9302175045013428 \n",
      "              Params: tensor([  5.3375, -17.1337])\n",
      "              Grad: tensor([-0.0051,  0.0291])\n",
      "Epoch: 2723, Loss: 2.930209159851074 \n",
      "              Params: tensor([  5.3376, -17.1340])\n",
      "              Grad: tensor([-0.0051,  0.0291])\n",
      "Epoch: 2724, Loss: 2.9302010536193848 \n",
      "              Params: tensor([  5.3376, -17.1343])\n",
      "              Grad: tensor([-0.0051,  0.0290])\n",
      "Epoch: 2725, Loss: 2.930190324783325 \n",
      "              Params: tensor([  5.3377, -17.1346])\n",
      "              Grad: tensor([-0.0051,  0.0290])\n",
      "Epoch: 2726, Loss: 2.930182695388794 \n",
      "              Params: tensor([  5.3377, -17.1349])\n",
      "              Grad: tensor([-0.0051,  0.0289])\n",
      "Epoch: 2727, Loss: 2.930172920227051 \n",
      "              Params: tensor([  5.3378, -17.1352])\n",
      "              Grad: tensor([-0.0051,  0.0289])\n",
      "Epoch: 2728, Loss: 2.930166482925415 \n",
      "              Params: tensor([  5.3378, -17.1355])\n",
      "              Grad: tensor([-0.0051,  0.0288])\n",
      "Epoch: 2729, Loss: 2.9301564693450928 \n",
      "              Params: tensor([  5.3379, -17.1358])\n",
      "              Grad: tensor([-0.0051,  0.0288])\n",
      "Epoch: 2730, Loss: 2.9301488399505615 \n",
      "              Params: tensor([  5.3379, -17.1360])\n",
      "              Grad: tensor([-0.0051,  0.0287])\n",
      "Epoch: 2731, Loss: 2.9301393032073975 \n",
      "              Params: tensor([  5.3380, -17.1363])\n",
      "              Grad: tensor([-0.0051,  0.0287])\n",
      "Epoch: 2732, Loss: 2.930131435394287 \n",
      "              Params: tensor([  5.3380, -17.1366])\n",
      "              Grad: tensor([-0.0050,  0.0286])\n",
      "Epoch: 2733, Loss: 2.9301233291625977 \n",
      "              Params: tensor([  5.3381, -17.1369])\n",
      "              Grad: tensor([-0.0050,  0.0286])\n",
      "Epoch: 2734, Loss: 2.9301130771636963 \n",
      "              Params: tensor([  5.3381, -17.1372])\n",
      "              Grad: tensor([-0.0050,  0.0285])\n",
      "Epoch: 2735, Loss: 2.930107355117798 \n",
      "              Params: tensor([  5.3382, -17.1375])\n",
      "              Grad: tensor([-0.0051,  0.0285])\n",
      "Epoch: 2736, Loss: 2.93009877204895 \n",
      "              Params: tensor([  5.3382, -17.1378])\n",
      "              Grad: tensor([-0.0050,  0.0284])\n",
      "Epoch: 2737, Loss: 2.9300901889801025 \n",
      "              Params: tensor([  5.3383, -17.1380])\n",
      "              Grad: tensor([-0.0050,  0.0284])\n",
      "Epoch: 2738, Loss: 2.9300811290740967 \n",
      "              Params: tensor([  5.3383, -17.1383])\n",
      "              Grad: tensor([-0.0050,  0.0283])\n",
      "Epoch: 2739, Loss: 2.9300730228424072 \n",
      "              Params: tensor([  5.3384, -17.1386])\n",
      "              Grad: tensor([-0.0050,  0.0283])\n",
      "Epoch: 2740, Loss: 2.9300642013549805 \n",
      "              Params: tensor([  5.3384, -17.1389])\n",
      "              Grad: tensor([-0.0050,  0.0282])\n",
      "Epoch: 2741, Loss: 2.930056095123291 \n",
      "              Params: tensor([  5.3385, -17.1392])\n",
      "              Grad: tensor([-0.0050,  0.0282])\n",
      "Epoch: 2742, Loss: 2.9300484657287598 \n",
      "              Params: tensor([  5.3385, -17.1395])\n",
      "              Grad: tensor([-0.0050,  0.0281])\n",
      "Epoch: 2743, Loss: 2.9300413131713867 \n",
      "              Params: tensor([  5.3386, -17.1397])\n",
      "              Grad: tensor([-0.0050,  0.0281])\n",
      "Epoch: 2744, Loss: 2.9300315380096436 \n",
      "              Params: tensor([  5.3386, -17.1400])\n",
      "              Grad: tensor([-0.0050,  0.0280])\n",
      "Epoch: 2745, Loss: 2.9300217628479004 \n",
      "              Params: tensor([  5.3387, -17.1403])\n",
      "              Grad: tensor([-0.0050,  0.0280])\n",
      "Epoch: 2746, Loss: 2.930016279220581 \n",
      "              Params: tensor([  5.3387, -17.1406])\n",
      "              Grad: tensor([-0.0049,  0.0279])\n",
      "Epoch: 2747, Loss: 2.9300081729888916 \n",
      "              Params: tensor([  5.3388, -17.1409])\n",
      "              Grad: tensor([-0.0049,  0.0279])\n",
      "Epoch: 2748, Loss: 2.9300003051757812 \n",
      "              Params: tensor([  5.3388, -17.1411])\n",
      "              Grad: tensor([-0.0049,  0.0279])\n",
      "Epoch: 2749, Loss: 2.9299917221069336 \n",
      "              Params: tensor([  5.3389, -17.1414])\n",
      "              Grad: tensor([-0.0049,  0.0278])\n",
      "Epoch: 2750, Loss: 2.929983377456665 \n",
      "              Params: tensor([  5.3389, -17.1417])\n",
      "              Grad: tensor([-0.0049,  0.0278])\n",
      "Epoch: 2751, Loss: 2.9299752712249756 \n",
      "              Params: tensor([  5.3390, -17.1420])\n",
      "              Grad: tensor([-0.0049,  0.0277])\n",
      "Epoch: 2752, Loss: 2.9299681186676025 \n",
      "              Params: tensor([  5.3390, -17.1422])\n",
      "              Grad: tensor([-0.0049,  0.0277])\n",
      "Epoch: 2753, Loss: 2.9299604892730713 \n",
      "              Params: tensor([  5.3391, -17.1425])\n",
      "              Grad: tensor([-0.0049,  0.0276])\n",
      "Epoch: 2754, Loss: 2.929953098297119 \n",
      "              Params: tensor([  5.3391, -17.1428])\n",
      "              Grad: tensor([-0.0049,  0.0276])\n",
      "Epoch: 2755, Loss: 2.929945230484009 \n",
      "              Params: tensor([  5.3392, -17.1431])\n",
      "              Grad: tensor([-0.0049,  0.0275])\n",
      "Epoch: 2756, Loss: 2.929936408996582 \n",
      "              Params: tensor([  5.3392, -17.1433])\n",
      "              Grad: tensor([-0.0049,  0.0275])\n",
      "Epoch: 2757, Loss: 2.929928779602051 \n",
      "              Params: tensor([  5.3392, -17.1436])\n",
      "              Grad: tensor([-0.0049,  0.0274])\n",
      "Epoch: 2758, Loss: 2.9299211502075195 \n",
      "              Params: tensor([  5.3393, -17.1439])\n",
      "              Grad: tensor([-0.0048,  0.0274])\n",
      "Epoch: 2759, Loss: 2.9299135208129883 \n",
      "              Params: tensor([  5.3393, -17.1442])\n",
      "              Grad: tensor([-0.0049,  0.0273])\n",
      "Epoch: 2760, Loss: 2.9299049377441406 \n",
      "              Params: tensor([  5.3394, -17.1444])\n",
      "              Grad: tensor([-0.0048,  0.0273])\n",
      "Epoch: 2761, Loss: 2.9298958778381348 \n",
      "              Params: tensor([  5.3394, -17.1447])\n",
      "              Grad: tensor([-0.0048,  0.0272])\n",
      "Epoch: 2762, Loss: 2.9298911094665527 \n",
      "              Params: tensor([  5.3395, -17.1450])\n",
      "              Grad: tensor([-0.0048,  0.0272])\n",
      "Epoch: 2763, Loss: 2.9298818111419678 \n",
      "              Params: tensor([  5.3395, -17.1453])\n",
      "              Grad: tensor([-0.0048,  0.0271])\n",
      "Epoch: 2764, Loss: 2.9298746585845947 \n",
      "              Params: tensor([  5.3396, -17.1455])\n",
      "              Grad: tensor([-0.0048,  0.0271])\n",
      "Epoch: 2765, Loss: 2.929868459701538 \n",
      "              Params: tensor([  5.3396, -17.1458])\n",
      "              Grad: tensor([-0.0048,  0.0271])\n",
      "Epoch: 2766, Loss: 2.9298593997955322 \n",
      "              Params: tensor([  5.3397, -17.1461])\n",
      "              Grad: tensor([-0.0048,  0.0270])\n",
      "Epoch: 2767, Loss: 2.929851770401001 \n",
      "              Params: tensor([  5.3397, -17.1463])\n",
      "              Grad: tensor([-0.0048,  0.0270])\n",
      "Epoch: 2768, Loss: 2.929844856262207 \n",
      "              Params: tensor([  5.3398, -17.1466])\n",
      "              Grad: tensor([-0.0048,  0.0269])\n",
      "Epoch: 2769, Loss: 2.929837942123413 \n",
      "              Params: tensor([  5.3398, -17.1469])\n",
      "              Grad: tensor([-0.0047,  0.0269])\n",
      "Epoch: 2770, Loss: 2.9298300743103027 \n",
      "              Params: tensor([  5.3399, -17.1471])\n",
      "              Grad: tensor([-0.0047,  0.0268])\n",
      "Epoch: 2771, Loss: 2.9298219680786133 \n",
      "              Params: tensor([  5.3399, -17.1474])\n",
      "              Grad: tensor([-0.0047,  0.0268])\n",
      "Epoch: 2772, Loss: 2.929816246032715 \n",
      "              Params: tensor([  5.3400, -17.1477])\n",
      "              Grad: tensor([-0.0047,  0.0267])\n",
      "Epoch: 2773, Loss: 2.929806709289551 \n",
      "              Params: tensor([  5.3400, -17.1479])\n",
      "              Grad: tensor([-0.0047,  0.0267])\n",
      "Epoch: 2774, Loss: 2.9297995567321777 \n",
      "              Params: tensor([  5.3401, -17.1482])\n",
      "              Grad: tensor([-0.0047,  0.0266])\n",
      "Epoch: 2775, Loss: 2.9297935962677 \n",
      "              Params: tensor([  5.3401, -17.1485])\n",
      "              Grad: tensor([-0.0047,  0.0266])\n",
      "Epoch: 2776, Loss: 2.929786443710327 \n",
      "              Params: tensor([  5.3402, -17.1487])\n",
      "              Grad: tensor([-0.0047,  0.0266])\n",
      "Epoch: 2777, Loss: 2.9297780990600586 \n",
      "              Params: tensor([  5.3402, -17.1490])\n",
      "              Grad: tensor([-0.0047,  0.0265])\n",
      "Epoch: 2778, Loss: 2.9297711849212646 \n",
      "              Params: tensor([  5.3402, -17.1493])\n",
      "              Grad: tensor([-0.0047,  0.0265])\n",
      "Epoch: 2779, Loss: 2.929764747619629 \n",
      "              Params: tensor([  5.3403, -17.1495])\n",
      "              Grad: tensor([-0.0047,  0.0264])\n",
      "Epoch: 2780, Loss: 2.9297568798065186 \n",
      "              Params: tensor([  5.3403, -17.1498])\n",
      "              Grad: tensor([-0.0047,  0.0264])\n",
      "Epoch: 2781, Loss: 2.9297499656677246 \n",
      "              Params: tensor([  5.3404, -17.1501])\n",
      "              Grad: tensor([-0.0046,  0.0263])\n",
      "Epoch: 2782, Loss: 2.9297430515289307 \n",
      "              Params: tensor([  5.3404, -17.1503])\n",
      "              Grad: tensor([-0.0046,  0.0263])\n",
      "Epoch: 2783, Loss: 2.929734945297241 \n",
      "              Params: tensor([  5.3405, -17.1506])\n",
      "              Grad: tensor([-0.0046,  0.0262])\n",
      "Epoch: 2784, Loss: 2.9297292232513428 \n",
      "              Params: tensor([  5.3405, -17.1508])\n",
      "              Grad: tensor([-0.0047,  0.0262])\n",
      "Epoch: 2785, Loss: 2.9297220706939697 \n",
      "              Params: tensor([  5.3406, -17.1511])\n",
      "              Grad: tensor([-0.0046,  0.0262])\n",
      "Epoch: 2786, Loss: 2.9297142028808594 \n",
      "              Params: tensor([  5.3406, -17.1514])\n",
      "              Grad: tensor([-0.0046,  0.0261])\n",
      "Epoch: 2787, Loss: 2.929706573486328 \n",
      "              Params: tensor([  5.3407, -17.1516])\n",
      "              Grad: tensor([-0.0046,  0.0261])\n",
      "Epoch: 2788, Loss: 2.9297006130218506 \n",
      "              Params: tensor([  5.3407, -17.1519])\n",
      "              Grad: tensor([-0.0046,  0.0260])\n",
      "Epoch: 2789, Loss: 2.929692029953003 \n",
      "              Params: tensor([  5.3408, -17.1522])\n",
      "              Grad: tensor([-0.0046,  0.0260])\n",
      "Epoch: 2790, Loss: 2.929685115814209 \n",
      "              Params: tensor([  5.3408, -17.1524])\n",
      "              Grad: tensor([-0.0046,  0.0259])\n",
      "Epoch: 2791, Loss: 2.929680585861206 \n",
      "              Params: tensor([  5.3408, -17.1527])\n",
      "              Grad: tensor([-0.0046,  0.0259])\n",
      "Epoch: 2792, Loss: 2.9296722412109375 \n",
      "              Params: tensor([  5.3409, -17.1529])\n",
      "              Grad: tensor([-0.0046,  0.0258])\n",
      "Epoch: 2793, Loss: 2.929666042327881 \n",
      "              Params: tensor([  5.3409, -17.1532])\n",
      "              Grad: tensor([-0.0046,  0.0258])\n",
      "Epoch: 2794, Loss: 2.9296586513519287 \n",
      "              Params: tensor([  5.3410, -17.1534])\n",
      "              Grad: tensor([-0.0045,  0.0258])\n",
      "Epoch: 2795, Loss: 2.9296529293060303 \n",
      "              Params: tensor([  5.3410, -17.1537])\n",
      "              Grad: tensor([-0.0045,  0.0257])\n",
      "Epoch: 2796, Loss: 2.9296462535858154 \n",
      "              Params: tensor([  5.3411, -17.1540])\n",
      "              Grad: tensor([-0.0045,  0.0257])\n",
      "Epoch: 2797, Loss: 2.929638147354126 \n",
      "              Params: tensor([  5.3411, -17.1542])\n",
      "              Grad: tensor([-0.0045,  0.0256])\n",
      "Epoch: 2798, Loss: 2.9296319484710693 \n",
      "              Params: tensor([  5.3412, -17.1545])\n",
      "              Grad: tensor([-0.0045,  0.0256])\n",
      "Epoch: 2799, Loss: 2.929626226425171 \n",
      "              Params: tensor([  5.3412, -17.1547])\n",
      "              Grad: tensor([-0.0045,  0.0255])\n",
      "Epoch: 2800, Loss: 2.929619789123535 \n",
      "              Params: tensor([  5.3413, -17.1550])\n",
      "              Grad: tensor([-0.0045,  0.0255])\n",
      "Epoch: 2801, Loss: 2.9296114444732666 \n",
      "              Params: tensor([  5.3413, -17.1552])\n",
      "              Grad: tensor([-0.0045,  0.0254])\n",
      "Epoch: 2802, Loss: 2.9296045303344727 \n",
      "              Params: tensor([  5.3413, -17.1555])\n",
      "              Grad: tensor([-0.0045,  0.0254])\n",
      "Epoch: 2803, Loss: 2.9295997619628906 \n",
      "              Params: tensor([  5.3414, -17.1557])\n",
      "              Grad: tensor([-0.0045,  0.0254])\n",
      "Epoch: 2804, Loss: 2.9295923709869385 \n",
      "              Params: tensor([  5.3414, -17.1560])\n",
      "              Grad: tensor([-0.0045,  0.0253])\n",
      "Epoch: 2805, Loss: 2.929586172103882 \n",
      "              Params: tensor([  5.3415, -17.1562])\n",
      "              Grad: tensor([-0.0045,  0.0253])\n",
      "Epoch: 2806, Loss: 2.9295785427093506 \n",
      "              Params: tensor([  5.3415, -17.1565])\n",
      "              Grad: tensor([-0.0045,  0.0252])\n",
      "Epoch: 2807, Loss: 2.929572343826294 \n",
      "              Params: tensor([  5.3416, -17.1568])\n",
      "              Grad: tensor([-0.0044,  0.0252])\n",
      "Epoch: 2808, Loss: 2.929565668106079 \n",
      "              Params: tensor([  5.3416, -17.1570])\n",
      "              Grad: tensor([-0.0044,  0.0251])\n",
      "Epoch: 2809, Loss: 2.9295592308044434 \n",
      "              Params: tensor([  5.3417, -17.1573])\n",
      "              Grad: tensor([-0.0044,  0.0251])\n",
      "Epoch: 2810, Loss: 2.929551124572754 \n",
      "              Params: tensor([  5.3417, -17.1575])\n",
      "              Grad: tensor([-0.0044,  0.0251])\n",
      "Epoch: 2811, Loss: 2.9295454025268555 \n",
      "              Params: tensor([  5.3417, -17.1578])\n",
      "              Grad: tensor([-0.0044,  0.0250])\n",
      "Epoch: 2812, Loss: 2.9295403957366943 \n",
      "              Params: tensor([  5.3418, -17.1580])\n",
      "              Grad: tensor([-0.0044,  0.0250])\n",
      "Epoch: 2813, Loss: 2.9295334815979004 \n",
      "              Params: tensor([  5.3418, -17.1583])\n",
      "              Grad: tensor([-0.0044,  0.0249])\n",
      "Epoch: 2814, Loss: 2.929527521133423 \n",
      "              Params: tensor([  5.3419, -17.1585])\n",
      "              Grad: tensor([-0.0044,  0.0249])\n",
      "Epoch: 2815, Loss: 2.929520606994629 \n",
      "              Params: tensor([  5.3419, -17.1588])\n",
      "              Grad: tensor([-0.0044,  0.0249])\n",
      "Epoch: 2816, Loss: 2.929513454437256 \n",
      "              Params: tensor([  5.3420, -17.1590])\n",
      "              Grad: tensor([-0.0044,  0.0248])\n",
      "Epoch: 2817, Loss: 2.929507255554199 \n",
      "              Params: tensor([  5.3420, -17.1592])\n",
      "              Grad: tensor([-0.0043,  0.0248])\n",
      "Epoch: 2818, Loss: 2.9295005798339844 \n",
      "              Params: tensor([  5.3421, -17.1595])\n",
      "              Grad: tensor([-0.0044,  0.0247])\n",
      "Epoch: 2819, Loss: 2.9294958114624023 \n",
      "              Params: tensor([  5.3421, -17.1597])\n",
      "              Grad: tensor([-0.0044,  0.0247])\n",
      "Epoch: 2820, Loss: 2.9294893741607666 \n",
      "              Params: tensor([  5.3421, -17.1600])\n",
      "              Grad: tensor([-0.0044,  0.0246])\n",
      "Epoch: 2821, Loss: 2.9294822216033936 \n",
      "              Params: tensor([  5.3422, -17.1602])\n",
      "              Grad: tensor([-0.0043,  0.0246])\n",
      "Epoch: 2822, Loss: 2.9294755458831787 \n",
      "              Params: tensor([  5.3422, -17.1605])\n",
      "              Grad: tensor([-0.0043,  0.0246])\n",
      "Epoch: 2823, Loss: 2.929471015930176 \n",
      "              Params: tensor([  5.3423, -17.1607])\n",
      "              Grad: tensor([-0.0043,  0.0245])\n",
      "Epoch: 2824, Loss: 2.9294631481170654 \n",
      "              Params: tensor([  5.3423, -17.1610])\n",
      "              Grad: tensor([-0.0043,  0.0245])\n",
      "Epoch: 2825, Loss: 2.929457664489746 \n",
      "              Params: tensor([  5.3424, -17.1612])\n",
      "              Grad: tensor([-0.0043,  0.0244])\n",
      "Epoch: 2826, Loss: 2.929452419281006 \n",
      "              Params: tensor([  5.3424, -17.1615])\n",
      "              Grad: tensor([-0.0043,  0.0244])\n",
      "Epoch: 2827, Loss: 2.9294447898864746 \n",
      "              Params: tensor([  5.3424, -17.1617])\n",
      "              Grad: tensor([-0.0043,  0.0243])\n",
      "Epoch: 2828, Loss: 2.9294393062591553 \n",
      "              Params: tensor([  5.3425, -17.1619])\n",
      "              Grad: tensor([-0.0043,  0.0243])\n",
      "Epoch: 2829, Loss: 2.9294326305389404 \n",
      "              Params: tensor([  5.3425, -17.1622])\n",
      "              Grad: tensor([-0.0043,  0.0243])\n",
      "Epoch: 2830, Loss: 2.929426670074463 \n",
      "              Params: tensor([  5.3426, -17.1624])\n",
      "              Grad: tensor([-0.0043,  0.0242])\n",
      "Epoch: 2831, Loss: 2.9294214248657227 \n",
      "              Params: tensor([  5.3426, -17.1627])\n",
      "              Grad: tensor([-0.0043,  0.0242])\n",
      "Epoch: 2832, Loss: 2.929415225982666 \n",
      "              Params: tensor([  5.3427, -17.1629])\n",
      "              Grad: tensor([-0.0043,  0.0241])\n",
      "Epoch: 2833, Loss: 2.9294090270996094 \n",
      "              Params: tensor([  5.3427, -17.1632])\n",
      "              Grad: tensor([-0.0043,  0.0241])\n",
      "Epoch: 2834, Loss: 2.929403781890869 \n",
      "              Params: tensor([  5.3427, -17.1634])\n",
      "              Grad: tensor([-0.0043,  0.0241])\n",
      "Epoch: 2835, Loss: 2.929396152496338 \n",
      "              Params: tensor([  5.3428, -17.1636])\n",
      "              Grad: tensor([-0.0042,  0.0240])\n",
      "Epoch: 2836, Loss: 2.9293906688690186 \n",
      "              Params: tensor([  5.3428, -17.1639])\n",
      "              Grad: tensor([-0.0042,  0.0240])\n",
      "Epoch: 2837, Loss: 2.9293832778930664 \n",
      "              Params: tensor([  5.3429, -17.1641])\n",
      "              Grad: tensor([-0.0042,  0.0239])\n",
      "Epoch: 2838, Loss: 2.929380178451538 \n",
      "              Params: tensor([  5.3429, -17.1644])\n",
      "              Grad: tensor([-0.0042,  0.0239])\n",
      "Epoch: 2839, Loss: 2.929373025894165 \n",
      "              Params: tensor([  5.3430, -17.1646])\n",
      "              Grad: tensor([-0.0042,  0.0239])\n",
      "Epoch: 2840, Loss: 2.929367780685425 \n",
      "              Params: tensor([  5.3430, -17.1648])\n",
      "              Grad: tensor([-0.0042,  0.0238])\n",
      "Epoch: 2841, Loss: 2.92936110496521 \n",
      "              Params: tensor([  5.3430, -17.1651])\n",
      "              Grad: tensor([-0.0042,  0.0238])\n",
      "Epoch: 2842, Loss: 2.9293556213378906 \n",
      "              Params: tensor([  5.3431, -17.1653])\n",
      "              Grad: tensor([-0.0042,  0.0237])\n",
      "Epoch: 2843, Loss: 2.9293508529663086 \n",
      "              Params: tensor([  5.3431, -17.1655])\n",
      "              Grad: tensor([-0.0042,  0.0237])\n",
      "Epoch: 2844, Loss: 2.9293439388275146 \n",
      "              Params: tensor([  5.3432, -17.1658])\n",
      "              Grad: tensor([-0.0042,  0.0237])\n",
      "Epoch: 2845, Loss: 2.929338216781616 \n",
      "              Params: tensor([  5.3432, -17.1660])\n",
      "              Grad: tensor([-0.0042,  0.0236])\n",
      "Epoch: 2846, Loss: 2.9293320178985596 \n",
      "              Params: tensor([  5.3432, -17.1662])\n",
      "              Grad: tensor([-0.0042,  0.0236])\n",
      "Epoch: 2847, Loss: 2.929327964782715 \n",
      "              Params: tensor([  5.3433, -17.1665])\n",
      "              Grad: tensor([-0.0042,  0.0235])\n",
      "Epoch: 2848, Loss: 2.9293205738067627 \n",
      "              Params: tensor([  5.3433, -17.1667])\n",
      "              Grad: tensor([-0.0041,  0.0235])\n",
      "Epoch: 2849, Loss: 2.9293158054351807 \n",
      "              Params: tensor([  5.3434, -17.1670])\n",
      "              Grad: tensor([-0.0041,  0.0235])\n",
      "Epoch: 2850, Loss: 2.929309129714966 \n",
      "              Params: tensor([  5.3434, -17.1672])\n",
      "              Grad: tensor([-0.0041,  0.0234])\n",
      "Epoch: 2851, Loss: 2.9293036460876465 \n",
      "              Params: tensor([  5.3435, -17.1674])\n",
      "              Grad: tensor([-0.0041,  0.0234])\n",
      "Epoch: 2852, Loss: 2.92930006980896 \n",
      "              Params: tensor([  5.3435, -17.1677])\n",
      "              Grad: tensor([-0.0041,  0.0233])\n",
      "Epoch: 2853, Loss: 2.929293155670166 \n",
      "              Params: tensor([  5.3435, -17.1679])\n",
      "              Grad: tensor([-0.0041,  0.0233])\n",
      "Epoch: 2854, Loss: 2.929287910461426 \n",
      "              Params: tensor([  5.3436, -17.1681])\n",
      "              Grad: tensor([-0.0041,  0.0233])\n",
      "Epoch: 2855, Loss: 2.929281711578369 \n",
      "              Params: tensor([  5.3436, -17.1684])\n",
      "              Grad: tensor([-0.0041,  0.0232])\n",
      "Epoch: 2856, Loss: 2.929276943206787 \n",
      "              Params: tensor([  5.3437, -17.1686])\n",
      "              Grad: tensor([-0.0041,  0.0232])\n",
      "Epoch: 2857, Loss: 2.9292707443237305 \n",
      "              Params: tensor([  5.3437, -17.1688])\n",
      "              Grad: tensor([-0.0041,  0.0231])\n",
      "Epoch: 2858, Loss: 2.9292662143707275 \n",
      "              Params: tensor([  5.3437, -17.1690])\n",
      "              Grad: tensor([-0.0041,  0.0231])\n",
      "Epoch: 2859, Loss: 2.92926025390625 \n",
      "              Params: tensor([  5.3438, -17.1693])\n",
      "              Grad: tensor([-0.0041,  0.0231])\n",
      "Epoch: 2860, Loss: 2.9292550086975098 \n",
      "              Params: tensor([  5.3438, -17.1695])\n",
      "              Grad: tensor([-0.0041,  0.0230])\n",
      "Epoch: 2861, Loss: 2.9292502403259277 \n",
      "              Params: tensor([  5.3439, -17.1697])\n",
      "              Grad: tensor([-0.0041,  0.0230])\n",
      "Epoch: 2862, Loss: 2.929243564605713 \n",
      "              Params: tensor([  5.3439, -17.1700])\n",
      "              Grad: tensor([-0.0040,  0.0229])\n",
      "Epoch: 2863, Loss: 2.9292376041412354 \n",
      "              Params: tensor([  5.3439, -17.1702])\n",
      "              Grad: tensor([-0.0040,  0.0229])\n",
      "Epoch: 2864, Loss: 2.929234266281128 \n",
      "              Params: tensor([  5.3440, -17.1704])\n",
      "              Grad: tensor([-0.0040,  0.0229])\n",
      "Epoch: 2865, Loss: 2.9292280673980713 \n",
      "              Params: tensor([  5.3440, -17.1707])\n",
      "              Grad: tensor([-0.0040,  0.0228])\n",
      "Epoch: 2866, Loss: 2.9292221069335938 \n",
      "              Params: tensor([  5.3441, -17.1709])\n",
      "              Grad: tensor([-0.0040,  0.0228])\n",
      "Epoch: 2867, Loss: 2.9292166233062744 \n",
      "              Params: tensor([  5.3441, -17.1711])\n",
      "              Grad: tensor([-0.0040,  0.0227])\n",
      "Epoch: 2868, Loss: 2.929210901260376 \n",
      "              Params: tensor([  5.3441, -17.1713])\n",
      "              Grad: tensor([-0.0040,  0.0227])\n",
      "Epoch: 2869, Loss: 2.9292075634002686 \n",
      "              Params: tensor([  5.3442, -17.1716])\n",
      "              Grad: tensor([-0.0040,  0.0227])\n",
      "Epoch: 2870, Loss: 2.9292006492614746 \n",
      "              Params: tensor([  5.3442, -17.1718])\n",
      "              Grad: tensor([-0.0040,  0.0226])\n",
      "Epoch: 2871, Loss: 2.9291954040527344 \n",
      "              Params: tensor([  5.3443, -17.1720])\n",
      "              Grad: tensor([-0.0040,  0.0226])\n",
      "Epoch: 2872, Loss: 2.9291908740997314 \n",
      "              Params: tensor([  5.3443, -17.1722])\n",
      "              Grad: tensor([-0.0040,  0.0226])\n",
      "Epoch: 2873, Loss: 2.929184675216675 \n",
      "              Params: tensor([  5.3443, -17.1725])\n",
      "              Grad: tensor([-0.0040,  0.0225])\n",
      "Epoch: 2874, Loss: 2.929180383682251 \n",
      "              Params: tensor([  5.3444, -17.1727])\n",
      "              Grad: tensor([-0.0040,  0.0225])\n",
      "Epoch: 2875, Loss: 2.9291746616363525 \n",
      "              Params: tensor([  5.3444, -17.1729])\n",
      "              Grad: tensor([-0.0040,  0.0224])\n",
      "Epoch: 2876, Loss: 2.9291696548461914 \n",
      "              Params: tensor([  5.3445, -17.1731])\n",
      "              Grad: tensor([-0.0040,  0.0224])\n",
      "Epoch: 2877, Loss: 2.9291646480560303 \n",
      "              Params: tensor([  5.3445, -17.1734])\n",
      "              Grad: tensor([-0.0040,  0.0224])\n",
      "Epoch: 2878, Loss: 2.9291601181030273 \n",
      "              Params: tensor([  5.3445, -17.1736])\n",
      "              Grad: tensor([-0.0039,  0.0223])\n",
      "Epoch: 2879, Loss: 2.929154872894287 \n",
      "              Params: tensor([  5.3446, -17.1738])\n",
      "              Grad: tensor([-0.0039,  0.0223])\n",
      "Epoch: 2880, Loss: 2.9291486740112305 \n",
      "              Params: tensor([  5.3446, -17.1740])\n",
      "              Grad: tensor([-0.0039,  0.0223])\n",
      "Epoch: 2881, Loss: 2.9291434288024902 \n",
      "              Params: tensor([  5.3447, -17.1742])\n",
      "              Grad: tensor([-0.0039,  0.0222])\n",
      "Epoch: 2882, Loss: 2.9291391372680664 \n",
      "              Params: tensor([  5.3447, -17.1745])\n",
      "              Grad: tensor([-0.0039,  0.0222])\n",
      "Epoch: 2883, Loss: 2.929133415222168 \n",
      "              Params: tensor([  5.3447, -17.1747])\n",
      "              Grad: tensor([-0.0039,  0.0221])\n",
      "Epoch: 2884, Loss: 2.9291276931762695 \n",
      "              Params: tensor([  5.3448, -17.1749])\n",
      "              Grad: tensor([-0.0039,  0.0221])\n",
      "Epoch: 2885, Loss: 2.92912220954895 \n",
      "              Params: tensor([  5.3448, -17.1751])\n",
      "              Grad: tensor([-0.0039,  0.0221])\n",
      "Epoch: 2886, Loss: 2.9291188716888428 \n",
      "              Params: tensor([  5.3449, -17.1754])\n",
      "              Grad: tensor([-0.0039,  0.0220])\n",
      "Epoch: 2887, Loss: 2.9291129112243652 \n",
      "              Params: tensor([  5.3449, -17.1756])\n",
      "              Grad: tensor([-0.0039,  0.0220])\n",
      "Epoch: 2888, Loss: 2.929107904434204 \n",
      "              Params: tensor([  5.3449, -17.1758])\n",
      "              Grad: tensor([-0.0039,  0.0220])\n",
      "Epoch: 2889, Loss: 2.9291040897369385 \n",
      "              Params: tensor([  5.3450, -17.1760])\n",
      "              Grad: tensor([-0.0039,  0.0219])\n",
      "Epoch: 2890, Loss: 2.929098606109619 \n",
      "              Params: tensor([  5.3450, -17.1762])\n",
      "              Grad: tensor([-0.0039,  0.0219])\n",
      "Epoch: 2891, Loss: 2.9290926456451416 \n",
      "              Params: tensor([  5.3450, -17.1764])\n",
      "              Grad: tensor([-0.0039,  0.0218])\n",
      "Epoch: 2892, Loss: 2.9290883541107178 \n",
      "              Params: tensor([  5.3451, -17.1767])\n",
      "              Grad: tensor([-0.0039,  0.0218])\n",
      "Epoch: 2893, Loss: 2.9290831089019775 \n",
      "              Params: tensor([  5.3451, -17.1769])\n",
      "              Grad: tensor([-0.0038,  0.0218])\n",
      "Epoch: 2894, Loss: 2.929079294204712 \n",
      "              Params: tensor([  5.3452, -17.1771])\n",
      "              Grad: tensor([-0.0038,  0.0217])\n",
      "Epoch: 2895, Loss: 2.929074287414551 \n",
      "              Params: tensor([  5.3452, -17.1773])\n",
      "              Grad: tensor([-0.0038,  0.0217])\n",
      "Epoch: 2896, Loss: 2.9290685653686523 \n",
      "              Params: tensor([  5.3452, -17.1775])\n",
      "              Grad: tensor([-0.0038,  0.0217])\n",
      "Epoch: 2897, Loss: 2.9290647506713867 \n",
      "              Params: tensor([  5.3453, -17.1777])\n",
      "              Grad: tensor([-0.0038,  0.0216])\n",
      "Epoch: 2898, Loss: 2.929058313369751 \n",
      "              Params: tensor([  5.3453, -17.1780])\n",
      "              Grad: tensor([-0.0038,  0.0216])\n",
      "Epoch: 2899, Loss: 2.9290544986724854 \n",
      "              Params: tensor([  5.3454, -17.1782])\n",
      "              Grad: tensor([-0.0038,  0.0215])\n",
      "Epoch: 2900, Loss: 2.9290504455566406 \n",
      "              Params: tensor([  5.3454, -17.1784])\n",
      "              Grad: tensor([-0.0038,  0.0215])\n",
      "Epoch: 2901, Loss: 2.929043769836426 \n",
      "              Params: tensor([  5.3454, -17.1786])\n",
      "              Grad: tensor([-0.0038,  0.0215])\n",
      "Epoch: 2902, Loss: 2.9290411472320557 \n",
      "              Params: tensor([  5.3455, -17.1788])\n",
      "              Grad: tensor([-0.0038,  0.0214])\n",
      "Epoch: 2903, Loss: 2.9290359020233154 \n",
      "              Params: tensor([  5.3455, -17.1790])\n",
      "              Grad: tensor([-0.0038,  0.0214])\n",
      "Epoch: 2904, Loss: 2.9290311336517334 \n",
      "              Params: tensor([  5.3455, -17.1793])\n",
      "              Grad: tensor([-0.0038,  0.0214])\n",
      "Epoch: 2905, Loss: 2.929025411605835 \n",
      "              Params: tensor([  5.3456, -17.1795])\n",
      "              Grad: tensor([-0.0038,  0.0213])\n",
      "Epoch: 2906, Loss: 2.9290213584899902 \n",
      "              Params: tensor([  5.3456, -17.1797])\n",
      "              Grad: tensor([-0.0038,  0.0213])\n",
      "Epoch: 2907, Loss: 2.9290170669555664 \n",
      "              Params: tensor([  5.3457, -17.1799])\n",
      "              Grad: tensor([-0.0037,  0.0213])\n",
      "Epoch: 2908, Loss: 2.929011583328247 \n",
      "              Params: tensor([  5.3457, -17.1801])\n",
      "              Grad: tensor([-0.0037,  0.0212])\n",
      "Epoch: 2909, Loss: 2.929007053375244 \n",
      "              Params: tensor([  5.3457, -17.1803])\n",
      "              Grad: tensor([-0.0037,  0.0212])\n",
      "Epoch: 2910, Loss: 2.9290032386779785 \n",
      "              Params: tensor([  5.3458, -17.1805])\n",
      "              Grad: tensor([-0.0037,  0.0211])\n",
      "Epoch: 2911, Loss: 2.928999423980713 \n",
      "              Params: tensor([  5.3458, -17.1807])\n",
      "              Grad: tensor([-0.0037,  0.0211])\n",
      "Epoch: 2912, Loss: 2.9289934635162354 \n",
      "              Params: tensor([  5.3458, -17.1809])\n",
      "              Grad: tensor([-0.0037,  0.0211])\n",
      "Epoch: 2913, Loss: 2.9289894104003906 \n",
      "              Params: tensor([  5.3459, -17.1812])\n",
      "              Grad: tensor([-0.0037,  0.0210])\n",
      "Epoch: 2914, Loss: 2.9289846420288086 \n",
      "              Params: tensor([  5.3459, -17.1814])\n",
      "              Grad: tensor([-0.0037,  0.0210])\n",
      "Epoch: 2915, Loss: 2.9289801120758057 \n",
      "              Params: tensor([  5.3460, -17.1816])\n",
      "              Grad: tensor([-0.0037,  0.0210])\n",
      "Epoch: 2916, Loss: 2.92897629737854 \n",
      "              Params: tensor([  5.3460, -17.1818])\n",
      "              Grad: tensor([-0.0037,  0.0209])\n",
      "Epoch: 2917, Loss: 2.9289710521698 \n",
      "              Params: tensor([  5.3460, -17.1820])\n",
      "              Grad: tensor([-0.0037,  0.0209])\n",
      "Epoch: 2918, Loss: 2.9289674758911133 \n",
      "              Params: tensor([  5.3461, -17.1822])\n",
      "              Grad: tensor([-0.0037,  0.0209])\n",
      "Epoch: 2919, Loss: 2.928961992263794 \n",
      "              Params: tensor([  5.3461, -17.1824])\n",
      "              Grad: tensor([-0.0037,  0.0208])\n",
      "Epoch: 2920, Loss: 2.928957939147949 \n",
      "              Params: tensor([  5.3461, -17.1826])\n",
      "              Grad: tensor([-0.0037,  0.0208])\n",
      "Epoch: 2921, Loss: 2.9289534091949463 \n",
      "              Params: tensor([  5.3462, -17.1828])\n",
      "              Grad: tensor([-0.0037,  0.0208])\n",
      "Epoch: 2922, Loss: 2.9289474487304688 \n",
      "              Params: tensor([  5.3462, -17.1830])\n",
      "              Grad: tensor([-0.0036,  0.0207])\n",
      "Epoch: 2923, Loss: 2.928943395614624 \n",
      "              Params: tensor([  5.3462, -17.1832])\n",
      "              Grad: tensor([-0.0037,  0.0207])\n",
      "Epoch: 2924, Loss: 2.9289400577545166 \n",
      "              Params: tensor([  5.3463, -17.1834])\n",
      "              Grad: tensor([-0.0036,  0.0206])\n",
      "Epoch: 2925, Loss: 2.9289352893829346 \n",
      "              Params: tensor([  5.3463, -17.1837])\n",
      "              Grad: tensor([-0.0036,  0.0206])\n",
      "Epoch: 2926, Loss: 2.928931951522827 \n",
      "              Params: tensor([  5.3464, -17.1839])\n",
      "              Grad: tensor([-0.0036,  0.0206])\n",
      "Epoch: 2927, Loss: 2.9289262294769287 \n",
      "              Params: tensor([  5.3464, -17.1841])\n",
      "              Grad: tensor([-0.0036,  0.0205])\n",
      "Epoch: 2928, Loss: 2.9289231300354004 \n",
      "              Params: tensor([  5.3464, -17.1843])\n",
      "              Grad: tensor([-0.0036,  0.0205])\n",
      "Epoch: 2929, Loss: 2.9289186000823975 \n",
      "              Params: tensor([  5.3465, -17.1845])\n",
      "              Grad: tensor([-0.0036,  0.0205])\n",
      "Epoch: 2930, Loss: 2.928913116455078 \n",
      "              Params: tensor([  5.3465, -17.1847])\n",
      "              Grad: tensor([-0.0036,  0.0204])\n",
      "Epoch: 2931, Loss: 2.9289093017578125 \n",
      "              Params: tensor([  5.3465, -17.1849])\n",
      "              Grad: tensor([-0.0036,  0.0204])\n",
      "Epoch: 2932, Loss: 2.9289040565490723 \n",
      "              Params: tensor([  5.3466, -17.1851])\n",
      "              Grad: tensor([-0.0036,  0.0204])\n",
      "Epoch: 2933, Loss: 2.9289016723632812 \n",
      "              Params: tensor([  5.3466, -17.1853])\n",
      "              Grad: tensor([-0.0036,  0.0203])\n",
      "Epoch: 2934, Loss: 2.928896903991699 \n",
      "              Params: tensor([  5.3466, -17.1855])\n",
      "              Grad: tensor([-0.0036,  0.0203])\n",
      "Epoch: 2935, Loss: 2.9288930892944336 \n",
      "              Params: tensor([  5.3467, -17.1857])\n",
      "              Grad: tensor([-0.0036,  0.0203])\n",
      "Epoch: 2936, Loss: 2.928887367248535 \n",
      "              Params: tensor([  5.3467, -17.1859])\n",
      "              Grad: tensor([-0.0036,  0.0202])\n",
      "Epoch: 2937, Loss: 2.9288833141326904 \n",
      "              Params: tensor([  5.3468, -17.1861])\n",
      "              Grad: tensor([-0.0035,  0.0202])\n",
      "Epoch: 2938, Loss: 2.928880214691162 \n",
      "              Params: tensor([  5.3468, -17.1863])\n",
      "              Grad: tensor([-0.0036,  0.0202])\n",
      "Epoch: 2939, Loss: 2.928877830505371 \n",
      "              Params: tensor([  5.3468, -17.1865])\n",
      "              Grad: tensor([-0.0036,  0.0201])\n",
      "Epoch: 2940, Loss: 2.9288711547851562 \n",
      "              Params: tensor([  5.3469, -17.1867])\n",
      "              Grad: tensor([-0.0035,  0.0201])\n",
      "Epoch: 2941, Loss: 2.9288671016693115 \n",
      "              Params: tensor([  5.3469, -17.1869])\n",
      "              Grad: tensor([-0.0035,  0.0201])\n",
      "Epoch: 2942, Loss: 2.928863763809204 \n",
      "              Params: tensor([  5.3469, -17.1871])\n",
      "              Grad: tensor([-0.0035,  0.0200])\n",
      "Epoch: 2943, Loss: 2.9288597106933594 \n",
      "              Params: tensor([  5.3470, -17.1873])\n",
      "              Grad: tensor([-0.0035,  0.0200])\n",
      "Epoch: 2944, Loss: 2.9288551807403564 \n",
      "              Params: tensor([  5.3470, -17.1875])\n",
      "              Grad: tensor([-0.0035,  0.0200])\n",
      "Epoch: 2945, Loss: 2.9288504123687744 \n",
      "              Params: tensor([  5.3470, -17.1877])\n",
      "              Grad: tensor([-0.0035,  0.0199])\n",
      "Epoch: 2946, Loss: 2.9288454055786133 \n",
      "              Params: tensor([  5.3471, -17.1879])\n",
      "              Grad: tensor([-0.0035,  0.0199])\n",
      "Epoch: 2947, Loss: 2.928842782974243 \n",
      "              Params: tensor([  5.3471, -17.1881])\n",
      "              Grad: tensor([-0.0035,  0.0199])\n",
      "Epoch: 2948, Loss: 2.9288382530212402 \n",
      "              Params: tensor([  5.3471, -17.1883])\n",
      "              Grad: tensor([-0.0035,  0.0198])\n",
      "Epoch: 2949, Loss: 2.9288330078125 \n",
      "              Params: tensor([  5.3472, -17.1885])\n",
      "              Grad: tensor([-0.0035,  0.0198])\n",
      "Epoch: 2950, Loss: 2.928830146789551 \n",
      "              Params: tensor([  5.3472, -17.1887])\n",
      "              Grad: tensor([-0.0035,  0.0198])\n",
      "Epoch: 2951, Loss: 2.928826093673706 \n",
      "              Params: tensor([  5.3472, -17.1889])\n",
      "              Grad: tensor([-0.0035,  0.0197])\n",
      "Epoch: 2952, Loss: 2.9288225173950195 \n",
      "              Params: tensor([  5.3473, -17.1891])\n",
      "              Grad: tensor([-0.0035,  0.0197])\n",
      "Epoch: 2953, Loss: 2.9288175106048584 \n",
      "              Params: tensor([  5.3473, -17.1893])\n",
      "              Grad: tensor([-0.0035,  0.0197])\n",
      "Epoch: 2954, Loss: 2.9288156032562256 \n",
      "              Params: tensor([  5.3474, -17.1895])\n",
      "              Grad: tensor([-0.0035,  0.0196])\n",
      "Epoch: 2955, Loss: 2.9288108348846436 \n",
      "              Params: tensor([  5.3474, -17.1897])\n",
      "              Grad: tensor([-0.0035,  0.0196])\n",
      "Epoch: 2956, Loss: 2.928804636001587 \n",
      "              Params: tensor([  5.3474, -17.1899])\n",
      "              Grad: tensor([-0.0034,  0.0196])\n",
      "Epoch: 2957, Loss: 2.9288015365600586 \n",
      "              Params: tensor([  5.3475, -17.1901])\n",
      "              Grad: tensor([-0.0035,  0.0195])\n",
      "Epoch: 2958, Loss: 2.9287989139556885 \n",
      "              Params: tensor([  5.3475, -17.1903])\n",
      "              Grad: tensor([-0.0034,  0.0195])\n",
      "Epoch: 2959, Loss: 2.9287948608398438 \n",
      "              Params: tensor([  5.3475, -17.1905])\n",
      "              Grad: tensor([-0.0034,  0.0195])\n",
      "Epoch: 2960, Loss: 2.9287893772125244 \n",
      "              Params: tensor([  5.3476, -17.1907])\n",
      "              Grad: tensor([-0.0034,  0.0194])\n",
      "Epoch: 2961, Loss: 2.928788661956787 \n",
      "              Params: tensor([  5.3476, -17.1908])\n",
      "              Grad: tensor([-0.0034,  0.0194])\n",
      "Epoch: 2962, Loss: 2.9287827014923096 \n",
      "              Params: tensor([  5.3476, -17.1910])\n",
      "              Grad: tensor([-0.0034,  0.0194])\n",
      "Epoch: 2963, Loss: 2.928778648376465 \n",
      "              Params: tensor([  5.3477, -17.1912])\n",
      "              Grad: tensor([-0.0034,  0.0193])\n",
      "Epoch: 2964, Loss: 2.928774833679199 \n",
      "              Params: tensor([  5.3477, -17.1914])\n",
      "              Grad: tensor([-0.0034,  0.0193])\n",
      "Epoch: 2965, Loss: 2.9287710189819336 \n",
      "              Params: tensor([  5.3477, -17.1916])\n",
      "              Grad: tensor([-0.0034,  0.0193])\n",
      "Epoch: 2966, Loss: 2.9287667274475098 \n",
      "              Params: tensor([  5.3478, -17.1918])\n",
      "              Grad: tensor([-0.0034,  0.0192])\n",
      "Epoch: 2967, Loss: 2.928764581680298 \n",
      "              Params: tensor([  5.3478, -17.1920])\n",
      "              Grad: tensor([-0.0034,  0.0192])\n",
      "Epoch: 2968, Loss: 2.928760528564453 \n",
      "              Params: tensor([  5.3478, -17.1922])\n",
      "              Grad: tensor([-0.0034,  0.0192])\n",
      "Epoch: 2969, Loss: 2.928757667541504 \n",
      "              Params: tensor([  5.3479, -17.1924])\n",
      "              Grad: tensor([-0.0034,  0.0191])\n",
      "Epoch: 2970, Loss: 2.9287521839141846 \n",
      "              Params: tensor([  5.3479, -17.1926])\n",
      "              Grad: tensor([-0.0034,  0.0191])\n",
      "Epoch: 2971, Loss: 2.9287497997283936 \n",
      "              Params: tensor([  5.3479, -17.1928])\n",
      "              Grad: tensor([-0.0034,  0.0191])\n",
      "Epoch: 2972, Loss: 2.9287452697753906 \n",
      "              Params: tensor([  5.3480, -17.1930])\n",
      "              Grad: tensor([-0.0034,  0.0190])\n",
      "Epoch: 2973, Loss: 2.928741216659546 \n",
      "              Params: tensor([  5.3480, -17.1931])\n",
      "              Grad: tensor([-0.0034,  0.0190])\n",
      "Epoch: 2974, Loss: 2.9287374019622803 \n",
      "              Params: tensor([  5.3480, -17.1933])\n",
      "              Grad: tensor([-0.0034,  0.0190])\n",
      "Epoch: 2975, Loss: 2.92873477935791 \n",
      "              Params: tensor([  5.3481, -17.1935])\n",
      "              Grad: tensor([-0.0033,  0.0189])\n",
      "Epoch: 2976, Loss: 2.928729772567749 \n",
      "              Params: tensor([  5.3481, -17.1937])\n",
      "              Grad: tensor([-0.0033,  0.0189])\n",
      "Epoch: 2977, Loss: 2.928727149963379 \n",
      "              Params: tensor([  5.3481, -17.1939])\n",
      "              Grad: tensor([-0.0033,  0.0189])\n",
      "Epoch: 2978, Loss: 2.928722620010376 \n",
      "              Params: tensor([  5.3482, -17.1941])\n",
      "              Grad: tensor([-0.0033,  0.0188])\n",
      "Epoch: 2979, Loss: 2.9287185668945312 \n",
      "              Params: tensor([  5.3482, -17.1943])\n",
      "              Grad: tensor([-0.0033,  0.0188])\n",
      "Epoch: 2980, Loss: 2.928715705871582 \n",
      "              Params: tensor([  5.3482, -17.1945])\n",
      "              Grad: tensor([-0.0033,  0.0188])\n",
      "Epoch: 2981, Loss: 2.9287118911743164 \n",
      "              Params: tensor([  5.3483, -17.1947])\n",
      "              Grad: tensor([-0.0033,  0.0187])\n",
      "Epoch: 2982, Loss: 2.9287078380584717 \n",
      "              Params: tensor([  5.3483, -17.1948])\n",
      "              Grad: tensor([-0.0033,  0.0187])\n",
      "Epoch: 2983, Loss: 2.9287049770355225 \n",
      "              Params: tensor([  5.3483, -17.1950])\n",
      "              Grad: tensor([-0.0033,  0.0187])\n",
      "Epoch: 2984, Loss: 2.9286997318267822 \n",
      "              Params: tensor([  5.3484, -17.1952])\n",
      "              Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch: 2985, Loss: 2.9286980628967285 \n",
      "              Params: tensor([  5.3484, -17.1954])\n",
      "              Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch: 2986, Loss: 2.9286949634552 \n",
      "              Params: tensor([  5.3484, -17.1956])\n",
      "              Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch: 2987, Loss: 2.928690195083618 \n",
      "              Params: tensor([  5.3485, -17.1958])\n",
      "              Grad: tensor([-0.0033,  0.0186])\n",
      "Epoch: 2988, Loss: 2.928687334060669 \n",
      "              Params: tensor([  5.3485, -17.1960])\n",
      "              Grad: tensor([-0.0033,  0.0185])\n",
      "Epoch: 2989, Loss: 2.9286839962005615 \n",
      "              Params: tensor([  5.3485, -17.1961])\n",
      "              Grad: tensor([-0.0033,  0.0185])\n",
      "Epoch: 2990, Loss: 2.9286789894104004 \n",
      "              Params: tensor([  5.3486, -17.1963])\n",
      "              Grad: tensor([-0.0032,  0.0185])\n",
      "Epoch: 2991, Loss: 2.9286773204803467 \n",
      "              Params: tensor([  5.3486, -17.1965])\n",
      "              Grad: tensor([-0.0033,  0.0184])\n",
      "Epoch: 2992, Loss: 2.9286725521087646 \n",
      "              Params: tensor([  5.3486, -17.1967])\n",
      "              Grad: tensor([-0.0033,  0.0184])\n",
      "Epoch: 2993, Loss: 2.9286692142486572 \n",
      "              Params: tensor([  5.3487, -17.1969])\n",
      "              Grad: tensor([-0.0033,  0.0184])\n",
      "Epoch: 2994, Loss: 2.92866587638855 \n",
      "              Params: tensor([  5.3487, -17.1971])\n",
      "              Grad: tensor([-0.0032,  0.0183])\n",
      "Epoch: 2995, Loss: 2.9286623001098633 \n",
      "              Params: tensor([  5.3487, -17.1972])\n",
      "              Grad: tensor([-0.0032,  0.0183])\n",
      "Epoch: 2996, Loss: 2.928659677505493 \n",
      "              Params: tensor([  5.3488, -17.1974])\n",
      "              Grad: tensor([-0.0032,  0.0183])\n",
      "Epoch: 2997, Loss: 2.9286558628082275 \n",
      "              Params: tensor([  5.3488, -17.1976])\n",
      "              Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch: 2998, Loss: 2.9286506175994873 \n",
      "              Params: tensor([  5.3488, -17.1978])\n",
      "              Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch: 2999, Loss: 2.9286484718322754 \n",
      "              Params: tensor([  5.3489, -17.1980])\n",
      "              Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch: 3000, Loss: 2.9286458492279053 \n",
      "              Params: tensor([  5.3489, -17.1982])\n",
      "              Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch: 3001, Loss: 2.928643226623535 \n",
      "              Params: tensor([  5.3489, -17.1983])\n",
      "              Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch: 3002, Loss: 2.928637742996216 \n",
      "              Params: tensor([  5.3489, -17.1985])\n",
      "              Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch: 3003, Loss: 2.928635358810425 \n",
      "              Params: tensor([  5.3490, -17.1987])\n",
      "              Grad: tensor([-0.0032,  0.0181])\n",
      "Epoch: 3004, Loss: 2.9286317825317383 \n",
      "              Params: tensor([  5.3490, -17.1989])\n",
      "              Grad: tensor([-0.0032,  0.0180])\n",
      "Epoch: 3005, Loss: 2.92862868309021 \n",
      "              Params: tensor([  5.3490, -17.1991])\n",
      "              Grad: tensor([-0.0032,  0.0180])\n",
      "Epoch: 3006, Loss: 2.9286253452301025 \n",
      "              Params: tensor([  5.3491, -17.1992])\n",
      "              Grad: tensor([-0.0032,  0.0180])\n",
      "Epoch: 3007, Loss: 2.9286210536956787 \n",
      "              Params: tensor([  5.3491, -17.1994])\n",
      "              Grad: tensor([-0.0032,  0.0179])\n",
      "Epoch: 3008, Loss: 2.928617000579834 \n",
      "              Params: tensor([  5.3491, -17.1996])\n",
      "              Grad: tensor([-0.0032,  0.0179])\n",
      "Epoch: 3009, Loss: 2.9286158084869385 \n",
      "              Params: tensor([  5.3492, -17.1998])\n",
      "              Grad: tensor([-0.0032,  0.0179])\n",
      "Epoch: 3010, Loss: 2.928611993789673 \n",
      "              Params: tensor([  5.3492, -17.2000])\n",
      "              Grad: tensor([-0.0032,  0.0178])\n",
      "Epoch: 3011, Loss: 2.928607940673828 \n",
      "              Params: tensor([  5.3492, -17.2001])\n",
      "              Grad: tensor([-0.0032,  0.0178])\n",
      "Epoch: 3012, Loss: 2.9286043643951416 \n",
      "              Params: tensor([  5.3493, -17.2003])\n",
      "              Grad: tensor([-0.0031,  0.0178])\n",
      "Epoch: 3013, Loss: 2.9286012649536133 \n",
      "              Params: tensor([  5.3493, -17.2005])\n",
      "              Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch: 3014, Loss: 2.928598642349243 \n",
      "              Params: tensor([  5.3493, -17.2007])\n",
      "              Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch: 3015, Loss: 2.928595542907715 \n",
      "              Params: tensor([  5.3494, -17.2008])\n",
      "              Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch: 3016, Loss: 2.928591728210449 \n",
      "              Params: tensor([  5.3494, -17.2010])\n",
      "              Grad: tensor([-0.0031,  0.0177])\n",
      "Epoch: 3017, Loss: 2.9285881519317627 \n",
      "              Params: tensor([  5.3494, -17.2012])\n",
      "              Grad: tensor([-0.0031,  0.0176])\n",
      "Epoch: 3018, Loss: 2.928586483001709 \n",
      "              Params: tensor([  5.3495, -17.2014])\n",
      "              Grad: tensor([-0.0031,  0.0176])\n",
      "Epoch: 3019, Loss: 2.9285826683044434 \n",
      "              Params: tensor([  5.3495, -17.2015])\n",
      "              Grad: tensor([-0.0031,  0.0176])\n",
      "Epoch: 3020, Loss: 2.9285802841186523 \n",
      "              Params: tensor([  5.3495, -17.2017])\n",
      "              Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch: 3021, Loss: 2.9285759925842285 \n",
      "              Params: tensor([  5.3495, -17.2019])\n",
      "              Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch: 3022, Loss: 2.928574323654175 \n",
      "              Params: tensor([  5.3496, -17.2021])\n",
      "              Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch: 3023, Loss: 2.9285695552825928 \n",
      "              Params: tensor([  5.3496, -17.2022])\n",
      "              Grad: tensor([-0.0031,  0.0175])\n",
      "Epoch: 3024, Loss: 2.928567409515381 \n",
      "              Params: tensor([  5.3496, -17.2024])\n",
      "              Grad: tensor([-0.0031,  0.0174])\n",
      "Epoch: 3025, Loss: 2.9285638332366943 \n",
      "              Params: tensor([  5.3497, -17.2026])\n",
      "              Grad: tensor([-0.0031,  0.0174])\n",
      "Epoch: 3026, Loss: 2.928560733795166 \n",
      "              Params: tensor([  5.3497, -17.2028])\n",
      "              Grad: tensor([-0.0030,  0.0174])\n",
      "Epoch: 3027, Loss: 2.9285566806793213 \n",
      "              Params: tensor([  5.3497, -17.2029])\n",
      "              Grad: tensor([-0.0031,  0.0173])\n",
      "Epoch: 3028, Loss: 2.9285547733306885 \n",
      "              Params: tensor([  5.3498, -17.2031])\n",
      "              Grad: tensor([-0.0031,  0.0173])\n",
      "Epoch: 3029, Loss: 2.9285507202148438 \n",
      "              Params: tensor([  5.3498, -17.2033])\n",
      "              Grad: tensor([-0.0031,  0.0173])\n",
      "Epoch: 3030, Loss: 2.9285478591918945 \n",
      "              Params: tensor([  5.3498, -17.2035])\n",
      "              Grad: tensor([-0.0031,  0.0172])\n",
      "Epoch: 3031, Loss: 2.9285454750061035 \n",
      "              Params: tensor([  5.3498, -17.2036])\n",
      "              Grad: tensor([-0.0030,  0.0172])\n",
      "Epoch: 3032, Loss: 2.9285430908203125 \n",
      "              Params: tensor([  5.3499, -17.2038])\n",
      "              Grad: tensor([-0.0030,  0.0172])\n",
      "Epoch: 3033, Loss: 2.9285390377044678 \n",
      "              Params: tensor([  5.3499, -17.2040])\n",
      "              Grad: tensor([-0.0030,  0.0172])\n",
      "Epoch: 3034, Loss: 2.9285356998443604 \n",
      "              Params: tensor([  5.3499, -17.2041])\n",
      "              Grad: tensor([-0.0030,  0.0171])\n",
      "Epoch: 3035, Loss: 2.928532361984253 \n",
      "              Params: tensor([  5.3500, -17.2043])\n",
      "              Grad: tensor([-0.0030,  0.0171])\n",
      "Epoch: 3036, Loss: 2.9285309314727783 \n",
      "              Params: tensor([  5.3500, -17.2045])\n",
      "              Grad: tensor([-0.0030,  0.0171])\n",
      "Epoch: 3037, Loss: 2.92852783203125 \n",
      "              Params: tensor([  5.3500, -17.2047])\n",
      "              Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch: 3038, Loss: 2.9285240173339844 \n",
      "              Params: tensor([  5.3501, -17.2048])\n",
      "              Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch: 3039, Loss: 2.928521156311035 \n",
      "              Params: tensor([  5.3501, -17.2050])\n",
      "              Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch: 3040, Loss: 2.928518533706665 \n",
      "              Params: tensor([  5.3501, -17.2052])\n",
      "              Grad: tensor([-0.0030,  0.0170])\n",
      "Epoch: 3041, Loss: 2.928514242172241 \n",
      "              Params: tensor([  5.3502, -17.2053])\n",
      "              Grad: tensor([-0.0030,  0.0169])\n",
      "Epoch: 3042, Loss: 2.928511619567871 \n",
      "              Params: tensor([  5.3502, -17.2055])\n",
      "              Grad: tensor([-0.0030,  0.0169])\n",
      "Epoch: 3043, Loss: 2.928508758544922 \n",
      "              Params: tensor([  5.3502, -17.2057])\n",
      "              Grad: tensor([-0.0030,  0.0169])\n",
      "Epoch: 3044, Loss: 2.9285051822662354 \n",
      "              Params: tensor([  5.3502, -17.2058])\n",
      "              Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch: 3045, Loss: 2.9285032749176025 \n",
      "              Params: tensor([  5.3503, -17.2060])\n",
      "              Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch: 3046, Loss: 2.928500175476074 \n",
      "              Params: tensor([  5.3503, -17.2062])\n",
      "              Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch: 3047, Loss: 2.928497552871704 \n",
      "              Params: tensor([  5.3503, -17.2063])\n",
      "              Grad: tensor([-0.0030,  0.0168])\n",
      "Epoch: 3048, Loss: 2.928495168685913 \n",
      "              Params: tensor([  5.3504, -17.2065])\n",
      "              Grad: tensor([-0.0030,  0.0167])\n",
      "Epoch: 3049, Loss: 2.92849063873291 \n",
      "              Params: tensor([  5.3504, -17.2067])\n",
      "              Grad: tensor([-0.0030,  0.0167])\n",
      "Epoch: 3050, Loss: 2.9284887313842773 \n",
      "              Params: tensor([  5.3504, -17.2068])\n",
      "              Grad: tensor([-0.0030,  0.0167])\n",
      "Epoch: 3051, Loss: 2.9284861087799072 \n",
      "              Params: tensor([  5.3504, -17.2070])\n",
      "              Grad: tensor([-0.0029,  0.0166])\n",
      "Epoch: 3052, Loss: 2.9284844398498535 \n",
      "              Params: tensor([  5.3505, -17.2072])\n",
      "              Grad: tensor([-0.0029,  0.0166])\n",
      "Epoch: 3053, Loss: 2.928480625152588 \n",
      "              Params: tensor([  5.3505, -17.2073])\n",
      "              Grad: tensor([-0.0029,  0.0166])\n",
      "Epoch: 3054, Loss: 2.9284772872924805 \n",
      "              Params: tensor([  5.3505, -17.2075])\n",
      "              Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch: 3055, Loss: 2.9284744262695312 \n",
      "              Params: tensor([  5.3506, -17.2077])\n",
      "              Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch: 3056, Loss: 2.9284722805023193 \n",
      "              Params: tensor([  5.3506, -17.2078])\n",
      "              Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch: 3057, Loss: 2.928469181060791 \n",
      "              Params: tensor([  5.3506, -17.2080])\n",
      "              Grad: tensor([-0.0029,  0.0165])\n",
      "Epoch: 3058, Loss: 2.9284677505493164 \n",
      "              Params: tensor([  5.3507, -17.2082])\n",
      "              Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch: 3059, Loss: 2.9284627437591553 \n",
      "              Params: tensor([  5.3507, -17.2083])\n",
      "              Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch: 3060, Loss: 2.928460121154785 \n",
      "              Params: tensor([  5.3507, -17.2085])\n",
      "              Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch: 3061, Loss: 2.9284582138061523 \n",
      "              Params: tensor([  5.3507, -17.2087])\n",
      "              Grad: tensor([-0.0029,  0.0164])\n",
      "Epoch: 3062, Loss: 2.9284555912017822 \n",
      "              Params: tensor([  5.3508, -17.2088])\n",
      "              Grad: tensor([-0.0029,  0.0163])\n",
      "Epoch: 3063, Loss: 2.928452491760254 \n",
      "              Params: tensor([  5.3508, -17.2090])\n",
      "              Grad: tensor([-0.0029,  0.0163])\n",
      "Epoch: 3064, Loss: 2.9284493923187256 \n",
      "              Params: tensor([  5.3508, -17.2091])\n",
      "              Grad: tensor([-0.0029,  0.0163])\n",
      "Epoch: 3065, Loss: 2.9284470081329346 \n",
      "              Params: tensor([  5.3509, -17.2093])\n",
      "              Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch: 3066, Loss: 2.928443193435669 \n",
      "              Params: tensor([  5.3509, -17.2095])\n",
      "              Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch: 3067, Loss: 2.928443670272827 \n",
      "              Params: tensor([  5.3509, -17.2096])\n",
      "              Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch: 3068, Loss: 2.9284400939941406 \n",
      "              Params: tensor([  5.3509, -17.2098])\n",
      "              Grad: tensor([-0.0029,  0.0162])\n",
      "Epoch: 3069, Loss: 2.9284353256225586 \n",
      "              Params: tensor([  5.3510, -17.2100])\n",
      "              Grad: tensor([-0.0029,  0.0161])\n",
      "Epoch: 3070, Loss: 2.9284353256225586 \n",
      "              Params: tensor([  5.3510, -17.2101])\n",
      "              Grad: tensor([-0.0029,  0.0161])\n",
      "Epoch: 3071, Loss: 2.92842960357666 \n",
      "              Params: tensor([  5.3510, -17.2103])\n",
      "              Grad: tensor([-0.0028,  0.0161])\n",
      "Epoch: 3072, Loss: 2.9284284114837646 \n",
      "              Params: tensor([  5.3511, -17.2104])\n",
      "              Grad: tensor([-0.0028,  0.0161])\n",
      "Epoch: 3073, Loss: 2.9284262657165527 \n",
      "              Params: tensor([  5.3511, -17.2106])\n",
      "              Grad: tensor([-0.0028,  0.0160])\n",
      "Epoch: 3074, Loss: 2.928422689437866 \n",
      "              Params: tensor([  5.3511, -17.2108])\n",
      "              Grad: tensor([-0.0028,  0.0160])\n",
      "Epoch: 3075, Loss: 2.9284212589263916 \n",
      "              Params: tensor([  5.3511, -17.2109])\n",
      "              Grad: tensor([-0.0028,  0.0160])\n",
      "Epoch: 3076, Loss: 2.928417205810547 \n",
      "              Params: tensor([  5.3512, -17.2111])\n",
      "              Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch: 3077, Loss: 2.928415536880493 \n",
      "              Params: tensor([  5.3512, -17.2112])\n",
      "              Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch: 3078, Loss: 2.928410530090332 \n",
      "              Params: tensor([  5.3512, -17.2114])\n",
      "              Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch: 3079, Loss: 2.928410291671753 \n",
      "              Params: tensor([  5.3512, -17.2116])\n",
      "              Grad: tensor([-0.0028,  0.0159])\n",
      "Epoch: 3080, Loss: 2.9284071922302246 \n",
      "              Params: tensor([  5.3513, -17.2117])\n",
      "              Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch: 3081, Loss: 2.9284040927886963 \n",
      "              Params: tensor([  5.3513, -17.2119])\n",
      "              Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch: 3082, Loss: 2.9284021854400635 \n",
      "              Params: tensor([  5.3513, -17.2120])\n",
      "              Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch: 3083, Loss: 2.928398847579956 \n",
      "              Params: tensor([  5.3514, -17.2122])\n",
      "              Grad: tensor([-0.0028,  0.0158])\n",
      "Epoch: 3084, Loss: 2.9283957481384277 \n",
      "              Params: tensor([  5.3514, -17.2123])\n",
      "              Grad: tensor([-0.0028,  0.0157])\n",
      "Epoch: 3085, Loss: 2.9283952713012695 \n",
      "              Params: tensor([  5.3514, -17.2125])\n",
      "              Grad: tensor([-0.0028,  0.0157])\n",
      "Epoch: 3086, Loss: 2.928391933441162 \n",
      "              Params: tensor([  5.3514, -17.2127])\n",
      "              Grad: tensor([-0.0027,  0.0157])\n",
      "Epoch: 3087, Loss: 2.928388833999634 \n",
      "              Params: tensor([  5.3515, -17.2128])\n",
      "              Grad: tensor([-0.0027,  0.0157])\n",
      "Epoch: 3088, Loss: 2.9283859729766846 \n",
      "              Params: tensor([  5.3515, -17.2130])\n",
      "              Grad: tensor([-0.0027,  0.0156])\n",
      "Epoch: 3089, Loss: 2.9283828735351562 \n",
      "              Params: tensor([  5.3515, -17.2131])\n",
      "              Grad: tensor([-0.0028,  0.0156])\n",
      "Epoch: 3090, Loss: 2.928382158279419 \n",
      "              Params: tensor([  5.3516, -17.2133])\n",
      "              Grad: tensor([-0.0028,  0.0156])\n",
      "Epoch: 3091, Loss: 2.9283790588378906 \n",
      "              Params: tensor([  5.3516, -17.2134])\n",
      "              Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch: 3092, Loss: 2.928378105163574 \n",
      "              Params: tensor([  5.3516, -17.2136])\n",
      "              Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch: 3093, Loss: 2.928375005722046 \n",
      "              Params: tensor([  5.3516, -17.2137])\n",
      "              Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch: 3094, Loss: 2.928372383117676 \n",
      "              Params: tensor([  5.3517, -17.2139])\n",
      "              Grad: tensor([-0.0027,  0.0155])\n",
      "Epoch: 3095, Loss: 2.928370475769043 \n",
      "              Params: tensor([  5.3517, -17.2141])\n",
      "              Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch: 3096, Loss: 2.928368330001831 \n",
      "              Params: tensor([  5.3517, -17.2142])\n",
      "              Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch: 3097, Loss: 2.9283640384674072 \n",
      "              Params: tensor([  5.3517, -17.2144])\n",
      "              Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch: 3098, Loss: 2.9283623695373535 \n",
      "              Params: tensor([  5.3518, -17.2145])\n",
      "              Grad: tensor([-0.0027,  0.0154])\n",
      "Epoch: 3099, Loss: 2.928360939025879 \n",
      "              Params: tensor([  5.3518, -17.2147])\n",
      "              Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch: 3100, Loss: 2.928356409072876 \n",
      "              Params: tensor([  5.3518, -17.2148])\n",
      "              Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch: 3101, Loss: 2.9283552169799805 \n",
      "              Params: tensor([  5.3519, -17.2150])\n",
      "              Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch: 3102, Loss: 2.9283533096313477 \n",
      "              Params: tensor([  5.3519, -17.2151])\n",
      "              Grad: tensor([-0.0027,  0.0153])\n",
      "Epoch: 3103, Loss: 2.9283487796783447 \n",
      "              Params: tensor([  5.3519, -17.2153])\n",
      "              Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch: 3104, Loss: 2.9283478260040283 \n",
      "              Params: tensor([  5.3519, -17.2154])\n",
      "              Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch: 3105, Loss: 2.928344964981079 \n",
      "              Params: tensor([  5.3520, -17.2156])\n",
      "              Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch: 3106, Loss: 2.9283432960510254 \n",
      "              Params: tensor([  5.3520, -17.2157])\n",
      "              Grad: tensor([-0.0027,  0.0152])\n",
      "Epoch: 3107, Loss: 2.928339958190918 \n",
      "              Params: tensor([  5.3520, -17.2159])\n",
      "              Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch: 3108, Loss: 2.9283385276794434 \n",
      "              Params: tensor([  5.3520, -17.2160])\n",
      "              Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch: 3109, Loss: 2.9283370971679688 \n",
      "              Params: tensor([  5.3521, -17.2162])\n",
      "              Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch: 3110, Loss: 2.928333282470703 \n",
      "              Params: tensor([  5.3521, -17.2163])\n",
      "              Grad: tensor([-0.0027,  0.0151])\n",
      "Epoch: 3111, Loss: 2.9283316135406494 \n",
      "              Params: tensor([  5.3521, -17.2165])\n",
      "              Grad: tensor([-0.0027,  0.0150])\n",
      "Epoch: 3112, Loss: 2.928328037261963 \n",
      "              Params: tensor([  5.3521, -17.2166])\n",
      "              Grad: tensor([-0.0026,  0.0150])\n",
      "Epoch: 3113, Loss: 2.9283287525177 \n",
      "              Params: tensor([  5.3522, -17.2168])\n",
      "              Grad: tensor([-0.0027,  0.0150])\n",
      "Epoch: 3114, Loss: 2.928323984146118 \n",
      "              Params: tensor([  5.3522, -17.2169])\n",
      "              Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch: 3115, Loss: 2.9283227920532227 \n",
      "              Params: tensor([  5.3522, -17.2171])\n",
      "              Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch: 3116, Loss: 2.9283204078674316 \n",
      "              Params: tensor([  5.3523, -17.2172])\n",
      "              Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch: 3117, Loss: 2.928318738937378 \n",
      "              Params: tensor([  5.3523, -17.2174])\n",
      "              Grad: tensor([-0.0026,  0.0149])\n",
      "Epoch: 3118, Loss: 2.9283149242401123 \n",
      "              Params: tensor([  5.3523, -17.2175])\n",
      "              Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch: 3119, Loss: 2.9283125400543213 \n",
      "              Params: tensor([  5.3523, -17.2177])\n",
      "              Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch: 3120, Loss: 2.9283103942871094 \n",
      "              Params: tensor([  5.3524, -17.2178])\n",
      "              Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch: 3121, Loss: 2.9283082485198975 \n",
      "              Params: tensor([  5.3524, -17.2180])\n",
      "              Grad: tensor([-0.0026,  0.0148])\n",
      "Epoch: 3122, Loss: 2.9283058643341064 \n",
      "              Params: tensor([  5.3524, -17.2181])\n",
      "              Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch: 3123, Loss: 2.9283041954040527 \n",
      "              Params: tensor([  5.3524, -17.2183])\n",
      "              Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch: 3124, Loss: 2.928302526473999 \n",
      "              Params: tensor([  5.3525, -17.2184])\n",
      "              Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch: 3125, Loss: 2.9282991886138916 \n",
      "              Params: tensor([  5.3525, -17.2186])\n",
      "              Grad: tensor([-0.0026,  0.0147])\n",
      "Epoch: 3126, Loss: 2.9282960891723633 \n",
      "              Params: tensor([  5.3525, -17.2187])\n",
      "              Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch: 3127, Loss: 2.928295135498047 \n",
      "              Params: tensor([  5.3525, -17.2189])\n",
      "              Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch: 3128, Loss: 2.928292751312256 \n",
      "              Params: tensor([  5.3526, -17.2190])\n",
      "              Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch: 3129, Loss: 2.9282913208007812 \n",
      "              Params: tensor([  5.3526, -17.2192])\n",
      "              Grad: tensor([-0.0026,  0.0146])\n",
      "Epoch: 3130, Loss: 2.9282877445220947 \n",
      "              Params: tensor([  5.3526, -17.2193])\n",
      "              Grad: tensor([-0.0026,  0.0145])\n",
      "Epoch: 3131, Loss: 2.9282867908477783 \n",
      "              Params: tensor([  5.3526, -17.2194])\n",
      "              Grad: tensor([-0.0026,  0.0145])\n",
      "Epoch: 3132, Loss: 2.9282846450805664 \n",
      "              Params: tensor([  5.3527, -17.2196])\n",
      "              Grad: tensor([-0.0025,  0.0145])\n",
      "Epoch: 3133, Loss: 2.9282822608947754 \n",
      "              Params: tensor([  5.3527, -17.2197])\n",
      "              Grad: tensor([-0.0026,  0.0145])\n",
      "Epoch: 3134, Loss: 2.9282796382904053 \n",
      "              Params: tensor([  5.3527, -17.2199])\n",
      "              Grad: tensor([-0.0026,  0.0144])\n",
      "Epoch: 3135, Loss: 2.928276300430298 \n",
      "              Params: tensor([  5.3527, -17.2200])\n",
      "              Grad: tensor([-0.0025,  0.0144])\n",
      "Epoch: 3136, Loss: 2.9282753467559814 \n",
      "              Params: tensor([  5.3528, -17.2202])\n",
      "              Grad: tensor([-0.0026,  0.0144])\n",
      "Epoch: 3137, Loss: 2.9282732009887695 \n",
      "              Params: tensor([  5.3528, -17.2203])\n",
      "              Grad: tensor([-0.0025,  0.0144])\n",
      "Epoch: 3138, Loss: 2.9282712936401367 \n",
      "              Params: tensor([  5.3528, -17.2205])\n",
      "              Grad: tensor([-0.0025,  0.0144])\n",
      "Epoch: 3139, Loss: 2.9282681941986084 \n",
      "              Params: tensor([  5.3528, -17.2206])\n",
      "              Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch: 3140, Loss: 2.928266763687134 \n",
      "              Params: tensor([  5.3529, -17.2207])\n",
      "              Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch: 3141, Loss: 2.9282643795013428 \n",
      "              Params: tensor([  5.3529, -17.2209])\n",
      "              Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch: 3142, Loss: 2.9282631874084473 \n",
      "              Params: tensor([  5.3529, -17.2210])\n",
      "              Grad: tensor([-0.0025,  0.0143])\n",
      "Epoch: 3143, Loss: 2.928260087966919 \n",
      "              Params: tensor([  5.3529, -17.2212])\n",
      "              Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch: 3144, Loss: 2.9282593727111816 \n",
      "              Params: tensor([  5.3530, -17.2213])\n",
      "              Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch: 3145, Loss: 2.928256034851074 \n",
      "              Params: tensor([  5.3530, -17.2214])\n",
      "              Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch: 3146, Loss: 2.928255319595337 \n",
      "              Params: tensor([  5.3530, -17.2216])\n",
      "              Grad: tensor([-0.0025,  0.0142])\n",
      "Epoch: 3147, Loss: 2.9282517433166504 \n",
      "              Params: tensor([  5.3530, -17.2217])\n",
      "              Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch: 3148, Loss: 2.928250312805176 \n",
      "              Params: tensor([  5.3531, -17.2219])\n",
      "              Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch: 3149, Loss: 2.928248643875122 \n",
      "              Params: tensor([  5.3531, -17.2220])\n",
      "              Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch: 3150, Loss: 2.9282455444335938 \n",
      "              Params: tensor([  5.3531, -17.2222])\n",
      "              Grad: tensor([-0.0025,  0.0141])\n",
      "Epoch: 3151, Loss: 2.9282448291778564 \n",
      "              Params: tensor([  5.3531, -17.2223])\n",
      "              Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch: 3152, Loss: 2.928241729736328 \n",
      "              Params: tensor([  5.3532, -17.2224])\n",
      "              Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch: 3153, Loss: 2.928239345550537 \n",
      "              Params: tensor([  5.3532, -17.2226])\n",
      "              Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch: 3154, Loss: 2.928236484527588 \n",
      "              Params: tensor([  5.3532, -17.2227])\n",
      "              Grad: tensor([-0.0025,  0.0140])\n",
      "Epoch: 3155, Loss: 2.928236246109009 \n",
      "              Params: tensor([  5.3532, -17.2229])\n",
      "              Grad: tensor([-0.0024,  0.0139])\n",
      "Epoch: 3156, Loss: 2.9282326698303223 \n",
      "              Params: tensor([  5.3533, -17.2230])\n",
      "              Grad: tensor([-0.0025,  0.0139])\n",
      "Epoch: 3157, Loss: 2.9282312393188477 \n",
      "              Params: tensor([  5.3533, -17.2231])\n",
      "              Grad: tensor([-0.0024,  0.0139])\n",
      "Epoch: 3158, Loss: 2.928230047225952 \n",
      "              Params: tensor([  5.3533, -17.2233])\n",
      "              Grad: tensor([-0.0025,  0.0139])\n",
      "Epoch: 3159, Loss: 2.928227424621582 \n",
      "              Params: tensor([  5.3533, -17.2234])\n",
      "              Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch: 3160, Loss: 2.9282257556915283 \n",
      "              Params: tensor([  5.3534, -17.2235])\n",
      "              Grad: tensor([-0.0025,  0.0138])\n",
      "Epoch: 3161, Loss: 2.928224802017212 \n",
      "              Params: tensor([  5.3534, -17.2237])\n",
      "              Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch: 3162, Loss: 2.928222417831421 \n",
      "              Params: tensor([  5.3534, -17.2238])\n",
      "              Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch: 3163, Loss: 2.9282188415527344 \n",
      "              Params: tensor([  5.3534, -17.2240])\n",
      "              Grad: tensor([-0.0024,  0.0138])\n",
      "Epoch: 3164, Loss: 2.928218126296997 \n",
      "              Params: tensor([  5.3535, -17.2241])\n",
      "              Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch: 3165, Loss: 2.928215980529785 \n",
      "              Params: tensor([  5.3535, -17.2242])\n",
      "              Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch: 3166, Loss: 2.9282150268554688 \n",
      "              Params: tensor([  5.3535, -17.2244])\n",
      "              Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch: 3167, Loss: 2.9282121658325195 \n",
      "              Params: tensor([  5.3535, -17.2245])\n",
      "              Grad: tensor([-0.0024,  0.0137])\n",
      "Epoch: 3168, Loss: 2.928210973739624 \n",
      "              Params: tensor([  5.3536, -17.2246])\n",
      "              Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch: 3169, Loss: 2.928209066390991 \n",
      "              Params: tensor([  5.3536, -17.2248])\n",
      "              Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch: 3170, Loss: 2.928206443786621 \n",
      "              Params: tensor([  5.3536, -17.2249])\n",
      "              Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch: 3171, Loss: 2.9282050132751465 \n",
      "              Params: tensor([  5.3536, -17.2250])\n",
      "              Grad: tensor([-0.0024,  0.0136])\n",
      "Epoch: 3172, Loss: 2.928203582763672 \n",
      "              Params: tensor([  5.3537, -17.2252])\n",
      "              Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch: 3173, Loss: 2.928201913833618 \n",
      "              Params: tensor([  5.3537, -17.2253])\n",
      "              Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch: 3174, Loss: 2.9282000064849854 \n",
      "              Params: tensor([  5.3537, -17.2255])\n",
      "              Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch: 3175, Loss: 2.9281961917877197 \n",
      "              Params: tensor([  5.3537, -17.2256])\n",
      "              Grad: tensor([-0.0024,  0.0135])\n",
      "Epoch: 3176, Loss: 2.928194522857666 \n",
      "              Params: tensor([  5.3538, -17.2257])\n",
      "              Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch: 3177, Loss: 2.928194522857666 \n",
      "              Params: tensor([  5.3538, -17.2259])\n",
      "              Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch: 3178, Loss: 2.9281914234161377 \n",
      "              Params: tensor([  5.3538, -17.2260])\n",
      "              Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch: 3179, Loss: 2.928189992904663 \n",
      "              Params: tensor([  5.3538, -17.2261])\n",
      "              Grad: tensor([-0.0024,  0.0134])\n",
      "Epoch: 3180, Loss: 2.9281883239746094 \n",
      "              Params: tensor([  5.3538, -17.2263])\n",
      "              Grad: tensor([-0.0023,  0.0134])\n",
      "Epoch: 3181, Loss: 2.9281859397888184 \n",
      "              Params: tensor([  5.3539, -17.2264])\n",
      "              Grad: tensor([-0.0023,  0.0133])\n",
      "Epoch: 3182, Loss: 2.928184747695923 \n",
      "              Params: tensor([  5.3539, -17.2265])\n",
      "              Grad: tensor([-0.0024,  0.0133])\n",
      "Epoch: 3183, Loss: 2.9281835556030273 \n",
      "              Params: tensor([  5.3539, -17.2267])\n",
      "              Grad: tensor([-0.0023,  0.0133])\n",
      "Epoch: 3184, Loss: 2.9281821250915527 \n",
      "              Params: tensor([  5.3539, -17.2268])\n",
      "              Grad: tensor([-0.0024,  0.0133])\n",
      "Epoch: 3185, Loss: 2.9281797409057617 \n",
      "              Params: tensor([  5.3540, -17.2269])\n",
      "              Grad: tensor([-0.0024,  0.0132])\n",
      "Epoch: 3186, Loss: 2.928177833557129 \n",
      "              Params: tensor([  5.3540, -17.2271])\n",
      "              Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch: 3187, Loss: 2.928175210952759 \n",
      "              Params: tensor([  5.3540, -17.2272])\n",
      "              Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch: 3188, Loss: 2.9281723499298096 \n",
      "              Params: tensor([  5.3540, -17.2273])\n",
      "              Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch: 3189, Loss: 2.928170680999756 \n",
      "              Params: tensor([  5.3541, -17.2275])\n",
      "              Grad: tensor([-0.0023,  0.0132])\n",
      "Epoch: 3190, Loss: 2.9281699657440186 \n",
      "              Params: tensor([  5.3541, -17.2276])\n",
      "              Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch: 3191, Loss: 2.9281692504882812 \n",
      "              Params: tensor([  5.3541, -17.2277])\n",
      "              Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch: 3192, Loss: 2.9281668663024902 \n",
      "              Params: tensor([  5.3541, -17.2278])\n",
      "              Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch: 3193, Loss: 2.928164005279541 \n",
      "              Params: tensor([  5.3542, -17.2280])\n",
      "              Grad: tensor([-0.0023,  0.0131])\n",
      "Epoch: 3194, Loss: 2.9281632900238037 \n",
      "              Params: tensor([  5.3542, -17.2281])\n",
      "              Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch: 3195, Loss: 2.92816162109375 \n",
      "              Params: tensor([  5.3542, -17.2282])\n",
      "              Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch: 3196, Loss: 2.9281599521636963 \n",
      "              Params: tensor([  5.3542, -17.2284])\n",
      "              Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch: 3197, Loss: 2.9281575679779053 \n",
      "              Params: tensor([  5.3542, -17.2285])\n",
      "              Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch: 3198, Loss: 2.928157091140747 \n",
      "              Params: tensor([  5.3543, -17.2286])\n",
      "              Grad: tensor([-0.0023,  0.0130])\n",
      "Epoch: 3199, Loss: 2.9281539916992188 \n",
      "              Params: tensor([  5.3543, -17.2288])\n",
      "              Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch: 3200, Loss: 2.9281516075134277 \n",
      "              Params: tensor([  5.3543, -17.2289])\n",
      "              Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch: 3201, Loss: 2.9281492233276367 \n",
      "              Params: tensor([  5.3543, -17.2290])\n",
      "              Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch: 3202, Loss: 2.928149938583374 \n",
      "              Params: tensor([  5.3544, -17.2291])\n",
      "              Grad: tensor([-0.0023,  0.0129])\n",
      "Epoch: 3203, Loss: 2.928147077560425 \n",
      "              Params: tensor([  5.3544, -17.2293])\n",
      "              Grad: tensor([-0.0022,  0.0129])\n",
      "Epoch: 3204, Loss: 2.92814564704895 \n",
      "              Params: tensor([  5.3544, -17.2294])\n",
      "              Grad: tensor([-0.0023,  0.0128])\n",
      "Epoch: 3205, Loss: 2.9281439781188965 \n",
      "              Params: tensor([  5.3544, -17.2295])\n",
      "              Grad: tensor([-0.0023,  0.0128])\n",
      "Epoch: 3206, Loss: 2.9281418323516846 \n",
      "              Params: tensor([  5.3544, -17.2297])\n",
      "              Grad: tensor([-0.0023,  0.0128])\n",
      "Epoch: 3207, Loss: 2.928140163421631 \n",
      "              Params: tensor([  5.3545, -17.2298])\n",
      "              Grad: tensor([-0.0022,  0.0128])\n",
      "Epoch: 3208, Loss: 2.928138494491577 \n",
      "              Params: tensor([  5.3545, -17.2299])\n",
      "              Grad: tensor([-0.0022,  0.0127])\n",
      "Epoch: 3209, Loss: 2.9281365871429443 \n",
      "              Params: tensor([  5.3545, -17.2300])\n",
      "              Grad: tensor([-0.0023,  0.0127])\n",
      "Epoch: 3210, Loss: 2.9281346797943115 \n",
      "              Params: tensor([  5.3545, -17.2302])\n",
      "              Grad: tensor([-0.0023,  0.0127])\n",
      "Epoch: 3211, Loss: 2.9281349182128906 \n",
      "              Params: tensor([  5.3546, -17.2303])\n",
      "              Grad: tensor([-0.0023,  0.0127])\n",
      "Epoch: 3212, Loss: 2.9281325340270996 \n",
      "              Params: tensor([  5.3546, -17.2304])\n",
      "              Grad: tensor([-0.0022,  0.0127])\n",
      "Epoch: 3213, Loss: 2.928130865097046 \n",
      "              Params: tensor([  5.3546, -17.2305])\n",
      "              Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch: 3214, Loss: 2.9281301498413086 \n",
      "              Params: tensor([  5.3546, -17.2307])\n",
      "              Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch: 3215, Loss: 2.9281256198883057 \n",
      "              Params: tensor([  5.3546, -17.2308])\n",
      "              Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch: 3216, Loss: 2.9281249046325684 \n",
      "              Params: tensor([  5.3547, -17.2309])\n",
      "              Grad: tensor([-0.0022,  0.0126])\n",
      "Epoch: 3217, Loss: 2.928124189376831 \n",
      "              Params: tensor([  5.3547, -17.2310])\n",
      "              Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch: 3218, Loss: 2.9281210899353027 \n",
      "              Params: tensor([  5.3547, -17.2312])\n",
      "              Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch: 3219, Loss: 2.9281210899353027 \n",
      "              Params: tensor([  5.3547, -17.2313])\n",
      "              Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch: 3220, Loss: 2.9281203746795654 \n",
      "              Params: tensor([  5.3548, -17.2314])\n",
      "              Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch: 3221, Loss: 2.9281179904937744 \n",
      "              Params: tensor([  5.3548, -17.2315])\n",
      "              Grad: tensor([-0.0022,  0.0125])\n",
      "Epoch: 3222, Loss: 2.9281165599823 \n",
      "              Params: tensor([  5.3548, -17.2317])\n",
      "              Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch: 3223, Loss: 2.928115129470825 \n",
      "              Params: tensor([  5.3548, -17.2318])\n",
      "              Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch: 3224, Loss: 2.9281129837036133 \n",
      "              Params: tensor([  5.3548, -17.2319])\n",
      "              Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch: 3225, Loss: 2.928110361099243 \n",
      "              Params: tensor([  5.3549, -17.2320])\n",
      "              Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch: 3226, Loss: 2.9281091690063477 \n",
      "              Params: tensor([  5.3549, -17.2322])\n",
      "              Grad: tensor([-0.0022,  0.0124])\n",
      "Epoch: 3227, Loss: 2.9281082153320312 \n",
      "              Params: tensor([  5.3549, -17.2323])\n",
      "              Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch: 3228, Loss: 2.9281046390533447 \n",
      "              Params: tensor([  5.3549, -17.2324])\n",
      "              Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch: 3229, Loss: 2.9281046390533447 \n",
      "              Params: tensor([  5.3550, -17.2325])\n",
      "              Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch: 3230, Loss: 2.9281041622161865 \n",
      "              Params: tensor([  5.3550, -17.2327])\n",
      "              Grad: tensor([-0.0022,  0.0123])\n",
      "Epoch: 3231, Loss: 2.9281020164489746 \n",
      "              Params: tensor([  5.3550, -17.2328])\n",
      "              Grad: tensor([-0.0021,  0.0123])\n",
      "Epoch: 3232, Loss: 2.9281013011932373 \n",
      "              Params: tensor([  5.3550, -17.2329])\n",
      "              Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch: 3233, Loss: 2.928098201751709 \n",
      "              Params: tensor([  5.3550, -17.2330])\n",
      "              Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch: 3234, Loss: 2.9280970096588135 \n",
      "              Params: tensor([  5.3551, -17.2331])\n",
      "              Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch: 3235, Loss: 2.9280951023101807 \n",
      "              Params: tensor([  5.3551, -17.2333])\n",
      "              Grad: tensor([-0.0022,  0.0122])\n",
      "Epoch: 3236, Loss: 2.9280943870544434 \n",
      "              Params: tensor([  5.3551, -17.2334])\n",
      "              Grad: tensor([-0.0022,  0.0121])\n",
      "Epoch: 3237, Loss: 2.928093194961548 \n",
      "              Params: tensor([  5.3551, -17.2335])\n",
      "              Grad: tensor([-0.0022,  0.0121])\n",
      "Epoch: 3238, Loss: 2.928091287612915 \n",
      "              Params: tensor([  5.3551, -17.2336])\n",
      "              Grad: tensor([-0.0022,  0.0121])\n",
      "Epoch: 3239, Loss: 2.9280898571014404 \n",
      "              Params: tensor([  5.3552, -17.2338])\n",
      "              Grad: tensor([-0.0021,  0.0121])\n",
      "Epoch: 3240, Loss: 2.9280877113342285 \n",
      "              Params: tensor([  5.3552, -17.2339])\n",
      "              Grad: tensor([-0.0021,  0.0121])\n",
      "Epoch: 3241, Loss: 2.9280855655670166 \n",
      "              Params: tensor([  5.3552, -17.2340])\n",
      "              Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch: 3242, Loss: 2.9280853271484375 \n",
      "              Params: tensor([  5.3552, -17.2341])\n",
      "              Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch: 3243, Loss: 2.928083896636963 \n",
      "              Params: tensor([  5.3553, -17.2342])\n",
      "              Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch: 3244, Loss: 2.9280824661254883 \n",
      "              Params: tensor([  5.3553, -17.2344])\n",
      "              Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch: 3245, Loss: 2.928079843521118 \n",
      "              Params: tensor([  5.3553, -17.2345])\n",
      "              Grad: tensor([-0.0021,  0.0120])\n",
      "Epoch: 3246, Loss: 2.9280786514282227 \n",
      "              Params: tensor([  5.3553, -17.2346])\n",
      "              Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch: 3247, Loss: 2.9280762672424316 \n",
      "              Params: tensor([  5.3553, -17.2347])\n",
      "              Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch: 3248, Loss: 2.92807674407959 \n",
      "              Params: tensor([  5.3554, -17.2348])\n",
      "              Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch: 3249, Loss: 2.928074836730957 \n",
      "              Params: tensor([  5.3554, -17.2350])\n",
      "              Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch: 3250, Loss: 2.928072452545166 \n",
      "              Params: tensor([  5.3554, -17.2351])\n",
      "              Grad: tensor([-0.0021,  0.0119])\n",
      "Epoch: 3251, Loss: 2.9280717372894287 \n",
      "              Params: tensor([  5.3554, -17.2352])\n",
      "              Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch: 3252, Loss: 2.9280710220336914 \n",
      "              Params: tensor([  5.3554, -17.2353])\n",
      "              Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch: 3253, Loss: 2.9280683994293213 \n",
      "              Params: tensor([  5.3555, -17.2354])\n",
      "              Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch: 3254, Loss: 2.9280686378479004 \n",
      "              Params: tensor([  5.3555, -17.2355])\n",
      "              Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch: 3255, Loss: 2.928065538406372 \n",
      "              Params: tensor([  5.3555, -17.2357])\n",
      "              Grad: tensor([-0.0021,  0.0118])\n",
      "Epoch: 3256, Loss: 2.9280645847320557 \n",
      "              Params: tensor([  5.3555, -17.2358])\n",
      "              Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch: 3257, Loss: 2.9280638694763184 \n",
      "              Params: tensor([  5.3555, -17.2359])\n",
      "              Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch: 3258, Loss: 2.928061008453369 \n",
      "              Params: tensor([  5.3556, -17.2360])\n",
      "              Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch: 3259, Loss: 2.928060293197632 \n",
      "              Params: tensor([  5.3556, -17.2361])\n",
      "              Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch: 3260, Loss: 2.9280571937561035 \n",
      "              Params: tensor([  5.3556, -17.2362])\n",
      "              Grad: tensor([-0.0021,  0.0117])\n",
      "Epoch: 3261, Loss: 2.9280576705932617 \n",
      "              Params: tensor([  5.3556, -17.2364])\n",
      "              Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch: 3262, Loss: 2.928055763244629 \n",
      "              Params: tensor([  5.3557, -17.2365])\n",
      "              Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch: 3263, Loss: 2.9280545711517334 \n",
      "              Params: tensor([  5.3557, -17.2366])\n",
      "              Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch: 3264, Loss: 2.9280524253845215 \n",
      "              Params: tensor([  5.3557, -17.2367])\n",
      "              Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch: 3265, Loss: 2.9280526638031006 \n",
      "              Params: tensor([  5.3557, -17.2368])\n",
      "              Grad: tensor([-0.0021,  0.0116])\n",
      "Epoch: 3266, Loss: 2.9280507564544678 \n",
      "              Params: tensor([  5.3557, -17.2369])\n",
      "              Grad: tensor([-0.0021,  0.0115])\n",
      "Epoch: 3267, Loss: 2.9280495643615723 \n",
      "              Params: tensor([  5.3558, -17.2371])\n",
      "              Grad: tensor([-0.0021,  0.0115])\n",
      "Epoch: 3268, Loss: 2.9280471801757812 \n",
      "              Params: tensor([  5.3558, -17.2372])\n",
      "              Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch: 3269, Loss: 2.928046226501465 \n",
      "              Params: tensor([  5.3558, -17.2373])\n",
      "              Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch: 3270, Loss: 2.9280455112457275 \n",
      "              Params: tensor([  5.3558, -17.2374])\n",
      "              Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch: 3271, Loss: 2.928044319152832 \n",
      "              Params: tensor([  5.3558, -17.2375])\n",
      "              Grad: tensor([-0.0020,  0.0115])\n",
      "Epoch: 3272, Loss: 2.928041934967041 \n",
      "              Params: tensor([  5.3559, -17.2376])\n",
      "              Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch: 3273, Loss: 2.9280402660369873 \n",
      "              Params: tensor([  5.3559, -17.2377])\n",
      "              Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch: 3274, Loss: 2.92803955078125 \n",
      "              Params: tensor([  5.3559, -17.2379])\n",
      "              Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch: 3275, Loss: 2.9280364513397217 \n",
      "              Params: tensor([  5.3559, -17.2380])\n",
      "              Grad: tensor([-0.0020,  0.0114])\n",
      "Epoch: 3276, Loss: 2.9280364513397217 \n",
      "              Params: tensor([  5.3559, -17.2381])\n",
      "              Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch: 3277, Loss: 2.928036689758301 \n",
      "              Params: tensor([  5.3560, -17.2382])\n",
      "              Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch: 3278, Loss: 2.9280340671539307 \n",
      "              Params: tensor([  5.3560, -17.2383])\n",
      "              Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch: 3279, Loss: 2.9280340671539307 \n",
      "              Params: tensor([  5.3560, -17.2384])\n",
      "              Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch: 3280, Loss: 2.9280312061309814 \n",
      "              Params: tensor([  5.3560, -17.2385])\n",
      "              Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch: 3281, Loss: 2.9280316829681396 \n",
      "              Params: tensor([  5.3560, -17.2386])\n",
      "              Grad: tensor([-0.0020,  0.0113])\n",
      "Epoch: 3282, Loss: 2.9280283451080322 \n",
      "              Params: tensor([  5.3561, -17.2388])\n",
      "              Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch: 3283, Loss: 2.9280266761779785 \n",
      "              Params: tensor([  5.3561, -17.2389])\n",
      "              Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch: 3284, Loss: 2.928025960922241 \n",
      "              Params: tensor([  5.3561, -17.2390])\n",
      "              Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch: 3285, Loss: 2.928025007247925 \n",
      "              Params: tensor([  5.3561, -17.2391])\n",
      "              Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch: 3286, Loss: 2.9280240535736084 \n",
      "              Params: tensor([  5.3561, -17.2392])\n",
      "              Grad: tensor([-0.0020,  0.0112])\n",
      "Epoch: 3287, Loss: 2.9280221462249756 \n",
      "              Params: tensor([  5.3562, -17.2393])\n",
      "              Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch: 3288, Loss: 2.928022623062134 \n",
      "              Params: tensor([  5.3562, -17.2394])\n",
      "              Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch: 3289, Loss: 2.928020715713501 \n",
      "              Params: tensor([  5.3562, -17.2395])\n",
      "              Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch: 3290, Loss: 2.9280190467834473 \n",
      "              Params: tensor([  5.3562, -17.2397])\n",
      "              Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch: 3291, Loss: 2.9280176162719727 \n",
      "              Params: tensor([  5.3562, -17.2398])\n",
      "              Grad: tensor([-0.0020,  0.0111])\n",
      "Epoch: 3292, Loss: 2.9280169010162354 \n",
      "              Params: tensor([  5.3563, -17.2399])\n",
      "              Grad: tensor([-0.0020,  0.0110])\n",
      "Epoch: 3293, Loss: 2.9280145168304443 \n",
      "              Params: tensor([  5.3563, -17.2400])\n",
      "              Grad: tensor([-0.0020,  0.0110])\n",
      "Epoch: 3294, Loss: 2.9280130863189697 \n",
      "              Params: tensor([  5.3563, -17.2401])\n",
      "              Grad: tensor([-0.0020,  0.0110])\n",
      "Epoch: 3295, Loss: 2.9280126094818115 \n",
      "              Params: tensor([  5.3563, -17.2402])\n",
      "              Grad: tensor([-0.0019,  0.0110])\n",
      "Epoch: 3296, Loss: 2.928011178970337 \n",
      "              Params: tensor([  5.3563, -17.2403])\n",
      "              Grad: tensor([-0.0019,  0.0110])\n",
      "Epoch: 3297, Loss: 2.928009033203125 \n",
      "              Params: tensor([  5.3563, -17.2404])\n",
      "              Grad: tensor([-0.0019,  0.0110])\n",
      "Epoch: 3298, Loss: 2.9280083179473877 \n",
      "              Params: tensor([  5.3564, -17.2405])\n",
      "              Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch: 3299, Loss: 2.928006172180176 \n",
      "              Params: tensor([  5.3564, -17.2406])\n",
      "              Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch: 3300, Loss: 2.9280073642730713 \n",
      "              Params: tensor([  5.3564, -17.2407])\n",
      "              Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch: 3301, Loss: 2.928006649017334 \n",
      "              Params: tensor([  5.3564, -17.2409])\n",
      "              Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch: 3302, Loss: 2.928004264831543 \n",
      "              Params: tensor([  5.3564, -17.2410])\n",
      "              Grad: tensor([-0.0019,  0.0109])\n",
      "Epoch: 3303, Loss: 2.92800235748291 \n",
      "              Params: tensor([  5.3565, -17.2411])\n",
      "              Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch: 3304, Loss: 2.928001642227173 \n",
      "              Params: tensor([  5.3565, -17.2412])\n",
      "              Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch: 3305, Loss: 2.927999973297119 \n",
      "              Params: tensor([  5.3565, -17.2413])\n",
      "              Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch: 3306, Loss: 2.92799973487854 \n",
      "              Params: tensor([  5.3565, -17.2414])\n",
      "              Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch: 3307, Loss: 2.9279983043670654 \n",
      "              Params: tensor([  5.3565, -17.2415])\n",
      "              Grad: tensor([-0.0019,  0.0108])\n",
      "Epoch: 3308, Loss: 2.927994728088379 \n",
      "              Params: tensor([  5.3566, -17.2416])\n",
      "              Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch: 3309, Loss: 2.927995443344116 \n",
      "              Params: tensor([  5.3566, -17.2417])\n",
      "              Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch: 3310, Loss: 2.9279935359954834 \n",
      "              Params: tensor([  5.3566, -17.2418])\n",
      "              Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch: 3311, Loss: 2.9279940128326416 \n",
      "              Params: tensor([  5.3566, -17.2419])\n",
      "              Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch: 3312, Loss: 2.9279913902282715 \n",
      "              Params: tensor([  5.3566, -17.2420])\n",
      "              Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch: 3313, Loss: 2.9279913902282715 \n",
      "              Params: tensor([  5.3567, -17.2421])\n",
      "              Grad: tensor([-0.0019,  0.0107])\n",
      "Epoch: 3314, Loss: 2.9279897212982178 \n",
      "              Params: tensor([  5.3567, -17.2423])\n",
      "              Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch: 3315, Loss: 2.9279890060424805 \n",
      "              Params: tensor([  5.3567, -17.2424])\n",
      "              Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch: 3316, Loss: 2.927987575531006 \n",
      "              Params: tensor([  5.3567, -17.2425])\n",
      "              Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch: 3317, Loss: 2.9279863834381104 \n",
      "              Params: tensor([  5.3567, -17.2426])\n",
      "              Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch: 3318, Loss: 2.927985429763794 \n",
      "              Params: tensor([  5.3567, -17.2427])\n",
      "              Grad: tensor([-0.0019,  0.0106])\n",
      "Epoch: 3319, Loss: 2.927983283996582 \n",
      "              Params: tensor([  5.3568, -17.2428])\n",
      "              Grad: tensor([-0.0018,  0.0106])\n",
      "Epoch: 3320, Loss: 2.927983283996582 \n",
      "              Params: tensor([  5.3568, -17.2429])\n",
      "              Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch: 3321, Loss: 2.927980899810791 \n",
      "              Params: tensor([  5.3568, -17.2430])\n",
      "              Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch: 3322, Loss: 2.9279801845550537 \n",
      "              Params: tensor([  5.3568, -17.2431])\n",
      "              Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch: 3323, Loss: 2.927978515625 \n",
      "              Params: tensor([  5.3568, -17.2432])\n",
      "              Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch: 3324, Loss: 2.927978515625 \n",
      "              Params: tensor([  5.3569, -17.2433])\n",
      "              Grad: tensor([-0.0018,  0.0105])\n",
      "Epoch: 3325, Loss: 2.9279768466949463 \n",
      "              Params: tensor([  5.3569, -17.2434])\n",
      "              Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch: 3326, Loss: 2.9279749393463135 \n",
      "              Params: tensor([  5.3569, -17.2435])\n",
      "              Grad: tensor([-0.0019,  0.0104])\n",
      "Epoch: 3327, Loss: 2.9279730319976807 \n",
      "              Params: tensor([  5.3569, -17.2436])\n",
      "              Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch: 3328, Loss: 2.927973747253418 \n",
      "              Params: tensor([  5.3569, -17.2437])\n",
      "              Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch: 3329, Loss: 2.927973985671997 \n",
      "              Params: tensor([  5.3570, -17.2438])\n",
      "              Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch: 3330, Loss: 2.927971601486206 \n",
      "              Params: tensor([  5.3570, -17.2439])\n",
      "              Grad: tensor([-0.0018,  0.0104])\n",
      "Epoch: 3331, Loss: 2.927971839904785 \n",
      "              Params: tensor([  5.3570, -17.2440])\n",
      "              Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch: 3332, Loss: 2.9279685020446777 \n",
      "              Params: tensor([  5.3570, -17.2441])\n",
      "              Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch: 3333, Loss: 2.9279685020446777 \n",
      "              Params: tensor([  5.3570, -17.2442])\n",
      "              Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch: 3334, Loss: 2.927966833114624 \n",
      "              Params: tensor([  5.3570, -17.2443])\n",
      "              Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch: 3335, Loss: 2.927967071533203 \n",
      "              Params: tensor([  5.3571, -17.2444])\n",
      "              Grad: tensor([-0.0018,  0.0103])\n",
      "Epoch: 3336, Loss: 2.9279634952545166 \n",
      "              Params: tensor([  5.3571, -17.2446])\n",
      "              Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch: 3337, Loss: 2.9279634952545166 \n",
      "              Params: tensor([  5.3571, -17.2447])\n",
      "              Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch: 3338, Loss: 2.927962303161621 \n",
      "              Params: tensor([  5.3571, -17.2448])\n",
      "              Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch: 3339, Loss: 2.927962303161621 \n",
      "              Params: tensor([  5.3571, -17.2449])\n",
      "              Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch: 3340, Loss: 2.9279603958129883 \n",
      "              Params: tensor([  5.3572, -17.2450])\n",
      "              Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch: 3341, Loss: 2.9279603958129883 \n",
      "              Params: tensor([  5.3572, -17.2451])\n",
      "              Grad: tensor([-0.0018,  0.0102])\n",
      "Epoch: 3342, Loss: 2.9279592037200928 \n",
      "              Params: tensor([  5.3572, -17.2452])\n",
      "              Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch: 3343, Loss: 2.9279580116271973 \n",
      "              Params: tensor([  5.3572, -17.2453])\n",
      "              Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch: 3344, Loss: 2.9279556274414062 \n",
      "              Params: tensor([  5.3572, -17.2454])\n",
      "              Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch: 3345, Loss: 2.9279556274414062 \n",
      "              Params: tensor([  5.3572, -17.2455])\n",
      "              Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch: 3346, Loss: 2.92795467376709 \n",
      "              Params: tensor([  5.3573, -17.2456])\n",
      "              Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch: 3347, Loss: 2.9279532432556152 \n",
      "              Params: tensor([  5.3573, -17.2457])\n",
      "              Grad: tensor([-0.0018,  0.0101])\n",
      "Epoch: 3348, Loss: 2.9279534816741943 \n",
      "              Params: tensor([  5.3573, -17.2458])\n",
      "              Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch: 3349, Loss: 2.927950859069824 \n",
      "              Params: tensor([  5.3573, -17.2459])\n",
      "              Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch: 3350, Loss: 2.927950382232666 \n",
      "              Params: tensor([  5.3573, -17.2460])\n",
      "              Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch: 3351, Loss: 2.927948236465454 \n",
      "              Params: tensor([  5.3573, -17.2461])\n",
      "              Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch: 3352, Loss: 2.9279470443725586 \n",
      "              Params: tensor([  5.3574, -17.2462])\n",
      "              Grad: tensor([-0.0017,  0.0100])\n",
      "Epoch: 3353, Loss: 2.927947998046875 \n",
      "              Params: tensor([  5.3574, -17.2463])\n",
      "              Grad: tensor([-0.0018,  0.0100])\n",
      "Epoch: 3354, Loss: 2.927945137023926 \n",
      "              Params: tensor([  5.3574, -17.2464])\n",
      "              Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch: 3355, Loss: 2.9279441833496094 \n",
      "              Params: tensor([  5.3574, -17.2465])\n",
      "              Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch: 3356, Loss: 2.927943468093872 \n",
      "              Params: tensor([  5.3574, -17.2466])\n",
      "              Grad: tensor([-0.0018,  0.0099])\n",
      "Epoch: 3357, Loss: 2.9279441833496094 \n",
      "              Params: tensor([  5.3575, -17.2467])\n",
      "              Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch: 3358, Loss: 2.9279417991638184 \n",
      "              Params: tensor([  5.3575, -17.2468])\n",
      "              Grad: tensor([-0.0017,  0.0099])\n",
      "Epoch: 3359, Loss: 2.927940607070923 \n",
      "              Params: tensor([  5.3575, -17.2469])\n",
      "              Grad: tensor([-0.0018,  0.0099])\n",
      "Epoch: 3360, Loss: 2.9279396533966064 \n",
      "              Params: tensor([  5.3575, -17.2470])\n",
      "              Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch: 3361, Loss: 2.9279379844665527 \n",
      "              Params: tensor([  5.3575, -17.2471])\n",
      "              Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch: 3362, Loss: 2.9279375076293945 \n",
      "              Params: tensor([  5.3575, -17.2472])\n",
      "              Grad: tensor([-0.0018,  0.0098])\n",
      "Epoch: 3363, Loss: 2.927936315536499 \n",
      "              Params: tensor([  5.3576, -17.2473])\n",
      "              Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch: 3364, Loss: 2.9279356002807617 \n",
      "              Params: tensor([  5.3576, -17.2474])\n",
      "              Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch: 3365, Loss: 2.927936553955078 \n",
      "              Params: tensor([  5.3576, -17.2474])\n",
      "              Grad: tensor([-0.0017,  0.0098])\n",
      "Epoch: 3366, Loss: 2.927934169769287 \n",
      "              Params: tensor([  5.3576, -17.2475])\n",
      "              Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch: 3367, Loss: 2.92793345451355 \n",
      "              Params: tensor([  5.3576, -17.2476])\n",
      "              Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch: 3368, Loss: 2.927932024002075 \n",
      "              Params: tensor([  5.3576, -17.2477])\n",
      "              Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch: 3369, Loss: 2.9279298782348633 \n",
      "              Params: tensor([  5.3577, -17.2478])\n",
      "              Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch: 3370, Loss: 2.9279282093048096 \n",
      "              Params: tensor([  5.3577, -17.2479])\n",
      "              Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch: 3371, Loss: 2.927931070327759 \n",
      "              Params: tensor([  5.3577, -17.2480])\n",
      "              Grad: tensor([-0.0017,  0.0097])\n",
      "Epoch: 3372, Loss: 2.927928924560547 \n",
      "              Params: tensor([  5.3577, -17.2481])\n",
      "              Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch: 3373, Loss: 2.927926540374756 \n",
      "              Params: tensor([  5.3577, -17.2482])\n",
      "              Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch: 3374, Loss: 2.9279258251190186 \n",
      "              Params: tensor([  5.3577, -17.2483])\n",
      "              Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch: 3375, Loss: 2.9279251098632812 \n",
      "              Params: tensor([  5.3578, -17.2484])\n",
      "              Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch: 3376, Loss: 2.927924156188965 \n",
      "              Params: tensor([  5.3578, -17.2485])\n",
      "              Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch: 3377, Loss: 2.9279227256774902 \n",
      "              Params: tensor([  5.3578, -17.2486])\n",
      "              Grad: tensor([-0.0017,  0.0096])\n",
      "Epoch: 3378, Loss: 2.9279236793518066 \n",
      "              Params: tensor([  5.3578, -17.2487])\n",
      "              Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch: 3379, Loss: 2.927922010421753 \n",
      "              Params: tensor([  5.3578, -17.2488])\n",
      "              Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch: 3380, Loss: 2.9279215335845947 \n",
      "              Params: tensor([  5.3578, -17.2489])\n",
      "              Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch: 3381, Loss: 2.927919626235962 \n",
      "              Params: tensor([  5.3579, -17.2490])\n",
      "              Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch: 3382, Loss: 2.9279181957244873 \n",
      "              Params: tensor([  5.3579, -17.2491])\n",
      "              Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch: 3383, Loss: 2.92791748046875 \n",
      "              Params: tensor([  5.3579, -17.2492])\n",
      "              Grad: tensor([-0.0017,  0.0095])\n",
      "Epoch: 3384, Loss: 2.9279167652130127 \n",
      "              Params: tensor([  5.3579, -17.2493])\n",
      "              Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch: 3385, Loss: 2.927914619445801 \n",
      "              Params: tensor([  5.3579, -17.2494])\n",
      "              Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch: 3386, Loss: 2.927915334701538 \n",
      "              Params: tensor([  5.3579, -17.2495])\n",
      "              Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch: 3387, Loss: 2.9279143810272217 \n",
      "              Params: tensor([  5.3580, -17.2496])\n",
      "              Grad: tensor([-0.0016,  0.0094])\n",
      "Epoch: 3388, Loss: 2.9279134273529053 \n",
      "              Params: tensor([  5.3580, -17.2496])\n",
      "              Grad: tensor([-0.0017,  0.0094])\n",
      "Epoch: 3389, Loss: 2.9279112815856934 \n",
      "              Params: tensor([  5.3580, -17.2497])\n",
      "              Grad: tensor([-0.0016,  0.0094])\n",
      "Epoch: 3390, Loss: 2.927912950515747 \n",
      "              Params: tensor([  5.3580, -17.2498])\n",
      "              Grad: tensor([-0.0017,  0.0093])\n",
      "Epoch: 3391, Loss: 2.9279112815856934 \n",
      "              Params: tensor([  5.3580, -17.2499])\n",
      "              Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch: 3392, Loss: 2.9279098510742188 \n",
      "              Params: tensor([  5.3580, -17.2500])\n",
      "              Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch: 3393, Loss: 2.9279088973999023 \n",
      "              Params: tensor([  5.3581, -17.2501])\n",
      "              Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch: 3394, Loss: 2.927908420562744 \n",
      "              Params: tensor([  5.3581, -17.2502])\n",
      "              Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch: 3395, Loss: 2.9279069900512695 \n",
      "              Params: tensor([  5.3581, -17.2503])\n",
      "              Grad: tensor([-0.0017,  0.0093])\n",
      "Epoch: 3396, Loss: 2.927906036376953 \n",
      "              Params: tensor([  5.3581, -17.2504])\n",
      "              Grad: tensor([-0.0016,  0.0093])\n",
      "Epoch: 3397, Loss: 2.9279050827026367 \n",
      "              Params: tensor([  5.3581, -17.2505])\n",
      "              Grad: tensor([-0.0017,  0.0092])\n",
      "Epoch: 3398, Loss: 2.9279050827026367 \n",
      "              Params: tensor([  5.3581, -17.2506])\n",
      "              Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch: 3399, Loss: 2.927903890609741 \n",
      "              Params: tensor([  5.3582, -17.2507])\n",
      "              Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch: 3400, Loss: 2.9279019832611084 \n",
      "              Params: tensor([  5.3582, -17.2508])\n",
      "              Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch: 3401, Loss: 2.9279022216796875 \n",
      "              Params: tensor([  5.3582, -17.2509])\n",
      "              Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch: 3402, Loss: 2.9279022216796875 \n",
      "              Params: tensor([  5.3582, -17.2509])\n",
      "              Grad: tensor([-0.0016,  0.0092])\n",
      "Epoch: 3403, Loss: 2.927899122238159 \n",
      "              Params: tensor([  5.3582, -17.2510])\n",
      "              Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch: 3404, Loss: 2.927899122238159 \n",
      "              Params: tensor([  5.3582, -17.2511])\n",
      "              Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch: 3405, Loss: 2.9278981685638428 \n",
      "              Params: tensor([  5.3583, -17.2512])\n",
      "              Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch: 3406, Loss: 2.927898645401001 \n",
      "              Params: tensor([  5.3583, -17.2513])\n",
      "              Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch: 3407, Loss: 2.927896022796631 \n",
      "              Params: tensor([  5.3583, -17.2514])\n",
      "              Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch: 3408, Loss: 2.9278948307037354 \n",
      "              Params: tensor([  5.3583, -17.2515])\n",
      "              Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch: 3409, Loss: 2.9278955459594727 \n",
      "              Params: tensor([  5.3583, -17.2516])\n",
      "              Grad: tensor([-0.0016,  0.0091])\n",
      "Epoch: 3410, Loss: 2.92789363861084 \n",
      "              Params: tensor([  5.3583, -17.2517])\n",
      "              Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch: 3411, Loss: 2.9278924465179443 \n",
      "              Params: tensor([  5.3584, -17.2518])\n",
      "              Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch: 3412, Loss: 2.9278924465179443 \n",
      "              Params: tensor([  5.3584, -17.2519])\n",
      "              Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch: 3413, Loss: 2.927891492843628 \n",
      "              Params: tensor([  5.3584, -17.2519])\n",
      "              Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch: 3414, Loss: 2.9278907775878906 \n",
      "              Params: tensor([  5.3584, -17.2520])\n",
      "              Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch: 3415, Loss: 2.927889823913574 \n",
      "              Params: tensor([  5.3584, -17.2521])\n",
      "              Grad: tensor([-0.0016,  0.0090])\n",
      "Epoch: 3416, Loss: 2.9278907775878906 \n",
      "              Params: tensor([  5.3584, -17.2522])\n",
      "              Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch: 3417, Loss: 2.9278879165649414 \n",
      "              Params: tensor([  5.3584, -17.2523])\n",
      "              Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch: 3418, Loss: 2.9278879165649414 \n",
      "              Params: tensor([  5.3585, -17.2524])\n",
      "              Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch: 3419, Loss: 2.9278855323791504 \n",
      "              Params: tensor([  5.3585, -17.2525])\n",
      "              Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch: 3420, Loss: 2.927886962890625 \n",
      "              Params: tensor([  5.3585, -17.2526])\n",
      "              Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch: 3421, Loss: 2.9278855323791504 \n",
      "              Params: tensor([  5.3585, -17.2527])\n",
      "              Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch: 3422, Loss: 2.9278838634490967 \n",
      "              Params: tensor([  5.3585, -17.2527])\n",
      "              Grad: tensor([-0.0016,  0.0089])\n",
      "Epoch: 3423, Loss: 2.9278829097747803 \n",
      "              Params: tensor([  5.3585, -17.2528])\n",
      "              Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch: 3424, Loss: 2.9278814792633057 \n",
      "              Params: tensor([  5.3586, -17.2529])\n",
      "              Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch: 3425, Loss: 2.9278810024261475 \n",
      "              Params: tensor([  5.3586, -17.2530])\n",
      "              Grad: tensor([-0.0016,  0.0088])\n",
      "Epoch: 3426, Loss: 2.92788028717041 \n",
      "              Params: tensor([  5.3586, -17.2531])\n",
      "              Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch: 3427, Loss: 2.927880048751831 \n",
      "              Params: tensor([  5.3586, -17.2532])\n",
      "              Grad: tensor([-0.0016,  0.0088])\n",
      "Epoch: 3428, Loss: 2.9278790950775146 \n",
      "              Params: tensor([  5.3586, -17.2533])\n",
      "              Grad: tensor([-0.0015,  0.0088])\n",
      "Epoch: 3429, Loss: 2.927877187728882 \n",
      "              Params: tensor([  5.3586, -17.2534])\n",
      "              Grad: tensor([-0.0016,  0.0087])\n",
      "Epoch: 3430, Loss: 2.9278764724731445 \n",
      "              Params: tensor([  5.3586, -17.2534])\n",
      "              Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch: 3431, Loss: 2.9278764724731445 \n",
      "              Params: tensor([  5.3587, -17.2535])\n",
      "              Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch: 3432, Loss: 2.9278762340545654 \n",
      "              Params: tensor([  5.3587, -17.2536])\n",
      "              Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch: 3433, Loss: 2.9278764724731445 \n",
      "              Params: tensor([  5.3587, -17.2537])\n",
      "              Grad: tensor([-0.0016,  0.0087])\n",
      "Epoch: 3434, Loss: 2.927875518798828 \n",
      "              Params: tensor([  5.3587, -17.2538])\n",
      "              Grad: tensor([-0.0016,  0.0087])\n",
      "Epoch: 3435, Loss: 2.927874803543091 \n",
      "              Params: tensor([  5.3587, -17.2539])\n",
      "              Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch: 3436, Loss: 2.927873134613037 \n",
      "              Params: tensor([  5.3587, -17.2540])\n",
      "              Grad: tensor([-0.0015,  0.0087])\n",
      "Epoch: 3437, Loss: 2.9278719425201416 \n",
      "              Params: tensor([  5.3588, -17.2541])\n",
      "              Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch: 3438, Loss: 2.927870750427246 \n",
      "              Params: tensor([  5.3588, -17.2541])\n",
      "              Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch: 3439, Loss: 2.927870273590088 \n",
      "              Params: tensor([  5.3588, -17.2542])\n",
      "              Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch: 3440, Loss: 2.927870988845825 \n",
      "              Params: tensor([  5.3588, -17.2543])\n",
      "              Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch: 3441, Loss: 2.927868604660034 \n",
      "              Params: tensor([  5.3588, -17.2544])\n",
      "              Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch: 3442, Loss: 2.9278688430786133 \n",
      "              Params: tensor([  5.3588, -17.2545])\n",
      "              Grad: tensor([-0.0015,  0.0086])\n",
      "Epoch: 3443, Loss: 2.9278669357299805 \n",
      "              Params: tensor([  5.3588, -17.2546])\n",
      "              Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch: 3444, Loss: 2.927865505218506 \n",
      "              Params: tensor([  5.3589, -17.2547])\n",
      "              Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch: 3445, Loss: 2.927866220474243 \n",
      "              Params: tensor([  5.3589, -17.2547])\n",
      "              Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch: 3446, Loss: 2.927865743637085 \n",
      "              Params: tensor([  5.3589, -17.2548])\n",
      "              Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch: 3447, Loss: 2.9278640747070312 \n",
      "              Params: tensor([  5.3589, -17.2549])\n",
      "              Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch: 3448, Loss: 2.9278626441955566 \n",
      "              Params: tensor([  5.3589, -17.2550])\n",
      "              Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch: 3449, Loss: 2.927863359451294 \n",
      "              Params: tensor([  5.3589, -17.2551])\n",
      "              Grad: tensor([-0.0015,  0.0085])\n",
      "Epoch: 3450, Loss: 2.9278619289398193 \n",
      "              Params: tensor([  5.3590, -17.2552])\n",
      "              Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch: 3451, Loss: 2.927863121032715 \n",
      "              Params: tensor([  5.3590, -17.2552])\n",
      "              Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch: 3452, Loss: 2.9278602600097656 \n",
      "              Params: tensor([  5.3590, -17.2553])\n",
      "              Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch: 3453, Loss: 2.9278600215911865 \n",
      "              Params: tensor([  5.3590, -17.2554])\n",
      "              Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch: 3454, Loss: 2.9278600215911865 \n",
      "              Params: tensor([  5.3590, -17.2555])\n",
      "              Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch: 3455, Loss: 2.927858591079712 \n",
      "              Params: tensor([  5.3590, -17.2556])\n",
      "              Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch: 3456, Loss: 2.9278578758239746 \n",
      "              Params: tensor([  5.3590, -17.2557])\n",
      "              Grad: tensor([-0.0015,  0.0084])\n",
      "Epoch: 3457, Loss: 2.9278581142425537 \n",
      "              Params: tensor([  5.3591, -17.2557])\n",
      "              Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch: 3458, Loss: 2.9278557300567627 \n",
      "              Params: tensor([  5.3591, -17.2558])\n",
      "              Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch: 3459, Loss: 2.927856683731079 \n",
      "              Params: tensor([  5.3591, -17.2559])\n",
      "              Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch: 3460, Loss: 2.927853584289551 \n",
      "              Params: tensor([  5.3591, -17.2560])\n",
      "              Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch: 3461, Loss: 2.9278554916381836 \n",
      "              Params: tensor([  5.3591, -17.2561])\n",
      "              Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch: 3462, Loss: 2.927853584289551 \n",
      "              Params: tensor([  5.3591, -17.2562])\n",
      "              Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch: 3463, Loss: 2.927853584289551 \n",
      "              Params: tensor([  5.3591, -17.2562])\n",
      "              Grad: tensor([-0.0015,  0.0083])\n",
      "Epoch: 3464, Loss: 2.9278512001037598 \n",
      "              Params: tensor([  5.3592, -17.2563])\n",
      "              Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch: 3465, Loss: 2.9278528690338135 \n",
      "              Params: tensor([  5.3592, -17.2564])\n",
      "              Grad: tensor([-0.0015,  0.0082])\n",
      "Epoch: 3466, Loss: 2.927851676940918 \n",
      "              Params: tensor([  5.3592, -17.2565])\n",
      "              Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch: 3467, Loss: 2.9278502464294434 \n",
      "              Params: tensor([  5.3592, -17.2566])\n",
      "              Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch: 3468, Loss: 2.927849054336548 \n",
      "              Params: tensor([  5.3592, -17.2567])\n",
      "              Grad: tensor([-0.0015,  0.0082])\n",
      "Epoch: 3469, Loss: 2.9278488159179688 \n",
      "              Params: tensor([  5.3592, -17.2567])\n",
      "              Grad: tensor([-0.0015,  0.0082])\n",
      "Epoch: 3470, Loss: 2.9278481006622314 \n",
      "              Params: tensor([  5.3592, -17.2568])\n",
      "              Grad: tensor([-0.0014,  0.0082])\n",
      "Epoch: 3471, Loss: 2.9278481006622314 \n",
      "              Params: tensor([  5.3593, -17.2569])\n",
      "              Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch: 3472, Loss: 2.9278459548950195 \n",
      "              Params: tensor([  5.3593, -17.2570])\n",
      "              Grad: tensor([-0.0015,  0.0081])\n",
      "Epoch: 3473, Loss: 2.9278459548950195 \n",
      "              Params: tensor([  5.3593, -17.2571])\n",
      "              Grad: tensor([-0.0015,  0.0081])\n",
      "Epoch: 3474, Loss: 2.927845001220703 \n",
      "              Params: tensor([  5.3593, -17.2571])\n",
      "              Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch: 3475, Loss: 2.9278440475463867 \n",
      "              Params: tensor([  5.3593, -17.2572])\n",
      "              Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch: 3476, Loss: 2.927844285964966 \n",
      "              Params: tensor([  5.3593, -17.2573])\n",
      "              Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch: 3477, Loss: 2.9278440475463867 \n",
      "              Params: tensor([  5.3593, -17.2574])\n",
      "              Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch: 3478, Loss: 2.927842855453491 \n",
      "              Params: tensor([  5.3594, -17.2575])\n",
      "              Grad: tensor([-0.0014,  0.0081])\n",
      "Epoch: 3479, Loss: 2.927842617034912 \n",
      "              Params: tensor([  5.3594, -17.2575])\n",
      "              Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch: 3480, Loss: 2.927841901779175 \n",
      "              Params: tensor([  5.3594, -17.2576])\n",
      "              Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch: 3481, Loss: 2.927839756011963 \n",
      "              Params: tensor([  5.3594, -17.2577])\n",
      "              Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch: 3482, Loss: 2.927841901779175 \n",
      "              Params: tensor([  5.3594, -17.2578])\n",
      "              Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch: 3483, Loss: 2.9278390407562256 \n",
      "              Params: tensor([  5.3594, -17.2579])\n",
      "              Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch: 3484, Loss: 2.9278383255004883 \n",
      "              Params: tensor([  5.3594, -17.2579])\n",
      "              Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch: 3485, Loss: 2.9278388023376465 \n",
      "              Params: tensor([  5.3595, -17.2580])\n",
      "              Grad: tensor([-0.0014,  0.0080])\n",
      "Epoch: 3486, Loss: 2.9278383255004883 \n",
      "              Params: tensor([  5.3595, -17.2581])\n",
      "              Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch: 3487, Loss: 2.9278366565704346 \n",
      "              Params: tensor([  5.3595, -17.2582])\n",
      "              Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch: 3488, Loss: 2.927834987640381 \n",
      "              Params: tensor([  5.3595, -17.2583])\n",
      "              Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch: 3489, Loss: 2.9278366565704346 \n",
      "              Params: tensor([  5.3595, -17.2583])\n",
      "              Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch: 3490, Loss: 2.927835702896118 \n",
      "              Params: tensor([  5.3595, -17.2584])\n",
      "              Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch: 3491, Loss: 2.9278345108032227 \n",
      "              Params: tensor([  5.3595, -17.2585])\n",
      "              Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch: 3492, Loss: 2.927833318710327 \n",
      "              Params: tensor([  5.3596, -17.2586])\n",
      "              Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch: 3493, Loss: 2.927832841873169 \n",
      "              Params: tensor([  5.3596, -17.2587])\n",
      "              Grad: tensor([-0.0014,  0.0079])\n",
      "Epoch: 3494, Loss: 2.927832841873169 \n",
      "              Params: tensor([  5.3596, -17.2587])\n",
      "              Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch: 3495, Loss: 2.9278321266174316 \n",
      "              Params: tensor([  5.3596, -17.2588])\n",
      "              Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch: 3496, Loss: 2.9278311729431152 \n",
      "              Params: tensor([  5.3596, -17.2589])\n",
      "              Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch: 3497, Loss: 2.927830457687378 \n",
      "              Params: tensor([  5.3596, -17.2590])\n",
      "              Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch: 3498, Loss: 2.9278299808502197 \n",
      "              Params: tensor([  5.3596, -17.2590])\n",
      "              Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch: 3499, Loss: 2.9278297424316406 \n",
      "              Params: tensor([  5.3597, -17.2591])\n",
      "              Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch: 3500, Loss: 2.9278290271759033 \n",
      "              Params: tensor([  5.3597, -17.2592])\n",
      "              Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch: 3501, Loss: 2.9278275966644287 \n",
      "              Params: tensor([  5.3597, -17.2593])\n",
      "              Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch: 3502, Loss: 2.927828073501587 \n",
      "              Params: tensor([  5.3597, -17.2594])\n",
      "              Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch: 3503, Loss: 2.9278268814086914 \n",
      "              Params: tensor([  5.3597, -17.2594])\n",
      "              Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch: 3504, Loss: 2.9278249740600586 \n",
      "              Params: tensor([  5.3597, -17.2595])\n",
      "              Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch: 3505, Loss: 2.9278268814086914 \n",
      "              Params: tensor([  5.3597, -17.2596])\n",
      "              Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch: 3506, Loss: 2.9278249740600586 \n",
      "              Params: tensor([  5.3597, -17.2597])\n",
      "              Grad: tensor([-0.0014,  0.0077])\n",
      "Epoch: 3507, Loss: 2.9278242588043213 \n",
      "              Params: tensor([  5.3598, -17.2597])\n",
      "              Grad: tensor([-0.0013,  0.0077])\n",
      "Epoch: 3508, Loss: 2.927823781967163 \n",
      "              Params: tensor([  5.3598, -17.2598])\n",
      "              Grad: tensor([-0.0013,  0.0077])\n",
      "Epoch: 3509, Loss: 2.927823781967163 \n",
      "              Params: tensor([  5.3598, -17.2599])\n",
      "              Grad: tensor([-0.0013,  0.0076])\n",
      "Epoch: 3510, Loss: 2.9278218746185303 \n",
      "              Params: tensor([  5.3598, -17.2600])\n",
      "              Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch: 3511, Loss: 2.9278223514556885 \n",
      "              Params: tensor([  5.3598, -17.2600])\n",
      "              Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch: 3512, Loss: 2.927821397781372 \n",
      "              Params: tensor([  5.3598, -17.2601])\n",
      "              Grad: tensor([-0.0013,  0.0076])\n",
      "Epoch: 3513, Loss: 2.9278199672698975 \n",
      "              Params: tensor([  5.3598, -17.2602])\n",
      "              Grad: tensor([-0.0013,  0.0076])\n",
      "Epoch: 3514, Loss: 2.9278197288513184 \n",
      "              Params: tensor([  5.3599, -17.2603])\n",
      "              Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch: 3515, Loss: 2.9278206825256348 \n",
      "              Params: tensor([  5.3599, -17.2604])\n",
      "              Grad: tensor([-0.0014,  0.0076])\n",
      "Epoch: 3516, Loss: 2.92781925201416 \n",
      "              Params: tensor([  5.3599, -17.2604])\n",
      "              Grad: tensor([-0.0014,  0.0075])\n",
      "Epoch: 3517, Loss: 2.92781925201416 \n",
      "              Params: tensor([  5.3599, -17.2605])\n",
      "              Grad: tensor([-0.0014,  0.0075])\n",
      "Epoch: 3518, Loss: 2.927818536758423 \n",
      "              Params: tensor([  5.3599, -17.2606])\n",
      "              Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch: 3519, Loss: 2.9278173446655273 \n",
      "              Params: tensor([  5.3599, -17.2607])\n",
      "              Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch: 3520, Loss: 2.9278173446655273 \n",
      "              Params: tensor([  5.3599, -17.2607])\n",
      "              Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch: 3521, Loss: 2.927816152572632 \n",
      "              Params: tensor([  5.3599, -17.2608])\n",
      "              Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch: 3522, Loss: 2.9278151988983154 \n",
      "              Params: tensor([  5.3600, -17.2609])\n",
      "              Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch: 3523, Loss: 2.9278159141540527 \n",
      "              Params: tensor([  5.3600, -17.2610])\n",
      "              Grad: tensor([-0.0013,  0.0075])\n",
      "Epoch: 3524, Loss: 2.9278147220611572 \n",
      "              Params: tensor([  5.3600, -17.2610])\n",
      "              Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch: 3525, Loss: 2.9278135299682617 \n",
      "              Params: tensor([  5.3600, -17.2611])\n",
      "              Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch: 3526, Loss: 2.9278130531311035 \n",
      "              Params: tensor([  5.3600, -17.2612])\n",
      "              Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch: 3527, Loss: 2.927812337875366 \n",
      "              Params: tensor([  5.3600, -17.2612])\n",
      "              Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch: 3528, Loss: 2.9278109073638916 \n",
      "              Params: tensor([  5.3600, -17.2613])\n",
      "              Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch: 3529, Loss: 2.927811622619629 \n",
      "              Params: tensor([  5.3601, -17.2614])\n",
      "              Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch: 3530, Loss: 2.927811622619629 \n",
      "              Params: tensor([  5.3601, -17.2615])\n",
      "              Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch: 3531, Loss: 2.927809953689575 \n",
      "              Params: tensor([  5.3601, -17.2615])\n",
      "              Grad: tensor([-0.0013,  0.0074])\n",
      "Epoch: 3532, Loss: 2.927809000015259 \n",
      "              Params: tensor([  5.3601, -17.2616])\n",
      "              Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch: 3533, Loss: 2.9278104305267334 \n",
      "              Params: tensor([  5.3601, -17.2617])\n",
      "              Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch: 3534, Loss: 2.927809238433838 \n",
      "              Params: tensor([  5.3601, -17.2618])\n",
      "              Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch: 3535, Loss: 2.9278082847595215 \n",
      "              Params: tensor([  5.3601, -17.2618])\n",
      "              Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch: 3536, Loss: 2.9278082847595215 \n",
      "              Params: tensor([  5.3601, -17.2619])\n",
      "              Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch: 3537, Loss: 2.9278059005737305 \n",
      "              Params: tensor([  5.3602, -17.2620])\n",
      "              Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch: 3538, Loss: 2.9278061389923096 \n",
      "              Params: tensor([  5.3602, -17.2621])\n",
      "              Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch: 3539, Loss: 2.9278054237365723 \n",
      "              Params: tensor([  5.3602, -17.2621])\n",
      "              Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch: 3540, Loss: 2.9278039932250977 \n",
      "              Params: tensor([  5.3602, -17.2622])\n",
      "              Grad: tensor([-0.0013,  0.0073])\n",
      "Epoch: 3541, Loss: 2.927804708480835 \n",
      "              Params: tensor([  5.3602, -17.2623])\n",
      "              Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch: 3542, Loss: 2.9278037548065186 \n",
      "              Params: tensor([  5.3602, -17.2623])\n",
      "              Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch: 3543, Loss: 2.927805185317993 \n",
      "              Params: tensor([  5.3602, -17.2624])\n",
      "              Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch: 3544, Loss: 2.9278037548065186 \n",
      "              Params: tensor([  5.3602, -17.2625])\n",
      "              Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch: 3545, Loss: 2.9278037548065186 \n",
      "              Params: tensor([  5.3603, -17.2626])\n",
      "              Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch: 3546, Loss: 2.927802801132202 \n",
      "              Params: tensor([  5.3603, -17.2626])\n",
      "              Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch: 3547, Loss: 2.927802085876465 \n",
      "              Params: tensor([  5.3603, -17.2627])\n",
      "              Grad: tensor([-0.0013,  0.0072])\n",
      "Epoch: 3548, Loss: 2.9278006553649902 \n",
      "              Params: tensor([  5.3603, -17.2628])\n",
      "              Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch: 3549, Loss: 2.9278006553649902 \n",
      "              Params: tensor([  5.3603, -17.2628])\n",
      "              Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch: 3550, Loss: 2.9277992248535156 \n",
      "              Params: tensor([  5.3603, -17.2629])\n",
      "              Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch: 3551, Loss: 2.9278008937835693 \n",
      "              Params: tensor([  5.3603, -17.2630])\n",
      "              Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch: 3552, Loss: 2.927797794342041 \n",
      "              Params: tensor([  5.3603, -17.2631])\n",
      "              Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch: 3553, Loss: 2.927798271179199 \n",
      "              Params: tensor([  5.3604, -17.2631])\n",
      "              Grad: tensor([-0.0012,  0.0071])\n",
      "Epoch: 3554, Loss: 2.927798271179199 \n",
      "              Params: tensor([  5.3604, -17.2632])\n",
      "              Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch: 3555, Loss: 2.927797794342041 \n",
      "              Params: tensor([  5.3604, -17.2633])\n",
      "              Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch: 3556, Loss: 2.927797794342041 \n",
      "              Params: tensor([  5.3604, -17.2633])\n",
      "              Grad: tensor([-0.0013,  0.0071])\n",
      "Epoch: 3557, Loss: 2.9277961254119873 \n",
      "              Params: tensor([  5.3604, -17.2634])\n",
      "              Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch: 3558, Loss: 2.92779541015625 \n",
      "              Params: tensor([  5.3604, -17.2635])\n",
      "              Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch: 3559, Loss: 2.9277963638305664 \n",
      "              Params: tensor([  5.3604, -17.2636])\n",
      "              Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch: 3560, Loss: 2.9277939796447754 \n",
      "              Params: tensor([  5.3604, -17.2636])\n",
      "              Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch: 3561, Loss: 2.92779541015625 \n",
      "              Params: tensor([  5.3605, -17.2637])\n",
      "              Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch: 3562, Loss: 2.927795171737671 \n",
      "              Params: tensor([  5.3605, -17.2638])\n",
      "              Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch: 3563, Loss: 2.927793025970459 \n",
      "              Params: tensor([  5.3605, -17.2638])\n",
      "              Grad: tensor([-0.0013,  0.0070])\n",
      "Epoch: 3564, Loss: 2.9277946949005127 \n",
      "              Params: tensor([  5.3605, -17.2639])\n",
      "              Grad: tensor([-0.0012,  0.0070])\n",
      "Epoch: 3565, Loss: 2.9277913570404053 \n",
      "              Params: tensor([  5.3605, -17.2640])\n",
      "              Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch: 3566, Loss: 2.9277913570404053 \n",
      "              Params: tensor([  5.3605, -17.2640])\n",
      "              Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch: 3567, Loss: 2.9277913570404053 \n",
      "              Params: tensor([  5.3605, -17.2641])\n",
      "              Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch: 3568, Loss: 2.9277901649475098 \n",
      "              Params: tensor([  5.3605, -17.2642])\n",
      "              Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch: 3569, Loss: 2.9277899265289307 \n",
      "              Params: tensor([  5.3606, -17.2642])\n",
      "              Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch: 3570, Loss: 2.9277894496917725 \n",
      "              Params: tensor([  5.3606, -17.2643])\n",
      "              Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch: 3571, Loss: 2.9277899265289307 \n",
      "              Params: tensor([  5.3606, -17.2644])\n",
      "              Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch: 3572, Loss: 2.9277894496917725 \n",
      "              Params: tensor([  5.3606, -17.2645])\n",
      "              Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch: 3573, Loss: 2.9277894496917725 \n",
      "              Params: tensor([  5.3606, -17.2645])\n",
      "              Grad: tensor([-0.0012,  0.0069])\n",
      "Epoch: 3574, Loss: 2.9277892112731934 \n",
      "              Params: tensor([  5.3606, -17.2646])\n",
      "              Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch: 3575, Loss: 2.9277868270874023 \n",
      "              Params: tensor([  5.3606, -17.2647])\n",
      "              Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch: 3576, Loss: 2.927786111831665 \n",
      "              Params: tensor([  5.3606, -17.2647])\n",
      "              Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch: 3577, Loss: 2.927788019180298 \n",
      "              Params: tensor([  5.3607, -17.2648])\n",
      "              Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch: 3578, Loss: 2.9277853965759277 \n",
      "              Params: tensor([  5.3607, -17.2649])\n",
      "              Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch: 3579, Loss: 2.9277849197387695 \n",
      "              Params: tensor([  5.3607, -17.2649])\n",
      "              Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch: 3580, Loss: 2.927786350250244 \n",
      "              Params: tensor([  5.3607, -17.2650])\n",
      "              Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch: 3581, Loss: 2.9277849197387695 \n",
      "              Params: tensor([  5.3607, -17.2651])\n",
      "              Grad: tensor([-0.0012,  0.0068])\n",
      "Epoch: 3582, Loss: 2.9277842044830322 \n",
      "              Params: tensor([  5.3607, -17.2651])\n",
      "              Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch: 3583, Loss: 2.927783966064453 \n",
      "              Params: tensor([  5.3607, -17.2652])\n",
      "              Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch: 3584, Loss: 2.9277825355529785 \n",
      "              Params: tensor([  5.3607, -17.2653])\n",
      "              Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch: 3585, Loss: 2.9277830123901367 \n",
      "              Params: tensor([  5.3607, -17.2653])\n",
      "              Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch: 3586, Loss: 2.927781105041504 \n",
      "              Params: tensor([  5.3608, -17.2654])\n",
      "              Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch: 3587, Loss: 2.927781820297241 \n",
      "              Params: tensor([  5.3608, -17.2655])\n",
      "              Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch: 3588, Loss: 2.927780866622925 \n",
      "              Params: tensor([  5.3608, -17.2655])\n",
      "              Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch: 3589, Loss: 2.927780866622925 \n",
      "              Params: tensor([  5.3608, -17.2656])\n",
      "              Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch: 3590, Loss: 2.927781105041504 \n",
      "              Params: tensor([  5.3608, -17.2657])\n",
      "              Grad: tensor([-0.0012,  0.0067])\n",
      "Epoch: 3591, Loss: 2.9277799129486084 \n",
      "              Params: tensor([  5.3608, -17.2657])\n",
      "              Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch: 3592, Loss: 2.9277801513671875 \n",
      "              Params: tensor([  5.3608, -17.2658])\n",
      "              Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch: 3593, Loss: 2.927778482437134 \n",
      "              Params: tensor([  5.3608, -17.2659])\n",
      "              Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch: 3594, Loss: 2.927778720855713 \n",
      "              Params: tensor([  5.3609, -17.2659])\n",
      "              Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch: 3595, Loss: 2.9277777671813965 \n",
      "              Params: tensor([  5.3609, -17.2660])\n",
      "              Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch: 3596, Loss: 2.9277780055999756 \n",
      "              Params: tensor([  5.3609, -17.2661])\n",
      "              Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch: 3597, Loss: 2.927778720855713 \n",
      "              Params: tensor([  5.3609, -17.2661])\n",
      "              Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch: 3598, Loss: 2.927776575088501 \n",
      "              Params: tensor([  5.3609, -17.2662])\n",
      "              Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch: 3599, Loss: 2.9277760982513428 \n",
      "              Params: tensor([  5.3609, -17.2663])\n",
      "              Grad: tensor([-0.0012,  0.0066])\n",
      "Epoch: 3600, Loss: 2.9277753829956055 \n",
      "              Params: tensor([  5.3609, -17.2663])\n",
      "              Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch: 3601, Loss: 2.9277756214141846 \n",
      "              Params: tensor([  5.3609, -17.2664])\n",
      "              Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch: 3602, Loss: 2.9277732372283936 \n",
      "              Params: tensor([  5.3609, -17.2665])\n",
      "              Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch: 3603, Loss: 2.9277749061584473 \n",
      "              Params: tensor([  5.3610, -17.2665])\n",
      "              Grad: tensor([-0.0012,  0.0065])\n",
      "Epoch: 3604, Loss: 2.9277749061584473 \n",
      "              Params: tensor([  5.3610, -17.2666])\n",
      "              Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch: 3605, Loss: 2.927774667739868 \n",
      "              Params: tensor([  5.3610, -17.2667])\n",
      "              Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch: 3606, Loss: 2.9277732372283936 \n",
      "              Params: tensor([  5.3610, -17.2667])\n",
      "              Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch: 3607, Loss: 2.9277727603912354 \n",
      "              Params: tensor([  5.3610, -17.2668])\n",
      "              Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch: 3608, Loss: 2.9277725219726562 \n",
      "              Params: tensor([  5.3610, -17.2668])\n",
      "              Grad: tensor([-0.0011,  0.0065])\n",
      "Epoch: 3609, Loss: 2.927772283554077 \n",
      "              Params: tensor([  5.3610, -17.2669])\n",
      "              Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch: 3610, Loss: 2.927771806716919 \n",
      "              Params: tensor([  5.3610, -17.2670])\n",
      "              Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch: 3611, Loss: 2.9277701377868652 \n",
      "              Params: tensor([  5.3611, -17.2670])\n",
      "              Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch: 3612, Loss: 2.92777156829834 \n",
      "              Params: tensor([  5.3611, -17.2671])\n",
      "              Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch: 3613, Loss: 2.9277710914611816 \n",
      "              Params: tensor([  5.3611, -17.2672])\n",
      "              Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch: 3614, Loss: 2.927769660949707 \n",
      "              Params: tensor([  5.3611, -17.2672])\n",
      "              Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch: 3615, Loss: 2.9277701377868652 \n",
      "              Params: tensor([  5.3611, -17.2673])\n",
      "              Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch: 3616, Loss: 2.9277689456939697 \n",
      "              Params: tensor([  5.3611, -17.2674])\n",
      "              Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch: 3617, Loss: 2.9277679920196533 \n",
      "              Params: tensor([  5.3611, -17.2674])\n",
      "              Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch: 3618, Loss: 2.927769422531128 \n",
      "              Params: tensor([  5.3611, -17.2675])\n",
      "              Grad: tensor([-0.0011,  0.0064])\n",
      "Epoch: 3619, Loss: 2.9277684688568115 \n",
      "              Params: tensor([  5.3611, -17.2675])\n",
      "              Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch: 3620, Loss: 2.9277665615081787 \n",
      "              Params: tensor([  5.3612, -17.2676])\n",
      "              Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch: 3621, Loss: 2.927767276763916 \n",
      "              Params: tensor([  5.3612, -17.2677])\n",
      "              Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch: 3622, Loss: 2.927767038345337 \n",
      "              Params: tensor([  5.3612, -17.2677])\n",
      "              Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch: 3623, Loss: 2.927765130996704 \n",
      "              Params: tensor([  5.3612, -17.2678])\n",
      "              Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch: 3624, Loss: 2.9277658462524414 \n",
      "              Params: tensor([  5.3612, -17.2679])\n",
      "              Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch: 3625, Loss: 2.927765130996704 \n",
      "              Params: tensor([  5.3612, -17.2679])\n",
      "              Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch: 3626, Loss: 2.927765130996704 \n",
      "              Params: tensor([  5.3612, -17.2680])\n",
      "              Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch: 3627, Loss: 2.9277639389038086 \n",
      "              Params: tensor([  5.3612, -17.2681])\n",
      "              Grad: tensor([-0.0011,  0.0063])\n",
      "Epoch: 3628, Loss: 2.9277641773223877 \n",
      "              Params: tensor([  5.3612, -17.2681])\n",
      "              Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch: 3629, Loss: 2.9277639389038086 \n",
      "              Params: tensor([  5.3613, -17.2682])\n",
      "              Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch: 3630, Loss: 2.927762031555176 \n",
      "              Params: tensor([  5.3613, -17.2682])\n",
      "              Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch: 3631, Loss: 2.927762746810913 \n",
      "              Params: tensor([  5.3613, -17.2683])\n",
      "              Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch: 3632, Loss: 2.927762508392334 \n",
      "              Params: tensor([  5.3613, -17.2684])\n",
      "              Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch: 3633, Loss: 2.927762031555176 \n",
      "              Params: tensor([  5.3613, -17.2684])\n",
      "              Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch: 3634, Loss: 2.9277613162994385 \n",
      "              Params: tensor([  5.3613, -17.2685])\n",
      "              Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch: 3635, Loss: 2.9277617931365967 \n",
      "              Params: tensor([  5.3613, -17.2685])\n",
      "              Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch: 3636, Loss: 2.9277594089508057 \n",
      "              Params: tensor([  5.3613, -17.2686])\n",
      "              Grad: tensor([-0.0011,  0.0062])\n",
      "Epoch: 3637, Loss: 2.9277608394622803 \n",
      "              Params: tensor([  5.3613, -17.2687])\n",
      "              Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch: 3638, Loss: 2.9277610778808594 \n",
      "              Params: tensor([  5.3614, -17.2687])\n",
      "              Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch: 3639, Loss: 2.927760362625122 \n",
      "              Params: tensor([  5.3614, -17.2688])\n",
      "              Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch: 3640, Loss: 2.9277594089508057 \n",
      "              Params: tensor([  5.3614, -17.2689])\n",
      "              Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch: 3641, Loss: 2.92775821685791 \n",
      "              Params: tensor([  5.3614, -17.2689])\n",
      "              Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch: 3642, Loss: 2.9277594089508057 \n",
      "              Params: tensor([  5.3614, -17.2690])\n",
      "              Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch: 3643, Loss: 2.9277572631835938 \n",
      "              Params: tensor([  5.3614, -17.2690])\n",
      "              Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch: 3644, Loss: 2.927757978439331 \n",
      "              Params: tensor([  5.3614, -17.2691])\n",
      "              Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch: 3645, Loss: 2.9277572631835938 \n",
      "              Params: tensor([  5.3614, -17.2692])\n",
      "              Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch: 3646, Loss: 2.9277570247650146 \n",
      "              Params: tensor([  5.3614, -17.2692])\n",
      "              Grad: tensor([-0.0011,  0.0061])\n",
      "Epoch: 3647, Loss: 2.9277565479278564 \n",
      "              Params: tensor([  5.3614, -17.2693])\n",
      "              Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch: 3648, Loss: 2.92775559425354 \n",
      "              Params: tensor([  5.3615, -17.2693])\n",
      "              Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch: 3649, Loss: 2.9277570247650146 \n",
      "              Params: tensor([  5.3615, -17.2694])\n",
      "              Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch: 3650, Loss: 2.92775559425354 \n",
      "              Params: tensor([  5.3615, -17.2695])\n",
      "              Grad: tensor([-0.0011,  0.0060])\n",
      "Epoch: 3651, Loss: 2.92775559425354 \n",
      "              Params: tensor([  5.3615, -17.2695])\n",
      "              Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch: 3652, Loss: 2.927755117416382 \n",
      "              Params: tensor([  5.3615, -17.2696])\n",
      "              Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch: 3653, Loss: 2.927755117416382 \n",
      "              Params: tensor([  5.3615, -17.2696])\n",
      "              Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch: 3654, Loss: 2.9277541637420654 \n",
      "              Params: tensor([  5.3615, -17.2697])\n",
      "              Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch: 3655, Loss: 2.9277536869049072 \n",
      "              Params: tensor([  5.3615, -17.2698])\n",
      "              Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch: 3656, Loss: 2.9277548789978027 \n",
      "              Params: tensor([  5.3615, -17.2698])\n",
      "              Grad: tensor([-0.0010,  0.0060])\n",
      "Epoch: 3657, Loss: 2.927753210067749 \n",
      "              Params: tensor([  5.3616, -17.2699])\n",
      "              Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch: 3658, Loss: 2.9277517795562744 \n",
      "              Params: tensor([  5.3616, -17.2699])\n",
      "              Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch: 3659, Loss: 2.9277536869049072 \n",
      "              Params: tensor([  5.3616, -17.2700])\n",
      "              Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch: 3660, Loss: 2.9277520179748535 \n",
      "              Params: tensor([  5.3616, -17.2701])\n",
      "              Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch: 3661, Loss: 2.927751302719116 \n",
      "              Params: tensor([  5.3616, -17.2701])\n",
      "              Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch: 3662, Loss: 2.9277520179748535 \n",
      "              Params: tensor([  5.3616, -17.2702])\n",
      "              Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch: 3663, Loss: 2.9277503490448 \n",
      "              Params: tensor([  5.3616, -17.2702])\n",
      "              Grad: tensor([-0.0011,  0.0059])\n",
      "Epoch: 3664, Loss: 2.9277493953704834 \n",
      "              Params: tensor([  5.3616, -17.2703])\n",
      "              Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch: 3665, Loss: 2.927751302719116 \n",
      "              Params: tensor([  5.3616, -17.2703])\n",
      "              Grad: tensor([-0.0010,  0.0059])\n",
      "Epoch: 3666, Loss: 2.9277496337890625 \n",
      "              Params: tensor([  5.3616, -17.2704])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3667, Loss: 2.9277503490448 \n",
      "              Params: tensor([  5.3617, -17.2705])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3668, Loss: 2.9277472496032715 \n",
      "              Params: tensor([  5.3617, -17.2705])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3669, Loss: 2.927748680114746 \n",
      "              Params: tensor([  5.3617, -17.2706])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3670, Loss: 2.9277474880218506 \n",
      "              Params: tensor([  5.3617, -17.2706])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3671, Loss: 2.927747964859009 \n",
      "              Params: tensor([  5.3617, -17.2707])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3672, Loss: 2.927748203277588 \n",
      "              Params: tensor([  5.3617, -17.2708])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3673, Loss: 2.9277472496032715 \n",
      "              Params: tensor([  5.3617, -17.2708])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3674, Loss: 2.9277467727661133 \n",
      "              Params: tensor([  5.3617, -17.2709])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3675, Loss: 2.927747964859009 \n",
      "              Params: tensor([  5.3617, -17.2709])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3676, Loss: 2.9277474880218506 \n",
      "              Params: tensor([  5.3617, -17.2710])\n",
      "              Grad: tensor([-0.0010,  0.0058])\n",
      "Epoch: 3677, Loss: 2.927746534347534 \n",
      "              Params: tensor([  5.3618, -17.2710])\n",
      "              Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch: 3678, Loss: 2.9277451038360596 \n",
      "              Params: tensor([  5.3618, -17.2711])\n",
      "              Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch: 3679, Loss: 2.9277451038360596 \n",
      "              Params: tensor([  5.3618, -17.2712])\n",
      "              Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch: 3680, Loss: 2.9277455806732178 \n",
      "              Params: tensor([  5.3618, -17.2712])\n",
      "              Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch: 3681, Loss: 2.927744150161743 \n",
      "              Params: tensor([  5.3618, -17.2713])\n",
      "              Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch: 3682, Loss: 2.927743434906006 \n",
      "              Params: tensor([  5.3618, -17.2713])\n",
      "              Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch: 3683, Loss: 2.9277427196502686 \n",
      "              Params: tensor([  5.3618, -17.2714])\n",
      "              Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch: 3684, Loss: 2.9277427196502686 \n",
      "              Params: tensor([  5.3618, -17.2714])\n",
      "              Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch: 3685, Loss: 2.927743434906006 \n",
      "              Params: tensor([  5.3618, -17.2715])\n",
      "              Grad: tensor([-0.0010,  0.0057])\n",
      "Epoch: 3686, Loss: 2.9277429580688477 \n",
      "              Params: tensor([  5.3618, -17.2716])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3687, Loss: 2.927744150161743 \n",
      "              Params: tensor([  5.3619, -17.2716])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3688, Loss: 2.9277422428131104 \n",
      "              Params: tensor([  5.3619, -17.2717])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3689, Loss: 2.9277422428131104 \n",
      "              Params: tensor([  5.3619, -17.2717])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3690, Loss: 2.927741765975952 \n",
      "              Params: tensor([  5.3619, -17.2718])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3691, Loss: 2.9277420043945312 \n",
      "              Params: tensor([  5.3619, -17.2718])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3692, Loss: 2.927741289138794 \n",
      "              Params: tensor([  5.3619, -17.2719])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3693, Loss: 2.9277405738830566 \n",
      "              Params: tensor([  5.3619, -17.2719])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3694, Loss: 2.927741050720215 \n",
      "              Params: tensor([  5.3619, -17.2720])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3695, Loss: 2.927741765975952 \n",
      "              Params: tensor([  5.3619, -17.2721])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3696, Loss: 2.9277405738830566 \n",
      "              Params: tensor([  5.3619, -17.2721])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3697, Loss: 2.9277405738830566 \n",
      "              Params: tensor([  5.3620, -17.2722])\n",
      "              Grad: tensor([-0.0010,  0.0056])\n",
      "Epoch: 3698, Loss: 2.9277398586273193 \n",
      "              Params: tensor([  5.3620, -17.2722])\n",
      "              Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch: 3699, Loss: 2.927738904953003 \n",
      "              Params: tensor([  5.3620, -17.2723])\n",
      "              Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch: 3700, Loss: 2.9277384281158447 \n",
      "              Params: tensor([  5.3620, -17.2723])\n",
      "              Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch: 3701, Loss: 2.9277381896972656 \n",
      "              Params: tensor([  5.3620, -17.2724])\n",
      "              Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch: 3702, Loss: 2.927737236022949 \n",
      "              Params: tensor([  5.3620, -17.2724])\n",
      "              Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch: 3703, Loss: 2.927736759185791 \n",
      "              Params: tensor([  5.3620, -17.2725])\n",
      "              Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch: 3704, Loss: 2.9277379512786865 \n",
      "              Params: tensor([  5.3620, -17.2726])\n",
      "              Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch: 3705, Loss: 2.9277374744415283 \n",
      "              Params: tensor([  5.3620, -17.2726])\n",
      "              Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch: 3706, Loss: 2.9277360439300537 \n",
      "              Params: tensor([  5.3620, -17.2727])\n",
      "              Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch: 3707, Loss: 2.927736520767212 \n",
      "              Params: tensor([  5.3621, -17.2727])\n",
      "              Grad: tensor([-0.0010,  0.0055])\n",
      "Epoch: 3708, Loss: 2.927736520767212 \n",
      "              Params: tensor([  5.3621, -17.2728])\n",
      "              Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch: 3709, Loss: 2.9277358055114746 \n",
      "              Params: tensor([  5.3621, -17.2728])\n",
      "              Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch: 3710, Loss: 2.927734136581421 \n",
      "              Params: tensor([  5.3621, -17.2729])\n",
      "              Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch: 3711, Loss: 2.927734613418579 \n",
      "              Params: tensor([  5.3621, -17.2729])\n",
      "              Grad: tensor([-0.0010,  0.0054])\n",
      "Epoch: 3712, Loss: 2.9277353286743164 \n",
      "              Params: tensor([  5.3621, -17.2730])\n",
      "              Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch: 3713, Loss: 2.927734375 \n",
      "              Params: tensor([  5.3621, -17.2730])\n",
      "              Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch: 3714, Loss: 2.9277336597442627 \n",
      "              Params: tensor([  5.3621, -17.2731])\n",
      "              Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch: 3715, Loss: 2.9277334213256836 \n",
      "              Params: tensor([  5.3621, -17.2732])\n",
      "              Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch: 3716, Loss: 2.927734136581421 \n",
      "              Params: tensor([  5.3621, -17.2732])\n",
      "              Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch: 3717, Loss: 2.9277329444885254 \n",
      "              Params: tensor([  5.3622, -17.2733])\n",
      "              Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch: 3718, Loss: 2.9277334213256836 \n",
      "              Params: tensor([  5.3622, -17.2733])\n",
      "              Grad: tensor([-0.0009,  0.0054])\n",
      "Epoch: 3719, Loss: 2.9277327060699463 \n",
      "              Params: tensor([  5.3622, -17.2734])\n",
      "              Grad: tensor([-0.0010,  0.0053])\n",
      "Epoch: 3720, Loss: 2.927732229232788 \n",
      "              Params: tensor([  5.3622, -17.2734])\n",
      "              Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch: 3721, Loss: 2.9277312755584717 \n",
      "              Params: tensor([  5.3622, -17.2735])\n",
      "              Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch: 3722, Loss: 2.9277307987213135 \n",
      "              Params: tensor([  5.3622, -17.2735])\n",
      "              Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch: 3723, Loss: 2.9277327060699463 \n",
      "              Params: tensor([  5.3622, -17.2736])\n",
      "              Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch: 3724, Loss: 2.9277303218841553 \n",
      "              Params: tensor([  5.3622, -17.2736])\n",
      "              Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch: 3725, Loss: 2.927729845046997 \n",
      "              Params: tensor([  5.3622, -17.2737])\n",
      "              Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch: 3726, Loss: 2.927731513977051 \n",
      "              Params: tensor([  5.3622, -17.2737])\n",
      "              Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch: 3727, Loss: 2.9277303218841553 \n",
      "              Params: tensor([  5.3622, -17.2738])\n",
      "              Grad: tensor([-0.0010,  0.0053])\n",
      "Epoch: 3728, Loss: 2.927731513977051 \n",
      "              Params: tensor([  5.3623, -17.2738])\n",
      "              Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch: 3729, Loss: 2.9277312755584717 \n",
      "              Params: tensor([  5.3623, -17.2739])\n",
      "              Grad: tensor([-0.0009,  0.0053])\n",
      "Epoch: 3730, Loss: 2.9277303218841553 \n",
      "              Params: tensor([  5.3623, -17.2740])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3731, Loss: 2.9277284145355225 \n",
      "              Params: tensor([  5.3623, -17.2740])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3732, Loss: 2.9277291297912598 \n",
      "              Params: tensor([  5.3623, -17.2741])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3733, Loss: 2.9277291297912598 \n",
      "              Params: tensor([  5.3623, -17.2741])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3734, Loss: 2.9277284145355225 \n",
      "              Params: tensor([  5.3623, -17.2742])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3735, Loss: 2.927727699279785 \n",
      "              Params: tensor([  5.3623, -17.2742])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3736, Loss: 2.9277281761169434 \n",
      "              Params: tensor([  5.3623, -17.2743])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3737, Loss: 2.9277281761169434 \n",
      "              Params: tensor([  5.3623, -17.2743])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3738, Loss: 2.927727460861206 \n",
      "              Params: tensor([  5.3623, -17.2744])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3739, Loss: 2.9277281761169434 \n",
      "              Params: tensor([  5.3624, -17.2744])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3740, Loss: 2.927727699279785 \n",
      "              Params: tensor([  5.3624, -17.2745])\n",
      "              Grad: tensor([-0.0009,  0.0052])\n",
      "Epoch: 3741, Loss: 2.9277265071868896 \n",
      "              Params: tensor([  5.3624, -17.2745])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3742, Loss: 2.927727460861206 \n",
      "              Params: tensor([  5.3624, -17.2746])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3743, Loss: 2.9277257919311523 \n",
      "              Params: tensor([  5.3624, -17.2746])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3744, Loss: 2.9277260303497314 \n",
      "              Params: tensor([  5.3624, -17.2747])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3745, Loss: 2.927725076675415 \n",
      "              Params: tensor([  5.3624, -17.2747])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3746, Loss: 2.927725076675415 \n",
      "              Params: tensor([  5.3624, -17.2748])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3747, Loss: 2.927725076675415 \n",
      "              Params: tensor([  5.3624, -17.2748])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3748, Loss: 2.9277231693267822 \n",
      "              Params: tensor([  5.3624, -17.2749])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3749, Loss: 2.9277243614196777 \n",
      "              Params: tensor([  5.3624, -17.2749])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3750, Loss: 2.9277236461639404 \n",
      "              Params: tensor([  5.3625, -17.2750])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3751, Loss: 2.927725315093994 \n",
      "              Params: tensor([  5.3625, -17.2750])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3752, Loss: 2.9277238845825195 \n",
      "              Params: tensor([  5.3625, -17.2751])\n",
      "              Grad: tensor([-0.0009,  0.0051])\n",
      "Epoch: 3753, Loss: 2.9277238845825195 \n",
      "              Params: tensor([  5.3625, -17.2751])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3754, Loss: 2.9277231693267822 \n",
      "              Params: tensor([  5.3625, -17.2752])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3755, Loss: 2.927722692489624 \n",
      "              Params: tensor([  5.3625, -17.2752])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3756, Loss: 2.927722692489624 \n",
      "              Params: tensor([  5.3625, -17.2753])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3757, Loss: 2.9277215003967285 \n",
      "              Params: tensor([  5.3625, -17.2753])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3758, Loss: 2.927722930908203 \n",
      "              Params: tensor([  5.3625, -17.2754])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3759, Loss: 2.9277215003967285 \n",
      "              Params: tensor([  5.3625, -17.2754])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3760, Loss: 2.927722692489624 \n",
      "              Params: tensor([  5.3625, -17.2755])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3761, Loss: 2.9277212619781494 \n",
      "              Params: tensor([  5.3626, -17.2755])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3762, Loss: 2.9277215003967285 \n",
      "              Params: tensor([  5.3626, -17.2756])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3763, Loss: 2.927719831466675 \n",
      "              Params: tensor([  5.3626, -17.2756])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3764, Loss: 2.927720069885254 \n",
      "              Params: tensor([  5.3626, -17.2757])\n",
      "              Grad: tensor([-0.0009,  0.0050])\n",
      "Epoch: 3765, Loss: 2.9277191162109375 \n",
      "              Params: tensor([  5.3626, -17.2757])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3766, Loss: 2.927720785140991 \n",
      "              Params: tensor([  5.3626, -17.2758])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3767, Loss: 2.9277193546295166 \n",
      "              Params: tensor([  5.3626, -17.2758])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3768, Loss: 2.9277191162109375 \n",
      "              Params: tensor([  5.3626, -17.2759])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3769, Loss: 2.9277191162109375 \n",
      "              Params: tensor([  5.3626, -17.2759])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3770, Loss: 2.9277188777923584 \n",
      "              Params: tensor([  5.3626, -17.2760])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3771, Loss: 2.9277188777923584 \n",
      "              Params: tensor([  5.3626, -17.2760])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3772, Loss: 2.927719831466675 \n",
      "              Params: tensor([  5.3626, -17.2761])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3773, Loss: 2.927718162536621 \n",
      "              Params: tensor([  5.3627, -17.2761])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3774, Loss: 2.9277184009552 \n",
      "              Params: tensor([  5.3627, -17.2762])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3775, Loss: 2.927717447280884 \n",
      "              Params: tensor([  5.3627, -17.2762])\n",
      "              Grad: tensor([-0.0009,  0.0049])\n",
      "Epoch: 3776, Loss: 2.927718162536621 \n",
      "              Params: tensor([  5.3627, -17.2763])\n",
      "              Grad: tensor([-0.0008,  0.0049])\n",
      "Epoch: 3777, Loss: 2.9277167320251465 \n",
      "              Params: tensor([  5.3627, -17.2763])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3778, Loss: 2.9277169704437256 \n",
      "              Params: tensor([  5.3627, -17.2764])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3779, Loss: 2.927716016769409 \n",
      "              Params: tensor([  5.3627, -17.2764])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3780, Loss: 2.9277162551879883 \n",
      "              Params: tensor([  5.3627, -17.2765])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3781, Loss: 2.9277167320251465 \n",
      "              Params: tensor([  5.3627, -17.2765])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3782, Loss: 2.9277167320251465 \n",
      "              Params: tensor([  5.3627, -17.2766])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3783, Loss: 2.927716016769409 \n",
      "              Params: tensor([  5.3627, -17.2766])\n",
      "              Grad: tensor([-0.0009,  0.0048])\n",
      "Epoch: 3784, Loss: 2.9277150630950928 \n",
      "              Params: tensor([  5.3627, -17.2767])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3785, Loss: 2.927715301513672 \n",
      "              Params: tensor([  5.3628, -17.2767])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3786, Loss: 2.927715301513672 \n",
      "              Params: tensor([  5.3628, -17.2767])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3787, Loss: 2.9277150630950928 \n",
      "              Params: tensor([  5.3628, -17.2768])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3788, Loss: 2.9277145862579346 \n",
      "              Params: tensor([  5.3628, -17.2768])\n",
      "              Grad: tensor([-0.0008,  0.0048])\n",
      "Epoch: 3789, Loss: 2.9277150630950928 \n",
      "              Params: tensor([  5.3628, -17.2769])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3790, Loss: 2.9277138710021973 \n",
      "              Params: tensor([  5.3628, -17.2769])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3791, Loss: 2.927713632583618 \n",
      "              Params: tensor([  5.3628, -17.2770])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3792, Loss: 2.9277138710021973 \n",
      "              Params: tensor([  5.3628, -17.2770])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3793, Loss: 2.927713632583618 \n",
      "              Params: tensor([  5.3628, -17.2771])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3794, Loss: 2.92771315574646 \n",
      "              Params: tensor([  5.3628, -17.2771])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3795, Loss: 2.9277143478393555 \n",
      "              Params: tensor([  5.3628, -17.2772])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3796, Loss: 2.927712917327881 \n",
      "              Params: tensor([  5.3629, -17.2772])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3797, Loss: 2.9277122020721436 \n",
      "              Params: tensor([  5.3629, -17.2773])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3798, Loss: 2.9277124404907227 \n",
      "              Params: tensor([  5.3629, -17.2773])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3799, Loss: 2.927712917327881 \n",
      "              Params: tensor([  5.3629, -17.2774])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3800, Loss: 2.927711248397827 \n",
      "              Params: tensor([  5.3629, -17.2774])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3801, Loss: 2.9277117252349854 \n",
      "              Params: tensor([  5.3629, -17.2775])\n",
      "              Grad: tensor([-0.0008,  0.0047])\n",
      "Epoch: 3802, Loss: 2.9277124404907227 \n",
      "              Params: tensor([  5.3629, -17.2775])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3803, Loss: 2.9277114868164062 \n",
      "              Params: tensor([  5.3629, -17.2775])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3804, Loss: 2.9277122020721436 \n",
      "              Params: tensor([  5.3629, -17.2776])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3805, Loss: 2.927710771560669 \n",
      "              Params: tensor([  5.3629, -17.2776])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3806, Loss: 2.92771053314209 \n",
      "              Params: tensor([  5.3629, -17.2777])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3807, Loss: 2.927710771560669 \n",
      "              Params: tensor([  5.3629, -17.2777])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3808, Loss: 2.9277093410491943 \n",
      "              Params: tensor([  5.3629, -17.2778])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3809, Loss: 2.9277100563049316 \n",
      "              Params: tensor([  5.3630, -17.2778])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3810, Loss: 2.9277100563049316 \n",
      "              Params: tensor([  5.3630, -17.2779])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3811, Loss: 2.927708387374878 \n",
      "              Params: tensor([  5.3630, -17.2779])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3812, Loss: 2.9277079105377197 \n",
      "              Params: tensor([  5.3630, -17.2780])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3813, Loss: 2.927708625793457 \n",
      "              Params: tensor([  5.3630, -17.2780])\n",
      "              Grad: tensor([-0.0008,  0.0046])\n",
      "Epoch: 3814, Loss: 2.9277093410491943 \n",
      "              Params: tensor([  5.3630, -17.2781])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3815, Loss: 2.9277098178863525 \n",
      "              Params: tensor([  5.3630, -17.2781])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3816, Loss: 2.9277079105377197 \n",
      "              Params: tensor([  5.3630, -17.2781])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3817, Loss: 2.9277079105377197 \n",
      "              Params: tensor([  5.3630, -17.2782])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3818, Loss: 2.927706241607666 \n",
      "              Params: tensor([  5.3630, -17.2782])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3819, Loss: 2.927706718444824 \n",
      "              Params: tensor([  5.3630, -17.2783])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3820, Loss: 2.9277076721191406 \n",
      "              Params: tensor([  5.3630, -17.2783])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3821, Loss: 2.9277074337005615 \n",
      "              Params: tensor([  5.3631, -17.2784])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3822, Loss: 2.9277074337005615 \n",
      "              Params: tensor([  5.3631, -17.2784])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3823, Loss: 2.927706718444824 \n",
      "              Params: tensor([  5.3631, -17.2785])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3824, Loss: 2.9277079105377197 \n",
      "              Params: tensor([  5.3631, -17.2785])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3825, Loss: 2.9277074337005615 \n",
      "              Params: tensor([  5.3631, -17.2786])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3826, Loss: 2.927706003189087 \n",
      "              Params: tensor([  5.3631, -17.2786])\n",
      "              Grad: tensor([-0.0008,  0.0045])\n",
      "Epoch: 3827, Loss: 2.9277069568634033 \n",
      "              Params: tensor([  5.3631, -17.2786])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3828, Loss: 2.9277052879333496 \n",
      "              Params: tensor([  5.3631, -17.2787])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3829, Loss: 2.927706003189087 \n",
      "              Params: tensor([  5.3631, -17.2787])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3830, Loss: 2.927706003189087 \n",
      "              Params: tensor([  5.3631, -17.2788])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3831, Loss: 2.9277052879333496 \n",
      "              Params: tensor([  5.3631, -17.2788])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3832, Loss: 2.9277048110961914 \n",
      "              Params: tensor([  5.3631, -17.2789])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3833, Loss: 2.9277052879333496 \n",
      "              Params: tensor([  5.3631, -17.2789])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3834, Loss: 2.9277048110961914 \n",
      "              Params: tensor([  5.3632, -17.2789])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3835, Loss: 2.9277048110961914 \n",
      "              Params: tensor([  5.3632, -17.2790])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3836, Loss: 2.9277045726776123 \n",
      "              Params: tensor([  5.3632, -17.2790])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3837, Loss: 2.927703619003296 \n",
      "              Params: tensor([  5.3632, -17.2791])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3838, Loss: 2.927703619003296 \n",
      "              Params: tensor([  5.3632, -17.2791])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3839, Loss: 2.927703619003296 \n",
      "              Params: tensor([  5.3632, -17.2792])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3840, Loss: 2.9277031421661377 \n",
      "              Params: tensor([  5.3632, -17.2792])\n",
      "              Grad: tensor([-0.0008,  0.0044])\n",
      "Epoch: 3841, Loss: 2.9277021884918213 \n",
      "              Params: tensor([  5.3632, -17.2793])\n",
      "              Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch: 3842, Loss: 2.9277029037475586 \n",
      "              Params: tensor([  5.3632, -17.2793])\n",
      "              Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch: 3843, Loss: 2.9277029037475586 \n",
      "              Params: tensor([  5.3632, -17.2793])\n",
      "              Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch: 3844, Loss: 2.927703619003296 \n",
      "              Params: tensor([  5.3632, -17.2794])\n",
      "              Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch: 3845, Loss: 2.9277024269104004 \n",
      "              Params: tensor([  5.3632, -17.2794])\n",
      "              Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch: 3846, Loss: 2.9277007579803467 \n",
      "              Params: tensor([  5.3632, -17.2795])\n",
      "              Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch: 3847, Loss: 2.9277029037475586 \n",
      "              Params: tensor([  5.3633, -17.2795])\n",
      "              Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch: 3848, Loss: 2.927701711654663 \n",
      "              Params: tensor([  5.3633, -17.2796])\n",
      "              Grad: tensor([-0.0008,  0.0043])\n",
      "Epoch: 3849, Loss: 2.927700996398926 \n",
      "              Params: tensor([  5.3633, -17.2796])\n",
      "              Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch: 3850, Loss: 2.927700996398926 \n",
      "              Params: tensor([  5.3633, -17.2796])\n",
      "              Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch: 3851, Loss: 2.9277029037475586 \n",
      "              Params: tensor([  5.3633, -17.2797])\n",
      "              Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch: 3852, Loss: 2.9277002811431885 \n",
      "              Params: tensor([  5.3633, -17.2797])\n",
      "              Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch: 3853, Loss: 2.927700996398926 \n",
      "              Params: tensor([  5.3633, -17.2798])\n",
      "              Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch: 3854, Loss: 2.927700996398926 \n",
      "              Params: tensor([  5.3633, -17.2798])\n",
      "              Grad: tensor([-0.0007,  0.0043])\n",
      "Epoch: 3855, Loss: 2.9277000427246094 \n",
      "              Params: tensor([  5.3633, -17.2799])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3856, Loss: 2.9277000427246094 \n",
      "              Params: tensor([  5.3633, -17.2799])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3857, Loss: 2.9276998043060303 \n",
      "              Params: tensor([  5.3633, -17.2799])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3858, Loss: 2.9277007579803467 \n",
      "              Params: tensor([  5.3633, -17.2800])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3859, Loss: 2.927699327468872 \n",
      "              Params: tensor([  5.3633, -17.2800])\n",
      "              Grad: tensor([-0.0008,  0.0042])\n",
      "Epoch: 3860, Loss: 2.927699327468872 \n",
      "              Params: tensor([  5.3634, -17.2801])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3861, Loss: 2.9277002811431885 \n",
      "              Params: tensor([  5.3634, -17.2801])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3862, Loss: 2.927699327468872 \n",
      "              Params: tensor([  5.3634, -17.2801])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3863, Loss: 2.9276983737945557 \n",
      "              Params: tensor([  5.3634, -17.2802])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3864, Loss: 2.927699327468872 \n",
      "              Params: tensor([  5.3634, -17.2802])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3865, Loss: 2.927696943283081 \n",
      "              Params: tensor([  5.3634, -17.2803])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3866, Loss: 2.9277000427246094 \n",
      "              Params: tensor([  5.3634, -17.2803])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3867, Loss: 2.927699327468872 \n",
      "              Params: tensor([  5.3634, -17.2804])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3868, Loss: 2.9276976585388184 \n",
      "              Params: tensor([  5.3634, -17.2804])\n",
      "              Grad: tensor([-0.0007,  0.0042])\n",
      "Epoch: 3869, Loss: 2.927696943283081 \n",
      "              Params: tensor([  5.3634, -17.2804])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3870, Loss: 2.9276983737945557 \n",
      "              Params: tensor([  5.3634, -17.2805])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3871, Loss: 2.9276955127716064 \n",
      "              Params: tensor([  5.3634, -17.2805])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3872, Loss: 2.9276986122131348 \n",
      "              Params: tensor([  5.3634, -17.2806])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3873, Loss: 2.9276976585388184 \n",
      "              Params: tensor([  5.3634, -17.2806])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3874, Loss: 2.9276962280273438 \n",
      "              Params: tensor([  5.3635, -17.2806])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3875, Loss: 2.9276983737945557 \n",
      "              Params: tensor([  5.3635, -17.2807])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3876, Loss: 2.92769718170166 \n",
      "              Params: tensor([  5.3635, -17.2807])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3877, Loss: 2.9276962280273438 \n",
      "              Params: tensor([  5.3635, -17.2808])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3878, Loss: 2.92769718170166 \n",
      "              Params: tensor([  5.3635, -17.2808])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3879, Loss: 2.927696466445923 \n",
      "              Params: tensor([  5.3635, -17.2808])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3880, Loss: 2.9276959896087646 \n",
      "              Params: tensor([  5.3635, -17.2809])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3881, Loss: 2.9276959896087646 \n",
      "              Params: tensor([  5.3635, -17.2809])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3882, Loss: 2.9276955127716064 \n",
      "              Params: tensor([  5.3635, -17.2810])\n",
      "              Grad: tensor([-0.0007,  0.0041])\n",
      "Epoch: 3883, Loss: 2.9276955127716064 \n",
      "              Params: tensor([  5.3635, -17.2810])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3884, Loss: 2.92769455909729 \n",
      "              Params: tensor([  5.3635, -17.2810])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3885, Loss: 2.927696466445923 \n",
      "              Params: tensor([  5.3635, -17.2811])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3886, Loss: 2.9276959896087646 \n",
      "              Params: tensor([  5.3635, -17.2811])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3887, Loss: 2.92769455909729 \n",
      "              Params: tensor([  5.3635, -17.2812])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3888, Loss: 2.92769455909729 \n",
      "              Params: tensor([  5.3636, -17.2812])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3889, Loss: 2.927694082260132 \n",
      "              Params: tensor([  5.3636, -17.2812])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3890, Loss: 2.9276933670043945 \n",
      "              Params: tensor([  5.3636, -17.2813])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3891, Loss: 2.9276926517486572 \n",
      "              Params: tensor([  5.3636, -17.2813])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3892, Loss: 2.9276952743530273 \n",
      "              Params: tensor([  5.3636, -17.2814])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3893, Loss: 2.927694797515869 \n",
      "              Params: tensor([  5.3636, -17.2814])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3894, Loss: 2.9276938438415527 \n",
      "              Params: tensor([  5.3636, -17.2815])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3895, Loss: 2.9276955127716064 \n",
      "              Params: tensor([  5.3636, -17.2815])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3896, Loss: 2.9276926517486572 \n",
      "              Params: tensor([  5.3636, -17.2815])\n",
      "              Grad: tensor([-0.0007,  0.0040])\n",
      "Epoch: 3897, Loss: 2.9276926517486572 \n",
      "              Params: tensor([  5.3636, -17.2816])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3898, Loss: 2.927694082260132 \n",
      "              Params: tensor([  5.3636, -17.2816])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3899, Loss: 2.9276931285858154 \n",
      "              Params: tensor([  5.3636, -17.2817])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3900, Loss: 2.927692174911499 \n",
      "              Params: tensor([  5.3636, -17.2817])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3901, Loss: 2.927694082260132 \n",
      "              Params: tensor([  5.3636, -17.2817])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3902, Loss: 2.927692413330078 \n",
      "              Params: tensor([  5.3637, -17.2818])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3903, Loss: 2.9276933670043945 \n",
      "              Params: tensor([  5.3637, -17.2818])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3904, Loss: 2.9276914596557617 \n",
      "              Params: tensor([  5.3637, -17.2818])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3905, Loss: 2.927692174911499 \n",
      "              Params: tensor([  5.3637, -17.2819])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3906, Loss: 2.927692174911499 \n",
      "              Params: tensor([  5.3637, -17.2819])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3907, Loss: 2.927692413330078 \n",
      "              Params: tensor([  5.3637, -17.2820])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3908, Loss: 2.9276914596557617 \n",
      "              Params: tensor([  5.3637, -17.2820])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3909, Loss: 2.927692174911499 \n",
      "              Params: tensor([  5.3637, -17.2820])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3910, Loss: 2.927690267562866 \n",
      "              Params: tensor([  5.3637, -17.2821])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3911, Loss: 2.9276914596557617 \n",
      "              Params: tensor([  5.3637, -17.2821])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3912, Loss: 2.9276907444000244 \n",
      "              Params: tensor([  5.3637, -17.2822])\n",
      "              Grad: tensor([-0.0007,  0.0039])\n",
      "Epoch: 3913, Loss: 2.9276914596557617 \n",
      "              Params: tensor([  5.3637, -17.2822])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3914, Loss: 2.927689552307129 \n",
      "              Params: tensor([  5.3637, -17.2822])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3915, Loss: 2.9276909828186035 \n",
      "              Params: tensor([  5.3637, -17.2823])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3916, Loss: 2.9276907444000244 \n",
      "              Params: tensor([  5.3637, -17.2823])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3917, Loss: 2.92768931388855 \n",
      "              Params: tensor([  5.3638, -17.2823])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3918, Loss: 2.927690267562866 \n",
      "              Params: tensor([  5.3638, -17.2824])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3919, Loss: 2.927689552307129 \n",
      "              Params: tensor([  5.3638, -17.2824])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3920, Loss: 2.927689552307129 \n",
      "              Params: tensor([  5.3638, -17.2825])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3921, Loss: 2.927689552307129 \n",
      "              Params: tensor([  5.3638, -17.2825])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3922, Loss: 2.9276888370513916 \n",
      "              Params: tensor([  5.3638, -17.2825])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3923, Loss: 2.9276885986328125 \n",
      "              Params: tensor([  5.3638, -17.2826])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3924, Loss: 2.9276888370513916 \n",
      "              Params: tensor([  5.3638, -17.2826])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3925, Loss: 2.927687644958496 \n",
      "              Params: tensor([  5.3638, -17.2826])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3926, Loss: 2.9276885986328125 \n",
      "              Params: tensor([  5.3638, -17.2827])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3927, Loss: 2.92768931388855 \n",
      "              Params: tensor([  5.3638, -17.2827])\n",
      "              Grad: tensor([-0.0007,  0.0038])\n",
      "Epoch: 3928, Loss: 2.9276883602142334 \n",
      "              Params: tensor([  5.3638, -17.2828])\n",
      "              Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch: 3929, Loss: 2.9276883602142334 \n",
      "              Params: tensor([  5.3638, -17.2828])\n",
      "              Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch: 3930, Loss: 2.927687883377075 \n",
      "              Params: tensor([  5.3638, -17.2828])\n",
      "              Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch: 3931, Loss: 2.927687644958496 \n",
      "              Params: tensor([  5.3638, -17.2829])\n",
      "              Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch: 3932, Loss: 2.927686929702759 \n",
      "              Params: tensor([  5.3639, -17.2829])\n",
      "              Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch: 3933, Loss: 2.9276888370513916 \n",
      "              Params: tensor([  5.3639, -17.2829])\n",
      "              Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch: 3934, Loss: 2.927687883377075 \n",
      "              Params: tensor([  5.3639, -17.2830])\n",
      "              Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch: 3935, Loss: 2.927686929702759 \n",
      "              Params: tensor([  5.3639, -17.2830])\n",
      "              Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch: 3936, Loss: 2.927687168121338 \n",
      "              Params: tensor([  5.3639, -17.2831])\n",
      "              Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch: 3937, Loss: 2.927686929702759 \n",
      "              Params: tensor([  5.3639, -17.2831])\n",
      "              Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch: 3938, Loss: 2.9276864528656006 \n",
      "              Params: tensor([  5.3639, -17.2831])\n",
      "              Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch: 3939, Loss: 2.9276864528656006 \n",
      "              Params: tensor([  5.3639, -17.2832])\n",
      "              Grad: tensor([-0.0007,  0.0037])\n",
      "Epoch: 3940, Loss: 2.927686929702759 \n",
      "              Params: tensor([  5.3639, -17.2832])\n",
      "              Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch: 3941, Loss: 2.9276864528656006 \n",
      "              Params: tensor([  5.3639, -17.2832])\n",
      "              Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch: 3942, Loss: 2.9276862144470215 \n",
      "              Params: tensor([  5.3639, -17.2833])\n",
      "              Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch: 3943, Loss: 2.9276864528656006 \n",
      "              Params: tensor([  5.3639, -17.2833])\n",
      "              Grad: tensor([-0.0006,  0.0037])\n",
      "Epoch: 3944, Loss: 2.9276862144470215 \n",
      "              Params: tensor([  5.3639, -17.2833])\n",
      "              Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch: 3945, Loss: 2.927685022354126 \n",
      "              Params: tensor([  5.3639, -17.2834])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3946, Loss: 2.927685499191284 \n",
      "              Params: tensor([  5.3639, -17.2834])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3947, Loss: 2.9276857376098633 \n",
      "              Params: tensor([  5.3640, -17.2835])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3948, Loss: 2.927685499191284 \n",
      "              Params: tensor([  5.3640, -17.2835])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3949, Loss: 2.9276864528656006 \n",
      "              Params: tensor([  5.3640, -17.2835])\n",
      "              Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch: 3950, Loss: 2.9276864528656006 \n",
      "              Params: tensor([  5.3640, -17.2836])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3951, Loss: 2.927686929702759 \n",
      "              Params: tensor([  5.3640, -17.2836])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3952, Loss: 2.927685022354126 \n",
      "              Params: tensor([  5.3640, -17.2836])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3953, Loss: 2.9276857376098633 \n",
      "              Params: tensor([  5.3640, -17.2837])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3954, Loss: 2.9276857376098633 \n",
      "              Params: tensor([  5.3640, -17.2837])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3955, Loss: 2.9276845455169678 \n",
      "              Params: tensor([  5.3640, -17.2837])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3956, Loss: 2.927683115005493 \n",
      "              Params: tensor([  5.3640, -17.2838])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3957, Loss: 2.9276838302612305 \n",
      "              Params: tensor([  5.3640, -17.2838])\n",
      "              Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch: 3958, Loss: 2.927684783935547 \n",
      "              Params: tensor([  5.3640, -17.2839])\n",
      "              Grad: tensor([-0.0006,  0.0036])\n",
      "Epoch: 3959, Loss: 2.9276840686798096 \n",
      "              Params: tensor([  5.3640, -17.2839])\n",
      "              Grad: tensor([-0.0007,  0.0036])\n",
      "Epoch: 3960, Loss: 2.9276840686798096 \n",
      "              Params: tensor([  5.3640, -17.2839])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3961, Loss: 2.9276840686798096 \n",
      "              Params: tensor([  5.3640, -17.2840])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3962, Loss: 2.927685499191284 \n",
      "              Params: tensor([  5.3640, -17.2840])\n",
      "              Grad: tensor([-0.0007,  0.0035])\n",
      "Epoch: 3963, Loss: 2.9276833534240723 \n",
      "              Params: tensor([  5.3641, -17.2840])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3964, Loss: 2.927684783935547 \n",
      "              Params: tensor([  5.3641, -17.2841])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3965, Loss: 2.927684783935547 \n",
      "              Params: tensor([  5.3641, -17.2841])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3966, Loss: 2.9276840686798096 \n",
      "              Params: tensor([  5.3641, -17.2841])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3967, Loss: 2.927682638168335 \n",
      "              Params: tensor([  5.3641, -17.2842])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3968, Loss: 2.927682638168335 \n",
      "              Params: tensor([  5.3641, -17.2842])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3969, Loss: 2.927682399749756 \n",
      "              Params: tensor([  5.3641, -17.2842])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3970, Loss: 2.927683115005493 \n",
      "              Params: tensor([  5.3641, -17.2843])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3971, Loss: 2.9276838302612305 \n",
      "              Params: tensor([  5.3641, -17.2843])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3972, Loss: 2.9276819229125977 \n",
      "              Params: tensor([  5.3641, -17.2843])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3973, Loss: 2.927683115005493 \n",
      "              Params: tensor([  5.3641, -17.2844])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3974, Loss: 2.9276833534240723 \n",
      "              Params: tensor([  5.3641, -17.2844])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3975, Loss: 2.927683115005493 \n",
      "              Params: tensor([  5.3641, -17.2844])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3976, Loss: 2.927682399749756 \n",
      "              Params: tensor([  5.3641, -17.2845])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3977, Loss: 2.9276819229125977 \n",
      "              Params: tensor([  5.3641, -17.2845])\n",
      "              Grad: tensor([-0.0006,  0.0035])\n",
      "Epoch: 3978, Loss: 2.9276816844940186 \n",
      "              Params: tensor([  5.3641, -17.2845])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3979, Loss: 2.9276809692382812 \n",
      "              Params: tensor([  5.3642, -17.2846])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3980, Loss: 2.9276816844940186 \n",
      "              Params: tensor([  5.3642, -17.2846])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3981, Loss: 2.9276819229125977 \n",
      "              Params: tensor([  5.3642, -17.2847])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3982, Loss: 2.9276816844940186 \n",
      "              Params: tensor([  5.3642, -17.2847])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3983, Loss: 2.9276816844940186 \n",
      "              Params: tensor([  5.3642, -17.2847])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3984, Loss: 2.927682399749756 \n",
      "              Params: tensor([  5.3642, -17.2848])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3985, Loss: 2.927682399749756 \n",
      "              Params: tensor([  5.3642, -17.2848])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3986, Loss: 2.927680253982544 \n",
      "              Params: tensor([  5.3642, -17.2848])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3987, Loss: 2.9276819229125977 \n",
      "              Params: tensor([  5.3642, -17.2849])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3988, Loss: 2.9276809692382812 \n",
      "              Params: tensor([  5.3642, -17.2849])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3989, Loss: 2.9276812076568604 \n",
      "              Params: tensor([  5.3642, -17.2849])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3990, Loss: 2.9276795387268066 \n",
      "              Params: tensor([  5.3642, -17.2850])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3991, Loss: 2.9276809692382812 \n",
      "              Params: tensor([  5.3642, -17.2850])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3992, Loss: 2.927680015563965 \n",
      "              Params: tensor([  5.3642, -17.2850])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3993, Loss: 2.927680015563965 \n",
      "              Params: tensor([  5.3642, -17.2851])\n",
      "              Grad: tensor([-0.0006,  0.0034])\n",
      "Epoch: 3994, Loss: 2.927680730819702 \n",
      "              Params: tensor([  5.3642, -17.2851])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 3995, Loss: 2.9276809692382812 \n",
      "              Params: tensor([  5.3642, -17.2851])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 3996, Loss: 2.9276788234710693 \n",
      "              Params: tensor([  5.3643, -17.2852])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 3997, Loss: 2.927680253982544 \n",
      "              Params: tensor([  5.3643, -17.2852])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 3998, Loss: 2.9276785850524902 \n",
      "              Params: tensor([  5.3643, -17.2852])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 3999, Loss: 2.927680253982544 \n",
      "              Params: tensor([  5.3643, -17.2853])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4000, Loss: 2.927680253982544 \n",
      "              Params: tensor([  5.3643, -17.2853])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4001, Loss: 2.927680730819702 \n",
      "              Params: tensor([  5.3643, -17.2853])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4002, Loss: 2.9276788234710693 \n",
      "              Params: tensor([  5.3643, -17.2854])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4003, Loss: 2.9276788234710693 \n",
      "              Params: tensor([  5.3643, -17.2854])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4004, Loss: 2.927680015563965 \n",
      "              Params: tensor([  5.3643, -17.2854])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4005, Loss: 2.9276795387268066 \n",
      "              Params: tensor([  5.3643, -17.2855])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4006, Loss: 2.9276771545410156 \n",
      "              Params: tensor([  5.3643, -17.2855])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4007, Loss: 2.927677869796753 \n",
      "              Params: tensor([  5.3643, -17.2855])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4008, Loss: 2.9276785850524902 \n",
      "              Params: tensor([  5.3643, -17.2856])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4009, Loss: 2.927678108215332 \n",
      "              Params: tensor([  5.3643, -17.2856])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4010, Loss: 2.9276785850524902 \n",
      "              Params: tensor([  5.3643, -17.2856])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4011, Loss: 2.9276785850524902 \n",
      "              Params: tensor([  5.3643, -17.2857])\n",
      "              Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4012, Loss: 2.9276788234710693 \n",
      "              Params: tensor([  5.3643, -17.2857])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4013, Loss: 2.9276769161224365 \n",
      "              Params: tensor([  5.3644, -17.2857])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4014, Loss: 2.9276769161224365 \n",
      "              Params: tensor([  5.3644, -17.2857])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4015, Loss: 2.9276769161224365 \n",
      "              Params: tensor([  5.3644, -17.2858])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4016, Loss: 2.9276788234710693 \n",
      "              Params: tensor([  5.3644, -17.2858])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4017, Loss: 2.9276773929595947 \n",
      "              Params: tensor([  5.3644, -17.2858])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4018, Loss: 2.927677869796753 \n",
      "              Params: tensor([  5.3644, -17.2859])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4019, Loss: 2.927677869796753 \n",
      "              Params: tensor([  5.3644, -17.2859])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4020, Loss: 2.9276773929595947 \n",
      "              Params: tensor([  5.3644, -17.2859])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4021, Loss: 2.9276771545410156 \n",
      "              Params: tensor([  5.3644, -17.2860])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4022, Loss: 2.927677869796753 \n",
      "              Params: tensor([  5.3644, -17.2860])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4023, Loss: 2.9276771545410156 \n",
      "              Params: tensor([  5.3644, -17.2860])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4024, Loss: 2.9276769161224365 \n",
      "              Params: tensor([  5.3644, -17.2861])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4025, Loss: 2.9276764392852783 \n",
      "              Params: tensor([  5.3644, -17.2861])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4026, Loss: 2.9276764392852783 \n",
      "              Params: tensor([  5.3644, -17.2861])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4027, Loss: 2.927675485610962 \n",
      "              Params: tensor([  5.3644, -17.2862])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4028, Loss: 2.9276769161224365 \n",
      "              Params: tensor([  5.3644, -17.2862])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4029, Loss: 2.9276742935180664 \n",
      "              Params: tensor([  5.3644, -17.2862])\n",
      "              Grad: tensor([-0.0006,  0.0032])\n",
      "Epoch: 4030, Loss: 2.9276764392852783 \n",
      "              Params: tensor([  5.3644, -17.2863])\n",
      "              Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch: 4031, Loss: 2.9276750087738037 \n",
      "              Params: tensor([  5.3645, -17.2863])\n",
      "              Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch: 4032, Loss: 2.927675485610962 \n",
      "              Params: tensor([  5.3645, -17.2863])\n",
      "              Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch: 4033, Loss: 2.927675485610962 \n",
      "              Params: tensor([  5.3645, -17.2864])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4034, Loss: 2.927675485610962 \n",
      "              Params: tensor([  5.3645, -17.2864])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4035, Loss: 2.9276742935180664 \n",
      "              Params: tensor([  5.3645, -17.2864])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4036, Loss: 2.9276742935180664 \n",
      "              Params: tensor([  5.3645, -17.2865])\n",
      "              Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch: 4037, Loss: 2.9276769161224365 \n",
      "              Params: tensor([  5.3645, -17.2865])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4038, Loss: 2.9276742935180664 \n",
      "              Params: tensor([  5.3645, -17.2865])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4039, Loss: 2.927675724029541 \n",
      "              Params: tensor([  5.3645, -17.2865])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4040, Loss: 2.9276750087738037 \n",
      "              Params: tensor([  5.3645, -17.2866])\n",
      "              Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch: 4041, Loss: 2.9276747703552246 \n",
      "              Params: tensor([  5.3645, -17.2866])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4042, Loss: 2.9276750087738037 \n",
      "              Params: tensor([  5.3645, -17.2866])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4043, Loss: 2.9276740550994873 \n",
      "              Params: tensor([  5.3645, -17.2867])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4044, Loss: 2.927673578262329 \n",
      "              Params: tensor([  5.3645, -17.2867])\n",
      "              Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch: 4045, Loss: 2.9276747703552246 \n",
      "              Params: tensor([  5.3645, -17.2867])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4046, Loss: 2.9276742935180664 \n",
      "              Params: tensor([  5.3645, -17.2868])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4047, Loss: 2.927673578262329 \n",
      "              Params: tensor([  5.3645, -17.2868])\n",
      "              Grad: tensor([-0.0006,  0.0031])\n",
      "Epoch: 4048, Loss: 2.9276747703552246 \n",
      "              Params: tensor([  5.3645, -17.2868])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4049, Loss: 2.9276723861694336 \n",
      "              Params: tensor([  5.3646, -17.2868])\n",
      "              Grad: tensor([-0.0005,  0.0031])\n",
      "Epoch: 4050, Loss: 2.9276747703552246 \n",
      "              Params: tensor([  5.3646, -17.2869])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4051, Loss: 2.9276750087738037 \n",
      "              Params: tensor([  5.3646, -17.2869])\n",
      "              Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch: 4052, Loss: 2.927673101425171 \n",
      "              Params: tensor([  5.3646, -17.2869])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4053, Loss: 2.92767333984375 \n",
      "              Params: tensor([  5.3646, -17.2870])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4054, Loss: 2.9276742935180664 \n",
      "              Params: tensor([  5.3646, -17.2870])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4055, Loss: 2.927673101425171 \n",
      "              Params: tensor([  5.3646, -17.2870])\n",
      "              Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch: 4056, Loss: 2.927673578262329 \n",
      "              Params: tensor([  5.3646, -17.2871])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4057, Loss: 2.9276723861694336 \n",
      "              Params: tensor([  5.3646, -17.2871])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4058, Loss: 2.9276740550994873 \n",
      "              Params: tensor([  5.3646, -17.2871])\n",
      "              Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch: 4059, Loss: 2.9276750087738037 \n",
      "              Params: tensor([  5.3646, -17.2872])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4060, Loss: 2.9276723861694336 \n",
      "              Params: tensor([  5.3646, -17.2872])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4061, Loss: 2.927673101425171 \n",
      "              Params: tensor([  5.3646, -17.2872])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4062, Loss: 2.9276747703552246 \n",
      "              Params: tensor([  5.3646, -17.2872])\n",
      "              Grad: tensor([-0.0006,  0.0030])\n",
      "Epoch: 4063, Loss: 2.9276726245880127 \n",
      "              Params: tensor([  5.3646, -17.2873])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4064, Loss: 2.927673578262329 \n",
      "              Params: tensor([  5.3646, -17.2873])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4065, Loss: 2.9276716709136963 \n",
      "              Params: tensor([  5.3646, -17.2873])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4066, Loss: 2.9276726245880127 \n",
      "              Params: tensor([  5.3646, -17.2874])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4067, Loss: 2.927673101425171 \n",
      "              Params: tensor([  5.3646, -17.2874])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4068, Loss: 2.9276716709136963 \n",
      "              Params: tensor([  5.3647, -17.2874])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4069, Loss: 2.9276719093322754 \n",
      "              Params: tensor([  5.3647, -17.2875])\n",
      "              Grad: tensor([-0.0005,  0.0030])\n",
      "Epoch: 4070, Loss: 2.92767333984375 \n",
      "              Params: tensor([  5.3647, -17.2875])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4071, Loss: 2.927673101425171 \n",
      "              Params: tensor([  5.3647, -17.2875])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4072, Loss: 2.927670955657959 \n",
      "              Params: tensor([  5.3647, -17.2875])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4073, Loss: 2.9276726245880127 \n",
      "              Params: tensor([  5.3647, -17.2876])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4074, Loss: 2.9276723861694336 \n",
      "              Params: tensor([  5.3647, -17.2876])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4075, Loss: 2.9276719093322754 \n",
      "              Params: tensor([  5.3647, -17.2876])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4076, Loss: 2.9276719093322754 \n",
      "              Params: tensor([  5.3647, -17.2877])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4077, Loss: 2.927670478820801 \n",
      "              Params: tensor([  5.3647, -17.2877])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4078, Loss: 2.9276723861694336 \n",
      "              Params: tensor([  5.3647, -17.2877])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4079, Loss: 2.9276716709136963 \n",
      "              Params: tensor([  5.3647, -17.2877])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4080, Loss: 2.927671194076538 \n",
      "              Params: tensor([  5.3647, -17.2878])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4081, Loss: 2.9276702404022217 \n",
      "              Params: tensor([  5.3647, -17.2878])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4082, Loss: 2.92767333984375 \n",
      "              Params: tensor([  5.3647, -17.2878])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4083, Loss: 2.9276716709136963 \n",
      "              Params: tensor([  5.3647, -17.2879])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4084, Loss: 2.927670478820801 \n",
      "              Params: tensor([  5.3647, -17.2879])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4085, Loss: 2.9276702404022217 \n",
      "              Params: tensor([  5.3647, -17.2879])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4086, Loss: 2.9276702404022217 \n",
      "              Params: tensor([  5.3647, -17.2879])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4087, Loss: 2.9276723861694336 \n",
      "              Params: tensor([  5.3647, -17.2880])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4088, Loss: 2.9276702404022217 \n",
      "              Params: tensor([  5.3648, -17.2880])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4089, Loss: 2.9276702404022217 \n",
      "              Params: tensor([  5.3648, -17.2880])\n",
      "              Grad: tensor([-0.0005,  0.0029])\n",
      "Epoch: 4090, Loss: 2.9276702404022217 \n",
      "              Params: tensor([  5.3648, -17.2881])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4091, Loss: 2.9276702404022217 \n",
      "              Params: tensor([  5.3648, -17.2881])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4092, Loss: 2.927670478820801 \n",
      "              Params: tensor([  5.3648, -17.2881])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4093, Loss: 2.9276702404022217 \n",
      "              Params: tensor([  5.3648, -17.2881])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4094, Loss: 2.9276692867279053 \n",
      "              Params: tensor([  5.3648, -17.2882])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4095, Loss: 2.927670478820801 \n",
      "              Params: tensor([  5.3648, -17.2882])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4096, Loss: 2.9276695251464844 \n",
      "              Params: tensor([  5.3648, -17.2882])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4097, Loss: 2.927670955657959 \n",
      "              Params: tensor([  5.3648, -17.2883])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4098, Loss: 2.9276695251464844 \n",
      "              Params: tensor([  5.3648, -17.2883])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4099, Loss: 2.927671194076538 \n",
      "              Params: tensor([  5.3648, -17.2883])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4100, Loss: 2.9276695251464844 \n",
      "              Params: tensor([  5.3648, -17.2883])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4101, Loss: 2.9276695251464844 \n",
      "              Params: tensor([  5.3648, -17.2884])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4102, Loss: 2.927671194076538 \n",
      "              Params: tensor([  5.3648, -17.2884])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4103, Loss: 2.927668809890747 \n",
      "              Params: tensor([  5.3648, -17.2884])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4104, Loss: 2.9276697635650635 \n",
      "              Params: tensor([  5.3648, -17.2885])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4105, Loss: 2.9276702404022217 \n",
      "              Params: tensor([  5.3648, -17.2885])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4106, Loss: 2.927670478820801 \n",
      "              Params: tensor([  5.3648, -17.2885])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4107, Loss: 2.9276695251464844 \n",
      "              Params: tensor([  5.3648, -17.2885])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4108, Loss: 2.9276680946350098 \n",
      "              Params: tensor([  5.3649, -17.2886])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4109, Loss: 2.927668571472168 \n",
      "              Params: tensor([  5.3649, -17.2886])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4110, Loss: 2.927668571472168 \n",
      "              Params: tensor([  5.3649, -17.2886])\n",
      "              Grad: tensor([-0.0005,  0.0028])\n",
      "Epoch: 4111, Loss: 2.9276695251464844 \n",
      "              Params: tensor([  5.3649, -17.2886])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4112, Loss: 2.9276697635650635 \n",
      "              Params: tensor([  5.3649, -17.2887])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4113, Loss: 2.9276695251464844 \n",
      "              Params: tensor([  5.3649, -17.2887])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4114, Loss: 2.9276695251464844 \n",
      "              Params: tensor([  5.3649, -17.2887])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4115, Loss: 2.927668809890747 \n",
      "              Params: tensor([  5.3649, -17.2887])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4116, Loss: 2.927668809890747 \n",
      "              Params: tensor([  5.3649, -17.2888])\n",
      "              Grad: tensor([-0.0004,  0.0027])\n",
      "Epoch: 4117, Loss: 2.9276702404022217 \n",
      "              Params: tensor([  5.3649, -17.2888])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4118, Loss: 2.9276692867279053 \n",
      "              Params: tensor([  5.3649, -17.2888])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4119, Loss: 2.9276697635650635 \n",
      "              Params: tensor([  5.3649, -17.2889])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4120, Loss: 2.9276678562164307 \n",
      "              Params: tensor([  5.3649, -17.2889])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4121, Loss: 2.9276680946350098 \n",
      "              Params: tensor([  5.3649, -17.2889])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4122, Loss: 2.9276692867279053 \n",
      "              Params: tensor([  5.3649, -17.2889])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4123, Loss: 2.9276680946350098 \n",
      "              Params: tensor([  5.3649, -17.2890])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4124, Loss: 2.9276695251464844 \n",
      "              Params: tensor([  5.3649, -17.2890])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4125, Loss: 2.927666425704956 \n",
      "              Params: tensor([  5.3649, -17.2890])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4126, Loss: 2.927668571472168 \n",
      "              Params: tensor([  5.3649, -17.2890])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4127, Loss: 2.9276678562164307 \n",
      "              Params: tensor([  5.3649, -17.2891])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4128, Loss: 2.927668571472168 \n",
      "              Params: tensor([  5.3649, -17.2891])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4129, Loss: 2.927666664123535 \n",
      "              Params: tensor([  5.3650, -17.2891])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4130, Loss: 2.9276673793792725 \n",
      "              Params: tensor([  5.3650, -17.2892])\n",
      "              Grad: tensor([-0.0004,  0.0027])\n",
      "Epoch: 4131, Loss: 2.9276680946350098 \n",
      "              Params: tensor([  5.3650, -17.2892])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4132, Loss: 2.9276673793792725 \n",
      "              Params: tensor([  5.3650, -17.2892])\n",
      "              Grad: tensor([-0.0005,  0.0027])\n",
      "Epoch: 4133, Loss: 2.9276671409606934 \n",
      "              Params: tensor([  5.3650, -17.2892])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4134, Loss: 2.927666425704956 \n",
      "              Params: tensor([  5.3650, -17.2893])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4135, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3650, -17.2893])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4136, Loss: 2.927668809890747 \n",
      "              Params: tensor([  5.3650, -17.2893])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4137, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3650, -17.2893])\n",
      "              Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch: 4138, Loss: 2.9276678562164307 \n",
      "              Params: tensor([  5.3650, -17.2894])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4139, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3650, -17.2894])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4140, Loss: 2.9276671409606934 \n",
      "              Params: tensor([  5.3650, -17.2894])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4141, Loss: 2.9276673793792725 \n",
      "              Params: tensor([  5.3650, -17.2894])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4142, Loss: 2.927666425704956 \n",
      "              Params: tensor([  5.3650, -17.2895])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4143, Loss: 2.9276671409606934 \n",
      "              Params: tensor([  5.3650, -17.2895])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4144, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3650, -17.2895])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4145, Loss: 2.9276671409606934 \n",
      "              Params: tensor([  5.3650, -17.2896])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4146, Loss: 2.9276671409606934 \n",
      "              Params: tensor([  5.3650, -17.2896])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4147, Loss: 2.927666664123535 \n",
      "              Params: tensor([  5.3650, -17.2896])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4148, Loss: 2.9276671409606934 \n",
      "              Params: tensor([  5.3650, -17.2896])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4149, Loss: 2.9276654720306396 \n",
      "              Params: tensor([  5.3650, -17.2897])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4150, Loss: 2.927666425704956 \n",
      "              Params: tensor([  5.3651, -17.2897])\n",
      "              Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch: 4151, Loss: 2.927666425704956 \n",
      "              Params: tensor([  5.3651, -17.2897])\n",
      "              Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch: 4152, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3651, -17.2897])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4153, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3651, -17.2898])\n",
      "              Grad: tensor([-0.0005,  0.0026])\n",
      "Epoch: 4154, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3651, -17.2898])\n",
      "              Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch: 4155, Loss: 2.9276657104492188 \n",
      "              Params: tensor([  5.3651, -17.2898])\n",
      "              Grad: tensor([-0.0004,  0.0026])\n",
      "Epoch: 4156, Loss: 2.9276657104492188 \n",
      "              Params: tensor([  5.3651, -17.2898])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4157, Loss: 2.9276654720306396 \n",
      "              Params: tensor([  5.3651, -17.2899])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4158, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3651, -17.2899])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4159, Loss: 2.9276654720306396 \n",
      "              Params: tensor([  5.3651, -17.2899])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4160, Loss: 2.927663564682007 \n",
      "              Params: tensor([  5.3651, -17.2899])\n",
      "              Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch: 4161, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3651, -17.2900])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4162, Loss: 2.9276649951934814 \n",
      "              Params: tensor([  5.3651, -17.2900])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4163, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3651, -17.2900])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4164, Loss: 2.927663564682007 \n",
      "              Params: tensor([  5.3651, -17.2900])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4165, Loss: 2.9276649951934814 \n",
      "              Params: tensor([  5.3651, -17.2901])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4166, Loss: 2.9276654720306396 \n",
      "              Params: tensor([  5.3651, -17.2901])\n",
      "              Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch: 4167, Loss: 2.9276649951934814 \n",
      "              Params: tensor([  5.3651, -17.2901])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4168, Loss: 2.9276657104492188 \n",
      "              Params: tensor([  5.3651, -17.2901])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4169, Loss: 2.927664279937744 \n",
      "              Params: tensor([  5.3651, -17.2902])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4170, Loss: 2.9276654720306396 \n",
      "              Params: tensor([  5.3651, -17.2902])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4171, Loss: 2.927665948867798 \n",
      "              Params: tensor([  5.3651, -17.2902])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4172, Loss: 2.9276633262634277 \n",
      "              Params: tensor([  5.3651, -17.2902])\n",
      "              Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch: 4173, Loss: 2.927664041519165 \n",
      "              Params: tensor([  5.3652, -17.2903])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4174, Loss: 2.927664041519165 \n",
      "              Params: tensor([  5.3652, -17.2903])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4175, Loss: 2.9276649951934814 \n",
      "              Params: tensor([  5.3652, -17.2903])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4176, Loss: 2.9276628494262695 \n",
      "              Params: tensor([  5.3652, -17.2903])\n",
      "              Grad: tensor([-0.0004,  0.0025])\n",
      "Epoch: 4177, Loss: 2.927664041519165 \n",
      "              Params: tensor([  5.3652, -17.2903])\n",
      "              Grad: tensor([-0.0005,  0.0025])\n",
      "Epoch: 4178, Loss: 2.927664041519165 \n",
      "              Params: tensor([  5.3652, -17.2904])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4179, Loss: 2.9276633262634277 \n",
      "              Params: tensor([  5.3652, -17.2904])\n",
      "              Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch: 4180, Loss: 2.927663564682007 \n",
      "              Params: tensor([  5.3652, -17.2904])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4181, Loss: 2.927664041519165 \n",
      "              Params: tensor([  5.3652, -17.2904])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4182, Loss: 2.9276633262634277 \n",
      "              Params: tensor([  5.3652, -17.2905])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4183, Loss: 2.927664279937744 \n",
      "              Params: tensor([  5.3652, -17.2905])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4184, Loss: 2.927664041519165 \n",
      "              Params: tensor([  5.3652, -17.2905])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4185, Loss: 2.927661657333374 \n",
      "              Params: tensor([  5.3652, -17.2905])\n",
      "              Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch: 4186, Loss: 2.9276649951934814 \n",
      "              Params: tensor([  5.3652, -17.2906])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4187, Loss: 2.9276633262634277 \n",
      "              Params: tensor([  5.3652, -17.2906])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4188, Loss: 2.927661657333374 \n",
      "              Params: tensor([  5.3652, -17.2906])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4189, Loss: 2.9276633262634277 \n",
      "              Params: tensor([  5.3652, -17.2906])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4190, Loss: 2.927664279937744 \n",
      "              Params: tensor([  5.3652, -17.2907])\n",
      "              Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch: 4191, Loss: 2.927664041519165 \n",
      "              Params: tensor([  5.3652, -17.2907])\n",
      "              Grad: tensor([-0.0005,  0.0024])\n",
      "Epoch: 4192, Loss: 2.9276621341705322 \n",
      "              Params: tensor([  5.3652, -17.2907])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4193, Loss: 2.9276633262634277 \n",
      "              Params: tensor([  5.3652, -17.2907])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4194, Loss: 2.9276628494262695 \n",
      "              Params: tensor([  5.3652, -17.2908])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4195, Loss: 2.9276649951934814 \n",
      "              Params: tensor([  5.3652, -17.2908])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4196, Loss: 2.927664041519165 \n",
      "              Params: tensor([  5.3653, -17.2908])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4197, Loss: 2.9276628494262695 \n",
      "              Params: tensor([  5.3653, -17.2908])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4198, Loss: 2.9276621341705322 \n",
      "              Params: tensor([  5.3653, -17.2909])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4199, Loss: 2.927663564682007 \n",
      "              Params: tensor([  5.3653, -17.2909])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4200, Loss: 2.9276633262634277 \n",
      "              Params: tensor([  5.3653, -17.2909])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4201, Loss: 2.927661180496216 \n",
      "              Params: tensor([  5.3653, -17.2909])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4202, Loss: 2.9276621341705322 \n",
      "              Params: tensor([  5.3653, -17.2910])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4203, Loss: 2.927661895751953 \n",
      "              Params: tensor([  5.3653, -17.2910])\n",
      "              Grad: tensor([-0.0004,  0.0024])\n",
      "Epoch: 4204, Loss: 2.9276628494262695 \n",
      "              Params: tensor([  5.3653, -17.2910])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4205, Loss: 2.9276628494262695 \n",
      "              Params: tensor([  5.3653, -17.2910])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4206, Loss: 2.927661895751953 \n",
      "              Params: tensor([  5.3653, -17.2910])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4207, Loss: 2.9276621341705322 \n",
      "              Params: tensor([  5.3653, -17.2911])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4208, Loss: 2.9276628494262695 \n",
      "              Params: tensor([  5.3653, -17.2911])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4209, Loss: 2.927663564682007 \n",
      "              Params: tensor([  5.3653, -17.2911])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4210, Loss: 2.927661657333374 \n",
      "              Params: tensor([  5.3653, -17.2911])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4211, Loss: 2.9276604652404785 \n",
      "              Params: tensor([  5.3653, -17.2912])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4212, Loss: 2.927661657333374 \n",
      "              Params: tensor([  5.3653, -17.2912])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4213, Loss: 2.9276621341705322 \n",
      "              Params: tensor([  5.3653, -17.2912])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4214, Loss: 2.9276621341705322 \n",
      "              Params: tensor([  5.3653, -17.2912])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4215, Loss: 2.927661180496216 \n",
      "              Params: tensor([  5.3653, -17.2913])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4216, Loss: 2.9276604652404785 \n",
      "              Params: tensor([  5.3653, -17.2913])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4217, Loss: 2.927661657333374 \n",
      "              Params: tensor([  5.3653, -17.2913])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4218, Loss: 2.9276621341705322 \n",
      "              Params: tensor([  5.3653, -17.2913])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4219, Loss: 2.9276628494262695 \n",
      "              Params: tensor([  5.3653, -17.2913])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4220, Loss: 2.9276621341705322 \n",
      "              Params: tensor([  5.3653, -17.2914])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4221, Loss: 2.927661895751953 \n",
      "              Params: tensor([  5.3654, -17.2914])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4222, Loss: 2.9276621341705322 \n",
      "              Params: tensor([  5.3654, -17.2914])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4223, Loss: 2.9276626110076904 \n",
      "              Params: tensor([  5.3654, -17.2914])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4224, Loss: 2.9276604652404785 \n",
      "              Params: tensor([  5.3654, -17.2915])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4225, Loss: 2.927661895751953 \n",
      "              Params: tensor([  5.3654, -17.2915])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4226, Loss: 2.9276604652404785 \n",
      "              Params: tensor([  5.3654, -17.2915])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4227, Loss: 2.927661180496216 \n",
      "              Params: tensor([  5.3654, -17.2915])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4228, Loss: 2.9276609420776367 \n",
      "              Params: tensor([  5.3654, -17.2915])\n",
      "              Grad: tensor([-0.0004,  0.0023])\n",
      "Epoch: 4229, Loss: 2.9276604652404785 \n",
      "              Params: tensor([  5.3654, -17.2916])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4230, Loss: 2.927661895751953 \n",
      "              Params: tensor([  5.3654, -17.2916])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4231, Loss: 2.927661657333374 \n",
      "              Params: tensor([  5.3654, -17.2916])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4232, Loss: 2.9276602268218994 \n",
      "              Params: tensor([  5.3654, -17.2916])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4233, Loss: 2.927661657333374 \n",
      "              Params: tensor([  5.3654, -17.2917])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4234, Loss: 2.9276609420776367 \n",
      "              Params: tensor([  5.3654, -17.2917])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4235, Loss: 2.927661657333374 \n",
      "              Params: tensor([  5.3654, -17.2917])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4236, Loss: 2.927661180496216 \n",
      "              Params: tensor([  5.3654, -17.2917])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4237, Loss: 2.9276602268218994 \n",
      "              Params: tensor([  5.3654, -17.2918])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4238, Loss: 2.9276609420776367 \n",
      "              Params: tensor([  5.3654, -17.2918])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4239, Loss: 2.9276602268218994 \n",
      "              Params: tensor([  5.3654, -17.2918])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4240, Loss: 2.927661895751953 \n",
      "              Params: tensor([  5.3654, -17.2918])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4241, Loss: 2.9276604652404785 \n",
      "              Params: tensor([  5.3654, -17.2918])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4242, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3654, -17.2919])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4243, Loss: 2.9276609420776367 \n",
      "              Params: tensor([  5.3654, -17.2919])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4244, Loss: 2.927661180496216 \n",
      "              Params: tensor([  5.3654, -17.2919])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4245, Loss: 2.927661657333374 \n",
      "              Params: tensor([  5.3654, -17.2919])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4246, Loss: 2.927659511566162 \n",
      "              Params: tensor([  5.3655, -17.2920])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4247, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3655, -17.2920])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4248, Loss: 2.927659749984741 \n",
      "              Params: tensor([  5.3655, -17.2920])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4249, Loss: 2.9276604652404785 \n",
      "              Params: tensor([  5.3655, -17.2920])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4250, Loss: 2.927661895751953 \n",
      "              Params: tensor([  5.3655, -17.2920])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4251, Loss: 2.9276602268218994 \n",
      "              Params: tensor([  5.3655, -17.2921])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4252, Loss: 2.9276602268218994 \n",
      "              Params: tensor([  5.3655, -17.2921])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4253, Loss: 2.9276602268218994 \n",
      "              Params: tensor([  5.3655, -17.2921])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4254, Loss: 2.9276604652404785 \n",
      "              Params: tensor([  5.3655, -17.2921])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4255, Loss: 2.927659749984741 \n",
      "              Params: tensor([  5.3655, -17.2921])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4256, Loss: 2.9276609420776367 \n",
      "              Params: tensor([  5.3655, -17.2922])\n",
      "              Grad: tensor([-0.0004,  0.0022])\n",
      "Epoch: 4257, Loss: 2.927658796310425 \n",
      "              Params: tensor([  5.3655, -17.2922])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4258, Loss: 2.927659511566162 \n",
      "              Params: tensor([  5.3655, -17.2922])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4259, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3655, -17.2922])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4260, Loss: 2.927658796310425 \n",
      "              Params: tensor([  5.3655, -17.2922])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4261, Loss: 2.927661657333374 \n",
      "              Params: tensor([  5.3655, -17.2923])\n",
      "              Grad: tensor([-0.0003,  0.0021])\n",
      "Epoch: 4262, Loss: 2.9276583194732666 \n",
      "              Params: tensor([  5.3655, -17.2923])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4263, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3655, -17.2923])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4264, Loss: 2.9276602268218994 \n",
      "              Params: tensor([  5.3655, -17.2923])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4265, Loss: 2.927658796310425 \n",
      "              Params: tensor([  5.3655, -17.2924])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4266, Loss: 2.9276604652404785 \n",
      "              Params: tensor([  5.3655, -17.2924])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4267, Loss: 2.927659511566162 \n",
      "              Params: tensor([  5.3655, -17.2924])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4268, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3655, -17.2924])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4269, Loss: 2.927659511566162 \n",
      "              Params: tensor([  5.3655, -17.2924])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4270, Loss: 2.9276602268218994 \n",
      "              Params: tensor([  5.3655, -17.2925])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4271, Loss: 2.927659749984741 \n",
      "              Params: tensor([  5.3655, -17.2925])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4272, Loss: 2.927659511566162 \n",
      "              Params: tensor([  5.3655, -17.2925])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4273, Loss: 2.9276578426361084 \n",
      "              Params: tensor([  5.3656, -17.2925])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4274, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3656, -17.2925])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4275, Loss: 2.927659511566162 \n",
      "              Params: tensor([  5.3656, -17.2926])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4276, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3656, -17.2926])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4277, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3656, -17.2926])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4278, Loss: 2.9276583194732666 \n",
      "              Params: tensor([  5.3656, -17.2926])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4279, Loss: 2.9276578426361084 \n",
      "              Params: tensor([  5.3656, -17.2926])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4280, Loss: 2.927658796310425 \n",
      "              Params: tensor([  5.3656, -17.2927])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4281, Loss: 2.927658796310425 \n",
      "              Params: tensor([  5.3656, -17.2927])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4282, Loss: 2.9276602268218994 \n",
      "              Params: tensor([  5.3656, -17.2927])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4283, Loss: 2.927659749984741 \n",
      "              Params: tensor([  5.3656, -17.2927])\n",
      "              Grad: tensor([-0.0004,  0.0021])\n",
      "Epoch: 4284, Loss: 2.9276578426361084 \n",
      "              Params: tensor([  5.3656, -17.2927])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4285, Loss: 2.927658796310425 \n",
      "              Params: tensor([  5.3656, -17.2928])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4286, Loss: 2.9276580810546875 \n",
      "              Params: tensor([  5.3656, -17.2928])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4287, Loss: 2.927658796310425 \n",
      "              Params: tensor([  5.3656, -17.2928])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4288, Loss: 2.9276583194732666 \n",
      "              Params: tensor([  5.3656, -17.2928])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4289, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3656, -17.2929])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4290, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3656, -17.2929])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4291, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3656, -17.2929])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4292, Loss: 2.927658796310425 \n",
      "              Params: tensor([  5.3656, -17.2929])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4293, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3656, -17.2929])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4294, Loss: 2.927658796310425 \n",
      "              Params: tensor([  5.3656, -17.2930])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4295, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3656, -17.2930])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4296, Loss: 2.927657127380371 \n",
      "              Params: tensor([  5.3656, -17.2930])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4297, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3656, -17.2930])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4298, Loss: 2.9276580810546875 \n",
      "              Params: tensor([  5.3656, -17.2930])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4299, Loss: 2.927659034729004 \n",
      "              Params: tensor([  5.3656, -17.2931])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4300, Loss: 2.927659511566162 \n",
      "              Params: tensor([  5.3657, -17.2931])\n",
      "              Grad: tensor([-0.0004,  0.0020])\n",
      "Epoch: 4301, Loss: 2.927657127380371 \n",
      "              Params: tensor([  5.3657, -17.2931])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4302, Loss: 2.9276578426361084 \n",
      "              Params: tensor([  5.3657, -17.2931])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4303, Loss: 2.9276580810546875 \n",
      "              Params: tensor([  5.3657, -17.2931])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4304, Loss: 2.9276583194732666 \n",
      "              Params: tensor([  5.3657, -17.2932])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4305, Loss: 2.9276578426361084 \n",
      "              Params: tensor([  5.3657, -17.2932])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4306, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3657, -17.2932])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4307, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3657, -17.2932])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4308, Loss: 2.9276580810546875 \n",
      "              Params: tensor([  5.3657, -17.2932])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4309, Loss: 2.927657127380371 \n",
      "              Params: tensor([  5.3657, -17.2932])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4310, Loss: 2.927659511566162 \n",
      "              Params: tensor([  5.3657, -17.2933])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4311, Loss: 2.927658796310425 \n",
      "              Params: tensor([  5.3657, -17.2933])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4312, Loss: 2.9276580810546875 \n",
      "              Params: tensor([  5.3657, -17.2933])\n",
      "              Grad: tensor([-0.0003,  0.0020])\n",
      "Epoch: 4313, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3657, -17.2933])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4314, Loss: 2.9276580810546875 \n",
      "              Params: tensor([  5.3657, -17.2933])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4315, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3657, -17.2934])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4316, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3657, -17.2934])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4317, Loss: 2.9276583194732666 \n",
      "              Params: tensor([  5.3657, -17.2934])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4318, Loss: 2.9276578426361084 \n",
      "              Params: tensor([  5.3657, -17.2934])\n",
      "              Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch: 4319, Loss: 2.927656650543213 \n",
      "              Params: tensor([  5.3657, -17.2934])\n",
      "              Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch: 4320, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3657, -17.2935])\n",
      "              Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch: 4321, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3657, -17.2935])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4322, Loss: 2.927657127380371 \n",
      "              Params: tensor([  5.3657, -17.2935])\n",
      "              Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch: 4323, Loss: 2.9276578426361084 \n",
      "              Params: tensor([  5.3657, -17.2935])\n",
      "              Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch: 4324, Loss: 2.9276583194732666 \n",
      "              Params: tensor([  5.3657, -17.2935])\n",
      "              Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch: 4325, Loss: 2.927656650543213 \n",
      "              Params: tensor([  5.3657, -17.2936])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4326, Loss: 2.927656650543213 \n",
      "              Params: tensor([  5.3657, -17.2936])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4327, Loss: 2.927656650543213 \n",
      "              Params: tensor([  5.3657, -17.2936])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4328, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3657, -17.2936])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4329, Loss: 2.927656650543213 \n",
      "              Params: tensor([  5.3657, -17.2936])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4330, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3658, -17.2936])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4331, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3658, -17.2937])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4332, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3658, -17.2937])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4333, Loss: 2.927656412124634 \n",
      "              Params: tensor([  5.3658, -17.2937])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4334, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3658, -17.2937])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4335, Loss: 2.927657127380371 \n",
      "              Params: tensor([  5.3658, -17.2937])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4336, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3658, -17.2938])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4337, Loss: 2.927656412124634 \n",
      "              Params: tensor([  5.3658, -17.2938])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4338, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3658, -17.2938])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4339, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3658, -17.2938])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4340, Loss: 2.927656412124634 \n",
      "              Params: tensor([  5.3658, -17.2938])\n",
      "              Grad: tensor([-0.0003,  0.0019])\n",
      "Epoch: 4341, Loss: 2.927657127380371 \n",
      "              Params: tensor([  5.3658, -17.2939])\n",
      "              Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch: 4342, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3658, -17.2939])\n",
      "              Grad: tensor([-0.0004,  0.0019])\n",
      "Epoch: 4343, Loss: 2.927656412124634 \n",
      "              Params: tensor([  5.3658, -17.2939])\n",
      "              Grad: tensor([-0.0004,  0.0018])\n",
      "Epoch: 4344, Loss: 2.927656650543213 \n",
      "              Params: tensor([  5.3658, -17.2939])\n",
      "              Grad: tensor([-0.0004,  0.0018])\n",
      "Epoch: 4345, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3658, -17.2939])\n",
      "              Grad: tensor([-0.0004,  0.0018])\n",
      "Epoch: 4346, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3658, -17.2940])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4347, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3658, -17.2940])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4348, Loss: 2.927656650543213 \n",
      "              Params: tensor([  5.3658, -17.2940])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4349, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3658, -17.2940])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4350, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3658, -17.2940])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4351, Loss: 2.92765736579895 \n",
      "              Params: tensor([  5.3658, -17.2941])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4352, Loss: 2.927656650543213 \n",
      "              Params: tensor([  5.3658, -17.2941])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4353, Loss: 2.9276552200317383 \n",
      "              Params: tensor([  5.3658, -17.2941])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4354, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3658, -17.2941])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4355, Loss: 2.927656412124634 \n",
      "              Params: tensor([  5.3658, -17.2941])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4356, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3658, -17.2941])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4357, Loss: 2.927656650543213 \n",
      "              Params: tensor([  5.3658, -17.2942])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4358, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3658, -17.2942])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4359, Loss: 2.927656412124634 \n",
      "              Params: tensor([  5.3658, -17.2942])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4360, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3659, -17.2942])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4361, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3659, -17.2942])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4362, Loss: 2.927657127380371 \n",
      "              Params: tensor([  5.3659, -17.2942])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4363, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3659, -17.2943])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4364, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3659, -17.2943])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4365, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3659, -17.2943])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4366, Loss: 2.927656412124634 \n",
      "              Params: tensor([  5.3659, -17.2943])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4367, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3659, -17.2943])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4368, Loss: 2.927654981613159 \n",
      "              Params: tensor([  5.3659, -17.2943])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4369, Loss: 2.927656412124634 \n",
      "              Params: tensor([  5.3659, -17.2944])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4370, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3659, -17.2944])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4371, Loss: 2.927656650543213 \n",
      "              Params: tensor([  5.3659, -17.2944])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4372, Loss: 2.9276552200317383 \n",
      "              Params: tensor([  5.3659, -17.2944])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4373, Loss: 2.927657127380371 \n",
      "              Params: tensor([  5.3659, -17.2944])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4374, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3659, -17.2945])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4375, Loss: 2.9276552200317383 \n",
      "              Params: tensor([  5.3659, -17.2945])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4376, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3659, -17.2945])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4377, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3659, -17.2945])\n",
      "              Grad: tensor([-0.0003,  0.0018])\n",
      "Epoch: 4378, Loss: 2.9276552200317383 \n",
      "              Params: tensor([  5.3659, -17.2945])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4379, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3659, -17.2945])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4380, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3659, -17.2946])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4381, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3659, -17.2946])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4382, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3659, -17.2946])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4383, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3659, -17.2946])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4384, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3659, -17.2946])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4385, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3659, -17.2946])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4386, Loss: 2.9276552200317383 \n",
      "              Params: tensor([  5.3659, -17.2947])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4387, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3659, -17.2947])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4388, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3659, -17.2947])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4389, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3659, -17.2947])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4390, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3659, -17.2947])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4391, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3659, -17.2947])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4392, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3659, -17.2948])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4393, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3660, -17.2948])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4394, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3660, -17.2948])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4395, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3660, -17.2948])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4396, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3660, -17.2948])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4397, Loss: 2.9276552200317383 \n",
      "              Params: tensor([  5.3660, -17.2948])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4398, Loss: 2.9276559352874756 \n",
      "              Params: tensor([  5.3660, -17.2949])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4399, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3660, -17.2949])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4400, Loss: 2.927654981613159 \n",
      "              Params: tensor([  5.3660, -17.2949])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4401, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3660, -17.2949])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4402, Loss: 2.927654981613159 \n",
      "              Params: tensor([  5.3660, -17.2949])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4403, Loss: 2.9276552200317383 \n",
      "              Params: tensor([  5.3660, -17.2949])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4404, Loss: 2.9276556968688965 \n",
      "              Params: tensor([  5.3660, -17.2950])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4405, Loss: 2.927654981613159 \n",
      "              Params: tensor([  5.3660, -17.2950])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4406, Loss: 2.927654981613159 \n",
      "              Params: tensor([  5.3660, -17.2950])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4407, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3660, -17.2950])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4408, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3660, -17.2950])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4409, Loss: 2.927654981613159 \n",
      "              Params: tensor([  5.3660, -17.2951])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4410, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3660, -17.2951])\n",
      "              Grad: tensor([-0.0003,  0.0017])\n",
      "Epoch: 4411, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3660, -17.2951])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4412, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3660, -17.2951])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4413, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3660, -17.2951])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4414, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3660, -17.2951])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4415, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3660, -17.2952])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4416, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3660, -17.2952])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4417, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3660, -17.2952])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4418, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3660, -17.2952])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4419, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3660, -17.2952])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4420, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3660, -17.2952])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4421, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3660, -17.2953])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4422, Loss: 2.9276552200317383 \n",
      "              Params: tensor([  5.3660, -17.2953])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4423, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3660, -17.2953])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4424, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3660, -17.2953])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4425, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3660, -17.2953])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4426, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3660, -17.2953])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4427, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3661, -17.2953])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4428, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3661, -17.2954])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4429, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3661, -17.2954])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4430, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3661, -17.2954])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4431, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3661, -17.2954])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4432, Loss: 2.9276552200317383 \n",
      "              Params: tensor([  5.3661, -17.2954])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4433, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3661, -17.2954])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4434, Loss: 2.9276552200317383 \n",
      "              Params: tensor([  5.3661, -17.2955])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4435, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3661, -17.2955])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4436, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3661, -17.2955])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4437, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3661, -17.2955])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4438, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3661, -17.2955])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4439, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3661, -17.2955])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4440, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3661, -17.2955])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4441, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3661, -17.2956])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4442, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3661, -17.2956])\n",
      "              Grad: tensor([-0.0002,  0.0016])\n",
      "Epoch: 4443, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3661, -17.2956])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4444, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3661, -17.2956])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4445, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3661, -17.2956])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4446, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3661, -17.2956])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4447, Loss: 2.927654266357422 \n",
      "              Params: tensor([  5.3661, -17.2957])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4448, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3661, -17.2957])\n",
      "              Grad: tensor([-0.0003,  0.0016])\n",
      "Epoch: 4449, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3661, -17.2957])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4450, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3661, -17.2957])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4451, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3661, -17.2957])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4452, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3661, -17.2957])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4453, Loss: 2.9276540279388428 \n",
      "              Params: tensor([  5.3661, -17.2957])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4454, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3661, -17.2958])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4455, Loss: 2.927654504776001 \n",
      "              Params: tensor([  5.3661, -17.2958])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4456, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3661, -17.2958])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4457, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3661, -17.2958])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4458, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3661, -17.2958])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4459, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3661, -17.2958])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4460, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3661, -17.2959])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4461, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3661, -17.2959])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4462, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3661, -17.2959])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4463, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3661, -17.2959])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4464, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3662, -17.2959])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4465, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3662, -17.2959])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4466, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3662, -17.2959])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4467, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3662, -17.2960])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4468, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3662, -17.2960])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4469, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3662, -17.2960])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4470, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3662, -17.2960])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4471, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3662, -17.2960])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4472, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3662, -17.2960])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4473, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3662, -17.2960])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4474, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3662, -17.2961])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4475, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3662, -17.2961])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4476, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3662, -17.2961])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4477, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3662, -17.2961])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4478, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3662, -17.2961])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4479, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3662, -17.2961])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4480, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3662, -17.2962])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4481, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3662, -17.2962])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4482, Loss: 2.9276535511016846 \n",
      "              Params: tensor([  5.3662, -17.2962])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4483, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3662, -17.2962])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4484, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3662, -17.2962])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4485, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3662, -17.2962])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4486, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3662, -17.2962])\n",
      "              Grad: tensor([-0.0003,  0.0015])\n",
      "Epoch: 4487, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3662, -17.2963])\n",
      "              Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 4488, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3662, -17.2963])\n",
      "              Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 4489, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3662, -17.2963])\n",
      "              Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 4490, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3662, -17.2963])\n",
      "              Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 4491, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3662, -17.2963])\n",
      "              Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 4492, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3662, -17.2963])\n",
      "              Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 4493, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3662, -17.2964])\n",
      "              Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 4494, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3662, -17.2964])\n",
      "              Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 4495, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3662, -17.2964])\n",
      "              Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 4496, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3662, -17.2964])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4497, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3662, -17.2964])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4498, Loss: 2.9276533126831055 \n",
      "              Params: tensor([  5.3662, -17.2964])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4499, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3662, -17.2964])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4500, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3662, -17.2964])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4501, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3662, -17.2965])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4502, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3663, -17.2965])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4503, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3663, -17.2965])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4504, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3663, -17.2965])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4505, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3663, -17.2965])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4506, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3663, -17.2965])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4507, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3663, -17.2965])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4508, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3663, -17.2966])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4509, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3663, -17.2966])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4510, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3663, -17.2966])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4511, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3663, -17.2966])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4512, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3663, -17.2966])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4513, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3663, -17.2966])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4514, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3663, -17.2966])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4515, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3663, -17.2966])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4516, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3663, -17.2967])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4517, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3663, -17.2967])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4518, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3663, -17.2967])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4519, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3663, -17.2967])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4520, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3663, -17.2967])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4521, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3663, -17.2967])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4522, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3663, -17.2967])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4523, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3663, -17.2968])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4524, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3663, -17.2968])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4525, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3663, -17.2968])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4526, Loss: 2.9276528358459473 \n",
      "              Params: tensor([  5.3663, -17.2968])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4527, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3663, -17.2968])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4528, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3663, -17.2968])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4529, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3663, -17.2968])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4530, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3663, -17.2969])\n",
      "              Grad: tensor([-0.0002,  0.0014])\n",
      "Epoch: 4531, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3663, -17.2969])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4532, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3663, -17.2969])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4533, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3663, -17.2969])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4534, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3663, -17.2969])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4535, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3663, -17.2969])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4536, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3663, -17.2969])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4537, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3663, -17.2969])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4538, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3663, -17.2970])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4539, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3663, -17.2970])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4540, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3663, -17.2970])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4541, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3663, -17.2970])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4542, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3663, -17.2970])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4543, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3663, -17.2970])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4544, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3664, -17.2970])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4545, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3664, -17.2971])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4546, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3664, -17.2971])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4547, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3664, -17.2971])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4548, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3664, -17.2971])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4549, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3664, -17.2971])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4550, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3664, -17.2971])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4551, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3664, -17.2971])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4552, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3664, -17.2971])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4553, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3664, -17.2972])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4554, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3664, -17.2972])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4555, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3664, -17.2972])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4556, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3664, -17.2972])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4557, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3664, -17.2972])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4558, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3664, -17.2972])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4559, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3664, -17.2972])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4560, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3664, -17.2973])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4561, Loss: 2.92765212059021 \n",
      "              Params: tensor([  5.3664, -17.2973])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4562, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3664, -17.2973])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4563, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3664, -17.2973])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4564, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3664, -17.2973])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4565, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3664, -17.2973])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4566, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3664, -17.2973])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4567, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3664, -17.2973])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4568, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3664, -17.2974])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4569, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3664, -17.2974])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4570, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3664, -17.2974])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4571, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3664, -17.2974])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4572, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3664, -17.2974])\n",
      "              Grad: tensor([-0.0002,  0.0013])\n",
      "Epoch: 4573, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3664, -17.2974])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4574, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3664, -17.2974])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4575, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3664, -17.2975])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4576, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3664, -17.2975])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4577, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3664, -17.2975])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4578, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3664, -17.2975])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4579, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3664, -17.2975])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4580, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3664, -17.2975])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4581, Loss: 2.927652597427368 \n",
      "              Params: tensor([  5.3664, -17.2975])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4582, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3664, -17.2975])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4583, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3664, -17.2976])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4584, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3664, -17.2976])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4585, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3664, -17.2976])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4586, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3664, -17.2976])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4587, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3664, -17.2976])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4588, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3665, -17.2976])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4589, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3665, -17.2976])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4590, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3665, -17.2976])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4591, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3665, -17.2976])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4592, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3665, -17.2977])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4593, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3665, -17.2977])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4594, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3665, -17.2977])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4595, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3665, -17.2977])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4596, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3665, -17.2977])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4597, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3665, -17.2977])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4598, Loss: 2.927651882171631 \n",
      "              Params: tensor([  5.3665, -17.2977])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4599, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3665, -17.2977])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4600, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3665, -17.2977])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4601, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3665, -17.2978])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4602, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3665, -17.2978])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4603, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3665, -17.2978])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4604, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3665, -17.2978])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4605, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3665, -17.2978])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4606, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3665, -17.2978])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4607, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3665, -17.2978])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4608, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3665, -17.2978])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4609, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3665, -17.2978])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4610, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3665, -17.2979])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4611, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3665, -17.2979])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4612, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3665, -17.2979])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4613, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3665, -17.2979])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4614, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3665, -17.2979])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4615, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3665, -17.2979])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4616, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3665, -17.2979])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4617, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3665, -17.2979])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4618, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3665, -17.2980])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4619, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3665, -17.2980])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4620, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3665, -17.2980])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4621, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3665, -17.2980])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4622, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3665, -17.2980])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4623, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3665, -17.2980])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4624, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3665, -17.2980])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4625, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3665, -17.2980])\n",
      "              Grad: tensor([-0.0002,  0.0012])\n",
      "Epoch: 4626, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3665, -17.2980])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4627, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3665, -17.2981])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4628, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3665, -17.2981])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4629, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3665, -17.2981])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4630, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3665, -17.2981])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4631, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3665, -17.2981])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4632, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3665, -17.2981])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4633, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3665, -17.2981])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4634, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3665, -17.2981])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4635, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3665, -17.2981])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4636, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3665, -17.2982])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4637, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3665, -17.2982])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4638, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2982])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4639, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3666, -17.2982])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4640, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2982])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4641, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3666, -17.2982])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4642, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2982])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4643, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2982])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4644, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3666, -17.2982])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4645, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3666, -17.2983])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4646, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2983])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4647, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3666, -17.2983])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4648, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3666, -17.2983])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4649, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2983])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4650, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3666, -17.2983])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4651, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2983])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4652, Loss: 2.9276506900787354 \n",
      "              Params: tensor([  5.3666, -17.2983])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4653, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3666, -17.2984])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4654, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3666, -17.2984])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4655, Loss: 2.9276511669158936 \n",
      "              Params: tensor([  5.3666, -17.2984])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4656, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3666, -17.2984])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4657, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3666, -17.2984])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4658, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2984])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4659, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3666, -17.2984])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4660, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3666, -17.2984])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4661, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3666, -17.2984])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4662, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3666, -17.2985])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4663, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3666, -17.2985])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4664, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3666, -17.2985])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4665, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3666, -17.2985])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4666, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3666, -17.2985])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4667, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3666, -17.2985])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4668, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3666, -17.2985])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4669, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2985])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4670, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3666, -17.2985])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4671, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3666, -17.2986])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4672, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3666, -17.2986])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4673, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3666, -17.2986])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4674, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2986])\n",
      "              Grad: tensor([-0.0002,  0.0011])\n",
      "Epoch: 4675, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3666, -17.2986])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4676, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3666, -17.2986])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4677, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2986])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4678, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3666, -17.2986])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4679, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3666, -17.2986])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4680, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3666, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4681, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3666, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4682, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3666, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4683, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3666, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4684, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3666, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4685, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3666, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4686, Loss: 2.9276514053344727 \n",
      "              Params: tensor([  5.3666, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4687, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3666, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4688, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3666, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4689, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4690, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3667, -17.2987])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4691, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2988])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4692, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2988])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4693, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3667, -17.2988])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4694, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3667, -17.2988])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4695, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3667, -17.2988])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4696, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2988])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4697, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3667, -17.2988])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4698, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3667, -17.2988])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4699, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3667, -17.2988])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4700, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2988])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4701, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4702, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4703, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4704, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4705, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4706, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4707, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4708, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4709, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4710, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4711, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3667, -17.2989])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4712, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2990])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4713, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2990])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4714, Loss: 2.9276504516601562 \n",
      "              Params: tensor([  5.3667, -17.2990])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4715, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3667, -17.2990])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4716, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2990])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4717, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3667, -17.2990])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4718, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3667, -17.2990])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4719, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2990])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4720, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3667, -17.2990])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4721, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2990])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4722, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4723, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4724, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4725, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4726, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4727, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4728, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4729, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4730, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4731, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4732, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2991])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4733, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2992])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4734, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2992])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4735, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3667, -17.2992])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4736, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2992])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4737, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3667, -17.2992])\n",
      "              Grad: tensor([-0.0002,  0.0010])\n",
      "Epoch: 4738, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3667, -17.2992])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4739, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2992])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4740, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3667, -17.2992])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4741, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3667, -17.2992])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4742, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2992])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4743, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3667, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4744, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3667, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4745, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3667, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4746, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3667, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4747, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4748, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3667, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4749, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3668, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4750, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3668, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4751, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4752, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3668, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4753, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3668, -17.2993])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4754, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3668, -17.2994])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4755, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3668, -17.2994])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4756, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3668, -17.2994])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4757, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3668, -17.2994])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4758, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3668, -17.2994])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4759, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3668, -17.2994])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4760, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3668, -17.2994])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4761, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2994])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4762, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3668, -17.2994])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4763, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3668, -17.2994])\n",
      "              Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch: 4764, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4765, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4766, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4767, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4768, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4769, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4770, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4771, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4772, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4773, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4774, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3668, -17.2995])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4775, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3668, -17.2996])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4776, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3668, -17.2996])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4777, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3668, -17.2996])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4778, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3668, -17.2996])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4779, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3668, -17.2996])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4780, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3668, -17.2996])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4781, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2996])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4782, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3668, -17.2996])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4783, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3668, -17.2996])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4784, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2996])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4785, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4786, Loss: 2.927650213241577 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4787, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4788, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4789, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4790, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4791, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4792, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4793, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch: 4794, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4795, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4796, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2997])\n",
      "              Grad: tensor([-0.0002,  0.0009])\n",
      "Epoch: 4797, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch: 4798, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch: 4799, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch: 4800, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch: 4801, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch: 4802, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0009])\n",
      "Epoch: 4803, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4804, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4805, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4806, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4807, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4808, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4809, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3668, -17.2998])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4810, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3668, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4811, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3668, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4812, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4813, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4814, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4815, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4816, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4817, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4818, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4819, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4820, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4821, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4822, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.2999])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4823, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4824, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4825, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4826, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4827, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4828, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4829, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4830, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4831, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4832, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4833, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4834, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4835, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.3000])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4836, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4837, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4838, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4839, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4840, Loss: 2.927649736404419 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4841, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4842, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4843, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4844, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4845, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4846, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4847, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4848, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3669, -17.3001])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4849, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4850, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4851, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4852, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4853, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4854, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4855, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4856, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4857, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4858, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4859, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4860, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4861, Loss: 2.927645206451416 \n",
      "              Params: tensor([  5.3669, -17.3002])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4862, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4863, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4864, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4865, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4866, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4867, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4868, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4869, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4870, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4871, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4872, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4873, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4874, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.3003])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4875, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4876, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4877, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4878, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4879, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4880, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0008])\n",
      "Epoch: 4881, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4882, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4883, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4884, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4885, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3669, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4886, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3670, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4887, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3670, -17.3004])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4888, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4889, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4890, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4891, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4892, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4893, Loss: 2.9276459217071533 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4894, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4895, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4896, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4897, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4898, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4899, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4900, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3670, -17.3005])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4901, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4902, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4903, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4904, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4905, Loss: 2.9276459217071533 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4906, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4907, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4908, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4909, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4910, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4911, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4912, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4913, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4914, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3670, -17.3006])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4915, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4916, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4917, Loss: 2.9276490211486816 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4918, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4919, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4920, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4921, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4922, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4923, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4924, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4925, Loss: 2.9276459217071533 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4926, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4927, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3007])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4928, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4929, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4930, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4931, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4932, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4933, Loss: 2.92764949798584 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4934, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4935, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4936, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4937, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4938, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4939, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4940, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3670, -17.3008])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4941, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4942, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4943, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4944, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4945, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4946, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4947, Loss: 2.9276487827301025 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4948, Loss: 2.9276459217071533 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4949, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-9.9361e-05,  6.6355e-04])\n",
      "Epoch: 4950, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-9.5367e-05,  6.6292e-04])\n",
      "Epoch: 4951, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-9.6858e-05,  6.6188e-04])\n",
      "Epoch: 4952, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4953, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4954, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4955, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4956, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4957, Loss: 2.927645683288574 \n",
      "              Params: tensor([  5.3670, -17.3009])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4958, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3670, -17.3010])\n",
      "              Grad: tensor([-9.8228e-05,  6.5479e-04])\n",
      "Epoch: 4959, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3670, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4960, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3670, -17.3010])\n",
      "              Grad: tensor([-9.8288e-05,  6.5270e-04])\n",
      "Epoch: 4961, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3670, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0007])\n",
      "Epoch: 4962, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3670, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4963, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3670, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4964, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3670, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4965, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3670, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4966, Loss: 2.927645683288574 \n",
      "              Params: tensor([  5.3671, -17.3010])\n",
      "              Grad: tensor([-9.7990e-05,  6.4683e-04])\n",
      "Epoch: 4967, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3671, -17.3010])\n",
      "              Grad: tensor([-9.7573e-05,  6.4588e-04])\n",
      "Epoch: 4968, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3671, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4969, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3671, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4970, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3671, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4971, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3671, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4972, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3671, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4973, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3671, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4974, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3671, -17.3010])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4975, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4976, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-9.5606e-05,  6.3694e-04])\n",
      "Epoch: 4977, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-9.8169e-05,  6.3545e-04])\n",
      "Epoch: 4978, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4979, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4980, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4981, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4982, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4983, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4984, Loss: 2.927645683288574 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4985, Loss: 2.9276480674743652 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-9.6440e-05,  6.2808e-04])\n",
      "Epoch: 4986, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4987, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4988, Loss: 2.9276466369628906 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4989, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4990, Loss: 2.9276463985443115 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4991, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4992, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3671, -17.3011])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4993, Loss: 2.927645683288574 \n",
      "              Params: tensor([  5.3671, -17.3012])\n",
      "              Grad: tensor([-9.2626e-05,  6.2042e-04])\n",
      "Epoch: 4994, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3671, -17.3012])\n",
      "              Grad: tensor([-9.8884e-05,  6.1837e-04])\n",
      "Epoch: 4995, Loss: 2.9276483058929443 \n",
      "              Params: tensor([  5.3671, -17.3012])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4996, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3671, -17.3012])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4997, Loss: 2.9276468753814697 \n",
      "              Params: tensor([  5.3671, -17.3012])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4998, Loss: 2.927647352218628 \n",
      "              Params: tensor([  5.3671, -17.3012])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n",
      "Epoch: 4999, Loss: 2.927647590637207 \n",
      "              Params: tensor([  5.3671, -17.3012])\n",
      "              Grad: tensor([-0.0001,  0.0006])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "learning_rate = 1e-2\n",
    "params = torch.tensor([1.0, 0.0]) #Initial guesses of w and b\n",
    "t_u_norm = t_u * 0.1\n",
    "\n",
    "#Run the loop\n",
    "params, final_loss = training_loop(n_epochs, learning_rate, params, t_u_norm, t_c)\n",
    "w = params[0]\n",
    "b = params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(t_u, t_c, label = 'data', color='r')\n",
    "plt.plot(t_u, model(t_u_norm, w, b), label = f'Model. w: {w:.2f}, b: {b:.2f}')\n",
    "plt.ylabel('Temperature / Celsius')\n",
    "plt.xlabel('Temperature / Unknown Units')\n",
    "plt.title(f'Linear Fit Using Parameters from Gradient Descent\\n Loss: {final_loss:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PyTorch Autograd</h1>\n",
    "Saves us the trouble of computing the gradient manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for _ in range(n_epochs):\n",
    "        if params.grad is not None: #Make sure to zero the gradients to prevent accumulation\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "\n",
    "        if (_ + 1) % 500 == 0:\n",
    "            print(f'Epoch: {_+1}, Loss: {loss} \\n\\\n",
    "                Params: {params}\\n\\\n",
    "                Grad: {params.grad}')\n",
    "        \n",
    "    return params, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, Loss: 7.860115051269531 \n",
      "                Params: tensor([ 4.0443, -9.8133], requires_grad=True)\n",
      "                Grad: tensor([-0.2252,  1.2748])\n",
      "Epoch: 1000, Loss: 3.828537940979004 \n",
      "                Params: tensor([  4.8021, -14.1031], requires_grad=True)\n",
      "                Grad: tensor([-0.0962,  0.5448])\n",
      "Epoch: 1500, Loss: 3.092191219329834 \n",
      "                Params: tensor([  5.1260, -15.9365], requires_grad=True)\n",
      "                Grad: tensor([-0.0411,  0.2328])\n",
      "Epoch: 2000, Loss: 2.957697868347168 \n",
      "                Params: tensor([  5.2644, -16.7200], requires_grad=True)\n",
      "                Grad: tensor([-0.0176,  0.0995])\n",
      "Epoch: 2500, Loss: 2.933133840560913 \n",
      "                Params: tensor([  5.3236, -17.0549], requires_grad=True)\n",
      "                Grad: tensor([-0.0075,  0.0425])\n",
      "Epoch: 3000, Loss: 2.9286484718322754 \n",
      "                Params: tensor([  5.3489, -17.1980], requires_grad=True)\n",
      "                Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch: 3500, Loss: 2.9278297424316406 \n",
      "                Params: tensor([  5.3597, -17.2591], requires_grad=True)\n",
      "                Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch: 4000, Loss: 2.9276793003082275 \n",
      "                Params: tensor([  5.3643, -17.2853], requires_grad=True)\n",
      "                Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4500, Loss: 2.927651882171631 \n",
      "                Params: tensor([  5.3662, -17.2964], requires_grad=True)\n",
      "                Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 5000, Loss: 2.9276468753814697 \n",
      "                Params: tensor([  5.3671, -17.3012], requires_grad=True)\n",
      "                Grad: tensor([-9.7275e-05,  6.1280e-04])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(5.3670845, dtype=float32), array(-17.301193, dtype=float32))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "learning_rate = 1e-2\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True) #setting requires_grad=True allows daughter tensors to have genetic memory\n",
    "\n",
    "params_f, loss_f = training_loop(\n",
    "    n_epochs, \n",
    "    learning_rate, \n",
    "    params, \n",
    "    t_u_norm, \n",
    "    t_c)\n",
    "\n",
    "w = params_f[0].detach().numpy()\n",
    "b = params_f[1].detach().numpy()\n",
    "\n",
    "w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(t_u, t_c, label = 'data', color='r')\n",
    "plt.plot(t_u, model(t_u_norm, *params.detach().numpy()), label = f'Model. w: {params[0]:.2f}, b: {params[1]:.2f}')\n",
    "plt.ylabel('Temperature / Celsius')\n",
    "plt.xlabel('Temperature / Unknown Units')\n",
    "plt.title(f'Linear Fit Using Parameters from Gradient Descent- Autograd\\n Loss: {final_loss:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs:int, optimizer, params:torch.Tensor, t_u:torch.Tensor, t_c:torch.Tensor)->Tuple[torch.Tensor, float]:\n",
    "    \"\"\"Uses a specified optimizer in order to train \"\"\"\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "\n",
    "        #generate predictions\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "\n",
    "        #Make sure to zero the gradients to prevent accumulation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (_ + 1) % 500 == 0:\n",
    "            print(f'Epoch: {_+1}, Loss: {loss} \\n\\\n",
    "                Params: {params}\\n\\\n",
    "                Grad: {params.grad}')\n",
    "\n",
    "    return params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, Loss: 7.860119819641113 \n",
      "                Params: tensor([ 4.0443, -9.8133], requires_grad=True)\n",
      "                Grad: tensor([-0.2252,  1.2748])\n",
      "Epoch: 1000, Loss: 3.828537940979004 \n",
      "                Params: tensor([  4.8021, -14.1031], requires_grad=True)\n",
      "                Grad: tensor([-0.0962,  0.5448])\n",
      "Epoch: 1500, Loss: 3.092191219329834 \n",
      "                Params: tensor([  5.1260, -15.9365], requires_grad=True)\n",
      "                Grad: tensor([-0.0411,  0.2328])\n",
      "Epoch: 2000, Loss: 2.957697868347168 \n",
      "                Params: tensor([  5.2644, -16.7200], requires_grad=True)\n",
      "                Grad: tensor([-0.0176,  0.0995])\n",
      "Epoch: 2500, Loss: 2.933133840560913 \n",
      "                Params: tensor([  5.3236, -17.0549], requires_grad=True)\n",
      "                Grad: tensor([-0.0075,  0.0425])\n",
      "Epoch: 3000, Loss: 2.9286484718322754 \n",
      "                Params: tensor([  5.3489, -17.1980], requires_grad=True)\n",
      "                Grad: tensor([-0.0032,  0.0182])\n",
      "Epoch: 3500, Loss: 2.9278297424316406 \n",
      "                Params: tensor([  5.3597, -17.2591], requires_grad=True)\n",
      "                Grad: tensor([-0.0014,  0.0078])\n",
      "Epoch: 4000, Loss: 2.9276793003082275 \n",
      "                Params: tensor([  5.3643, -17.2853], requires_grad=True)\n",
      "                Grad: tensor([-0.0006,  0.0033])\n",
      "Epoch: 4500, Loss: 2.927651882171631 \n",
      "                Params: tensor([  5.3662, -17.2964], requires_grad=True)\n",
      "                Grad: tensor([-0.0003,  0.0014])\n",
      "Epoch: 5000, Loss: 2.9276468753814697 \n",
      "                Params: tensor([  5.3671, -17.3012], requires_grad=True)\n",
      "                Grad: tensor([-9.7275e-05,  6.1280e-04])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([  5.3671, -17.3012], requires_grad=True),\n",
       " tensor(2.9276, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "\n",
    "#Use Stochastic Gradient Descent Optimizer\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "params_f, loss_f = training_loop(n_epochs, optimizer, params, t_u_norm, t_c)\n",
    "\n",
    "params_f, loss_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(t_u, t_c, label = 'data', color='r')\n",
    "plt.plot(t_u, model(t_u_norm, *params.detach().numpy()), label = f'Model. w: {params[0]:.2f}, b: {params[1]:.2f}')\n",
    "plt.ylabel('Temperature / Celsius')\n",
    "plt.xlabel('Temperature / Unknown Units')\n",
    "plt.title(f'Linear Fit Using Parameters from Gradient Descent- SGD\\n Loss = {loss_f:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, Loss: 7.6128997802734375 \n",
      "                Params: tensor([  0.4081, -10.0095], requires_grad=True)\n",
      "                Grad: tensor([-1.1792,  1.2604])\n",
      "Epoch: 1000, Loss: 3.086698293685913 \n",
      "                Params: tensor([  0.5131, -15.9629], requires_grad=True)\n",
      "                Grad: tensor([-0.2153,  0.2323])\n",
      "Epoch: 1500, Loss: 2.9285776615142822 \n",
      "                Params: tensor([  0.5350, -17.2022], requires_grad=True)\n",
      "                Grad: tensor([-0.0165,  0.0178])\n",
      "Epoch: 2000, Loss: 2.9276463985443115 \n",
      "                Params: tensor([  0.5367, -17.3021], requires_grad=True)\n",
      "                Grad: tensor([-0.0003,  0.0005])\n",
      "Epoch: 2500, Loss: 2.927645206451416 \n",
      "                Params: tensor([  0.5368, -17.3047], requires_grad=True)\n",
      "                Grad: tensor([-1.3733e-04,  1.1742e-05])\n",
      "Epoch: 3000, Loss: 2.9276459217071533 \n",
      "                Params: tensor([  0.5368, -17.3047], requires_grad=True)\n",
      "                Grad: tensor([-1.6785e-04,  8.1658e-06])\n",
      "Epoch: 3500, Loss: 2.927644968032837 \n",
      "                Params: tensor([  0.5368, -17.3047], requires_grad=True)\n",
      "                Grad: tensor([1.9789e-04, 1.2100e-05])\n",
      "Epoch: 4000, Loss: 2.927645683288574 \n",
      "                Params: tensor([  0.5368, -17.3048], requires_grad=True)\n",
      "                Grad: tensor([-1.8167e-04,  2.5928e-06])\n",
      "Epoch: 4500, Loss: 2.9276463985443115 \n",
      "                Params: tensor([  0.5368, -17.3048], requires_grad=True)\n",
      "                Grad: tensor([-2.8467e-04, -3.2783e-07])\n",
      "Epoch: 5000, Loss: 2.927645206451416 \n",
      "                Params: tensor([  0.5368, -17.3048], requires_grad=True)\n",
      "                Grad: tensor([-7.1239e-04, -9.4473e-06])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([  0.5368, -17.3048], requires_grad=True),\n",
       " tensor(2.9276, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "\n",
    "#ADAM allows us to use the unnormalised values of the unknown temperature readings\n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "\n",
    "params_f, loss_f = training_loop(n_epochs, optimizer, params, t_u, t_c)\n",
    "\n",
    "params_f, loss_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(t_u, t_c, label = 'data', color='r')\n",
    "plt.plot(t_u, model(t_u, *params.detach().numpy()), label = f'Model. w: {params[0]:.2f}, b: {params[1]:.2f}')\n",
    "plt.ylabel('Temperature / Celsius')\n",
    "plt.xlabel('Temperature / Unknown Units')\n",
    "plt.title(f'Linear Fit Using Parameters from Adam\\n Loss = {loss_f:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Validation and Training Sets</h1>\n",
    "We now look at splitting the data so that we can reserve some of our data for validation on top of the data that we have already used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7, 2, 1, 0, 4, 6, 3, 5, 9]), tensor([ 8, 10]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0] #Number of samples is equal to the length of the t_u tensor\n",
    "n_val = int(0.2 * n_samples) #Number of data points which we want to reserve for validation\n",
    "\n",
    "torch.manual_seed(1024)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set aside for training\n",
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "#Set aside for validation\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs:int, optimizer, params:torch.Tensor, train_t_u:torch.Tensor, val_t_u:torch.Tensor, train_t_c:torch.Tensor, \n",
    "                  val_t_c:torch.Tensor)->Tuple[torch.Tensor, float]:\n",
    "    \"\"\"Uses a specified optimizer in order to train \"\"\"\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "\n",
    "        #generate predictions\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #generate predictions\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "\n",
    "        #Make sure to zero the gradients to prevent accumulation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (_ + 1) % 500 == 0 or (_ + 1) <= 3:\n",
    "            print(f'Epoch: {_+1}, Training Loss: {train_loss.item():.2f}, Validation Loss: {val_loss.item():.2f}')\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1706.47, Validation Loss: 2022.26\n",
      "Epoch: 2, Training Loss: 1293.06, Validation Loss: 1520.13\n",
      "Epoch: 3, Training Loss: 939.98, Validation Loss: 1092.99\n",
      "Epoch: 500, Training Loss: 6.16, Validation Loss: 11.37\n",
      "Epoch: 1000, Training Loss: 2.51, Validation Loss: 6.12\n",
      "Epoch: 1500, Training Loss: 2.44, Validation Loss: 5.48\n",
      "Epoch: 2000, Training Loss: 2.44, Validation Loss: 5.45\n",
      "Epoch: 2500, Training Loss: 2.44, Validation Loss: 5.44\n",
      "Epoch: 3000, Training Loss: 2.44, Validation Loss: 5.44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5234, -16.5090], requires_grad=True)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3000\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "\n",
    "params_f = training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c)\n",
    "\n",
    "params_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(t_u, t_c, label = 'data', color='r')\n",
    "plt.plot(t_u, model(t_u, *params.detach().numpy()), label = f'Model. w: {params[0]:.2f}, b: {params[1]:.2f}')\n",
    "plt.ylabel('Temperature / Celsius')\n",
    "plt.xlabel('Temperature / Unknown Units')\n",
    "plt.title(f'Linear Fit Using Parameters from Adam\\n Loss = {loss_f:.2f}\\n Training Data Points = {len(train_indices)}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
